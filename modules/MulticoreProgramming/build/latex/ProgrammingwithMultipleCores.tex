% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,openany,oneside]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}


\title{Multicore Programming with OpenMP}
\date{March 07, 2014}
\release{}
\author{CSInParallel Project}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


This module depicts how to use OpenMP to solve a fairly simple mathematical calculation: estimating the area under a curve as a summation of trapezoids.  Some issues that arise when using multiple threads to complete this task are introduced.  Once you have working code, we discuss how you can estimate the \emph{speedup} you obtain using varying numbers of threads and have you think about how scaleable this problem is as you increase the number of trapezoids (the problem size) and use more threads.


\chapter{Getting Started with Multicore Programming using OpenMP}
\label{introOpenMP/GettingstartedwithOpenMP:getting-started-with-multicore-programming-using-openmp}\label{introOpenMP/GettingstartedwithOpenMP:multicore-programming-with-openmp}\label{introOpenMP/GettingstartedwithOpenMP::doc}

\section{Notes about this document}
\label{introOpenMP/GettingstartedwithOpenMP:notes-about-this-document}
This is designed to be a lab activity  that you will perform on linux machines and/or on the Intel Manycore Testing Lab (MTL).
\begin{quote}\begin{description}
\item[{Comments}] \leavevmode
Sections labeled like this are important explanations to pay attention to.

\end{description}\end{quote}
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{Dig Deeper:}

\medskip


Comments in this format indicate possible avenues of exploration for people seeking more challenge or depth of knowledge.
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\section{Multicore machines and shared memory}
\label{introOpenMP/GettingstartedwithOpenMP:multicore-machines-and-shared-memory}
Multicore CPUs have more than one ‘core’ processor that can execute
instructions at the same time.   The cores share main memory.  In the
next few activities, we will learn how to use a library called OpenMP to
enable us to write programs that can use multicore processors and shared
memory to write programs that can complete a task faster by taking
advantage of using many cores.  These programs are said to work “in
parallel”.  We will start with our own single machines, and then
eventually use a machine with a large number of cores provided by Intel
Corporation, called the Manycore Testing Lab (MTL).

Parallel programs use multiple ‘threads’ executing instructions
simultaneously to accomplish a task in a shorter amount of time than a
single-threaded version.  A process is an execution of a program. A
thread is an independent execution of (some of) a process's code that
shares (some) memory and/or other resources with that process. When
designing a parallel program, you need to determine what portions could
benefit from having many threads executing instructions at once.  In
this lab, we will see how we can use “task parallelism” to execute the
same task on different parts of a desired computation in threads and
gather the results when each task is finished.


\section{Getting started with OpenMP}
\label{introOpenMP/GettingstartedwithOpenMP:getting-started-with-openmp}
We will use a standard system for parallel programming
called\href{http://openmp.org/wp/}{ }\href{http://openmp.org/wp/}{OpenMP},
which enables a C or C++ programmer to take advantage of multi-core
parallelism primarily through preprocessor pragmas. These are directives
that enable the compiler to add and change code (in this case to add
code for executing sections of it in parallel).


\strong{See Also:}


More resources about OpenMP can be found here: \href{http://openmp.org/wp/resources/}{http://openmp.org/wp/resources/}.



We will begin with a short C++ program, parallelize it using OpenMP, and
improve the parallelized version. This initial development work can be
carried out on a linux machine.  Working this time with C++ will not be
too difficult, as we will not be using the object-oriented features of
the language, but will be taking advantage of easier printing of output.

The following program computes a Calculus value, the ``trapezoidal
approximation of
\begin{gather}
\begin{split}\int_0^x \sin(x) {d}{x}\end{split}\notag
\end{gather}
using $2^{20}$ equal subdivisions.”   The exact answer from this computation
should be 2.0.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}iostream\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}cmath\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}cstdlib\PYGZgt{}}
\PYG{k}{using} \PYG{k}{namespace} \PYG{n}{std}\PYG{p}{;}

\PYG{c+cm}{/* Demo program for OpenMP: computes trapezoidal approximation to an integral*/}

\PYG{k}{const} \PYG{k+kt}{double} \PYG{n}{pi} \PYG{o}{=} \PYG{l+m+mf}{3.141592653589793238462643383079}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{c+cm}{/* Variables */}
  \PYG{k+kt}{double} \PYG{n}{a} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{n}{pi}\PYG{p}{;}  \PYG{c+cm}{/* limits of integration */}\PYG{p}{;}
  \PYG{k+kt}{int} \PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1048576}\PYG{p}{;} \PYG{c+cm}{/* number of subdivisions = 2\PYGZca{}20 */}
  \PYG{k+kt}{double} \PYG{n}{h} \PYG{o}{=} \PYG{p}{(}\PYG{n}{b} \PYG{o}{-} \PYG{n}{a}\PYG{p}{)} \PYG{o}{/} \PYG{n}{n}\PYG{p}{;} \PYG{c+cm}{/* width of subdivision */}
  \PYG{k+kt}{double} \PYG{n}{integral}\PYG{p}{;} \PYG{c+cm}{/* accumulates answer */}
  \PYG{k+kt}{int} \PYG{n}{threadct} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}  \PYG{c+cm}{/* number of threads to use */}
  
  \PYG{c+cm}{/* parse command-line arg for number of threads */}
  \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{threadct} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}

  \PYG{k+kt}{double} \PYG{n}{f}\PYG{p}{(}\PYG{k+kt}{double} \PYG{n}{x}\PYG{p}{)}\PYG{p}{;}
    
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{ifdef \PYGZus{}OPENMP}
  \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{OMP defined, threadct = }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{threadct} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{else}
  \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{OMP not defined}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{endif}

  \PYG{n}{integral} \PYG{o}{=} \PYG{p}{(}\PYG{n}{f}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)} \PYG{o}{+} \PYG{n}{f}\PYG{p}{(}\PYG{n}{b}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mf}{2.0}\PYG{p}{;}
  \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}

  \PYG{k}{for}\PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{integral} \PYG{o}{+}\PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{a}\PYG{o}{+}\PYG{n}{i}\PYG{o}{*}\PYG{n}{h}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}
  
  \PYG{n}{integral} \PYG{o}{=} \PYG{n}{integral} \PYG{o}{*} \PYG{n}{h}\PYG{p}{;}
  \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{With n = }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{n} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ trapezoids, our estimate of the integral}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}}
    \PYG{l+s}{"}\PYG{l+s}{ from }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{a} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ to }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{b} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ is }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{integral} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
   
\PYG{k+kt}{double} \PYG{n}{f}\PYG{p}{(}\PYG{k+kt}{double} \PYG{n}{x}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{k}{return} \PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}
\begin{quote}\begin{description}
\item[{Comments}] \leavevmode\begin{itemize}
\item {} 
If a command line argument is given, the code segment below converts that argument to an integer and assigns that value to the variable threadct, overriding the default value of 1. This uses the two arguments of the function main(), namely argc and argv. This demo program makes no attempt to check whether a first command line argument argv{[}1{]} is actually an integer, so make sure it is (or omit it).

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
  \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{threadct} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
\end{Verbatim}
\begin{itemize}
\item {} 
The variable \emph{threadct} will be used later to control the number of threads to be used. Recall that a process is an execution of a program. A \textbf{thread} is an independent execution of (some of) a process's code that shares (some) memory and/or other resources with that process. We will modify this program to use multiple threads, which can be executed in parallel on a multi-core computer.

\item {} 
The preprocessor macro \_OPENMP is defined for C++ compilations that include support for OpenMP. Thus, the code segment below provides a way to check whether OpenMP is in use.

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{ifdef \PYGZus{}OPENMP}
  \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{OMP defined, threadct = }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{threadct} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{else}
  \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{OMP not defined}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{endif}
\end{Verbatim}
\begin{itemize}
\item {} 
The above code also shows the convenient way to print to stdout in C++.

\item {} 
The following lines contain the actual computation of the trapezoidal approximation:

\end{itemize}

\begin{Verbatim}[commandchars=\\\{\}]
  \PYG{n}{integral} \PYG{o}{=} \PYG{p}{(}\PYG{n}{f}\PYG{p}{(}\PYG{n}{a}\PYG{p}{)} \PYG{o}{+} \PYG{n}{f}\PYG{p}{(}\PYG{n}{b}\PYG{p}{)}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mf}{2.0}\PYG{p}{;}
  \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}

  \PYG{k}{for}\PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{integral} \PYG{o}{+}\PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{a}\PYG{o}{+}\PYG{n}{i}\PYG{o}{*}\PYG{n}{h}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}
  
  \PYG{n}{integral} \PYG{o}{=} \PYG{n}{integral} \PYG{o}{*} \PYG{n}{h}\PYG{p}{;}
\end{Verbatim}

Since n == $2^{20}$, the for loop adds over 1 million values. Later in this
lab, we will parallelize that loop, using multiple cores which
will each perform part of this summation, and look for speedup in the
program's performance.

\end{description}\end{quote}


\section{To Do:}
\label{introOpenMP/GettingstartedwithOpenMP:to-do}
On a linux machine, create a file named trap-omp.C containing the program above or grab it and save it from the following link:

\code{download trap-omp.C}  (on most browsers, right-click, save link as)

To compile your file, you can enter the command:

\begin{Verbatim}[commandchars=\\\{\}]
\%  g++ -o trap-omp trap-omp.C -lm -fopenmp
\end{Verbatim}

\begin{notice}{note}{Note:}
Here, \% represents your shell prompt, which is usually a machine name followed by either \% or \$.
\end{notice}

First, try running the program without a command-line argument, like this:

\begin{Verbatim}[commandchars=\\\{\}]
\%  ./trap-omp
\end{Verbatim}

This should print a line ``\_OPENMP defined, threadct = 1'', followed by a
line indicating the computation with an answer of 2.0. Next, try

\begin{Verbatim}[commandchars=\\\{\}]
\%  ./trap-omp 2
\end{Verbatim}

This should indicate a different thread count, but otherwise produce the
same output. Finally, try recompiling your program omitting
the -fopenmp flag. This should report \_OPENMP not defined, but give the
same answer 2.0.

Note that the program above is actually using only a single core, whether
or not a command-line argument is given. It is an ordinary C++ program
in every respect, and OpenMP does not magically change ordinary C++
programs; in particular, the variable \emph{threadct} is just an ordinary local
variable with no special computational meaning.

To request a parallel computation, add the following pragma preprocessor
directive, just before the for loop.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for num\PYGZus{}threads(threadct) \PYGZbs{}}
\PYG{c+cp}{     shared (a, n, h, integral) private(i)}
\end{Verbatim}

The resulting code will have this format:

\begin{Verbatim}[commandchars=\\\{\}]
  \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for num\PYGZus{}threads(threadct) \PYGZbs{}}
\PYG{c+cp}{     shared (a, n, h, integral) private(i)}
  \PYG{k}{for}\PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{integral} \PYG{o}{+}\PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{a}\PYG{o}{+}\PYG{n}{i}\PYG{o}{*}\PYG{n}{h}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}
\end{Verbatim}
\begin{quote}\begin{description}
\item[{Comments}] \leavevmode\begin{itemize}
\item {} 
Make sure no characters follow the backslash character before the end of the first line. This causes the two lines to be treated as a single pragma (useful to avoid long lines).

\item {} 
The phrase \textbf{omp parallel for} indicates that this is an OpenMP pragma for parallelizing the for loop that follows immediately. The OpenMP system will divide the $2^{20}$ iterations of that loop up into \emph{threadct} segments, each of which can be executed in parallel on multiple cores.

\item {} 
The OpenMP clause \textbf{num\_threads(threadct)} specifies the number of threads to use in the parallelization.

\item {} 
The clauses in the second line indicate whether the variables that appear in the for loop should be shared with the other threads, or should be local private variables used only by a single thread. Here, four of those variables are globally shared by all the threads, and only the loop control variable i is local to each particular thread.

\end{itemize}

\end{description}\end{quote}

Enter the above code change (add the pragma preprocessor directive), then compile and test the resulting executable with one thread, then  more than one thread, like this:

\begin{Verbatim}[commandchars=\\\{\}]
\%  g++ -o trap-omp trap-omp.C -lm -fopenmp
\%  ./trap-omp
\%  ./trap-omp 2
\%  ./trap-omp 3
etc.
\end{Verbatim}
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{Dig Deeper:}

\medskip

\begin{itemize}
\item {} 
The \href{https://computing.llnl.gov/tutorials/openMP/}{OpenMP tutorial} contains more information about advanced uses of OpenMP. Note that OpenMP is a combination of libraries and compiler directives that have been defined for both Fortran and C/C++.

\item {} 
OpenMP provides other ways to set the number of threads to use, namely the omp\_set\_num\_threads() library function (see \href{https://computing.llnl.gov/tutorials/openMP/\#RunTimeLibrary}{tutorial section on library routines}), and the OMP\_NUM\_THREADS linux/unix environment variable (see \href{https://computing.llnl.gov/tutorials/openMP/\#EnvironmentVariables}{tutorial section on environment variables}).

\item {} 
OpenMP provides several other clauses for managing variable locality, initialization, etc. Examples: default, firstprivate, lastprivate, copyprivate.  You could investigate this further in the \href{https://computing.llnl.gov/tutorials/openMP/\#Clauses}{tutorial section pertaining to clauses}.

\end{itemize}
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\section{What's happening?}
\label{introOpenMP/GettingstartedwithOpenMP:what-s-happening}
After inserting the parallel for pragma, observe that for threadct == 1 (e.g., no
command-line argument), the program runs
and produces the correct result of 2.0, but that

\begin{Verbatim}[commandchars=\\\{\}]
\% ./trap-omp 2
\end{Verbatim}

which sets threadct == 2, sometimes produces an incorrect answer (perhaps about
1.5). What happens with repeated runs with that and other (positive)
thread counts? Can you explain why?

\textbf{Note:} Try reasoning out why the computed answer is correct for one
thread but incorrect for two or more threads. Hint: All of the values
being added in this particular loop are positive values, and the
erroneous answer is too low.

If you figure out the cause, think about how to fix the problem. You may
use the\href{http://openmp.org/wp/}{ }\href{http://openmp.org/wp/}{OpenMP
website} or other resources to research your
solution, if desired.


\chapter{Creating a correct threaded version}
\label{correctOpenMP/FixingOurOpenMPcode:creating-a-correct-threaded-version}\label{correctOpenMP/FixingOurOpenMPcode::doc}

\section{Race Conditions}
\label{correctOpenMP/FixingOurOpenMPcode:race-conditions}
A program has a race condition if the correct behavior of that program
depends on the timing of its execution. With 2 or more threads, the
program trap-omp.C has a race condition concerning the shared variable
\code{integral}, which is the accumulator for the summation performed by that
program's for loop.

When threadct == 1, the single thread of execution updates the shared variable
\code{integral} on every iteration, by reading the prior value of the memory location for
\code{integral}, computing and adding the value f(a+i*h), then storing the result into that memory location. (Recall that a variable is a named location in main memory.)

But when threadct \textgreater{} 1, there are at least two independent threads, executed on separate physical cores, that are reading then writing the memory location for
\code{integral}. The incorrect answer results when the reads and writes of that memory location get out of order. Here is one example of how unfortunate ordering can happen with two threads:

\begin{tabular}{|p{0.317\linewidth}|p{0.317\linewidth}|p{0.317\linewidth}|}
\hline
\textbf{
Threads running:
} & \textbf{
Thread 1
} & \textbf{
Thread 2
}\\\hline

source code:
 & 
\begin{DUlineblock}{0em}
\item[] integral += f(a+i*h);
\end{DUlineblock}
 & 
\begin{DUlineblock}{0em}
\item[] integral += f(a+i*h);
\end{DUlineblock}
\\\hline

execution of binary code:
 & 
\begin{DUlineblock}{0em}
\item[] 
\item[] 1. read value of integral
\item[] 2. add  f(a+i*h)
\end{DUlineblock}

\begin{DUlineblock}{0em}
\item[] 3. write sum to  integral
\end{DUlineblock}
 & 
\begin{DUlineblock}{0em}
\item[] 
\item[] 
\item[] 1. read value of integral
\item[] 2. add  f(a+i*h)
\item[] 
\item[] 3. write sum to  integral
\end{DUlineblock}
\\\hline
\end{tabular}


In this example, during one poorly timed iteration for each thread,
Thread 2 reads the value of the memory location integral before Thread 1
can write its sum back to integral. The consequence is that Thread 2
replaces (overwrites) Thread 1's value of integral, so the amount added
by Thread 1 is omitted from the final value of the accumulator integral.
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{To Do}

\medskip


Can you think of other situations where unfortunate ordering of thread
operations leads to an incorrect value of integral? Write down at least
one other bad timing scenario.  Other scenarios will work (you may have seen some of these during testing of your code).  Write down one of these.
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\section{Avoiding Race Conditions}
\label{correctOpenMP/FixingOurOpenMPcode:avoiding-race-conditions}
One approach to avoiding this program's race condition is to use a separate local variable integral for each thread instead of a global variable that is shared by all the threads. But declaring integral to be private instead of shared in the pragma will only generate threadct partial sums in those local variables named integral -- the partial sums in those temporary local variables will not be added to the program's variable integral. In fact, the value in those temporary local variables will be discarded when each thread finishes its work for the parallel for if we simply make integral private instead of shared.
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{To Do}

\medskip


Can you re-explain this situation in your own words?
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}

Fortunately, OpenMP provides a convenient and effective solution to this problem. The OpenMP clause

\begin{Verbatim}[commandchars=\\\{\}]
reduction(+: integral)
\end{Verbatim}

will
\begin{enumerate}
\item {} 
cause the variable integral to be private (local) during the
execution of each thread, and

\item {} 
add the results of all those private variables, then finally

\item {} 
store that sum of private variables in the global variable named
integral.

\end{enumerate}


\section{Reduction}
\label{correctOpenMP/FixingOurOpenMPcode:reduction}
This code example contains a very common \emph{pattern} found in
parallel programs: \textbf{reducing several values down to one}.  This is so common that the designers of OpenMP chose to make it simple to declare that a variable was involved on a reduction.  The code here represents one way that reduction takes place: during a step-wise calculation of smaller pieces of a larger problem.  The other common type of reduction is to accumulate all of the values stored in an array.


\strong{See Also:}


You can use other arithmetic operators besdides plus in the reduction clause-- see \href{https://computing.llnl.gov/tutorials/openMP/\#REDUCTION}{the OpenMP tutorial section on reduction}



According to the OpenMP Tutorial, here is how the reduction is being done inside the compiled OpenMP code (similar to how we described it above, but generalized to any reduction operator):
\begin{quote}

``A private copy for each listed variable is created for each thread.  At the end of the reduction, the reduction operation is applied to all private copies of the shared variable, and the final result is written to the global shared variable.''
\end{quote}

Thus, a variable such as \code{integral} that is declared in a reduction clause is both private to each thread and ultimately a shared variable.
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{To Do}

\medskip


Add the above reduction clause to your OpenMP pragma and remove integral from the list of shared variables, so that the pragma in your code appears as shown below.  Then recompile and test your program many times with different numbers of threads. You should now see the correct answer of 2.0 every time you execute it with multiple threads -- a correct multi-core program!
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for num\PYGZus{}threads(threadct) \PYGZbs{}}
\PYG{c+cp}{     shared (a, n, h) reduction(+: integral) private(i)}
  \PYG{k}{for}\PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{integral} \PYG{o}{+}\PYG{o}{=} \PYG{n}{f}\PYG{p}{(}\PYG{n}{a}\PYG{o}{+}\PYG{n}{i}\PYG{o}{*}\PYG{n}{h}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}
\end{Verbatim}
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{Tricky Business}

\medskip


Note that with the incorrect version, you sometimes got lucky (perhaps even many times) as you tried running it over and over. This is one of the most perplexing and difficult aspects of parallel programming: \textbf{it can be difficult to find your bugs because many times your program will apear to be correct}.  Remember this: you need to be diligent about thinking through your solutions and questioning whether you have coded it correctly.
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\section{Thread-safe Code}
\label{correctOpenMP/FixingOurOpenMPcode:thread-safe-code}
A code segment is said to be \emph{thread-safe} if it remains correct when
executed by multiple independent threads. The body of this loop is
not thread-safe; we were able to make it so by indicating that a \emph{reduction}
was taking place on the variable \code{integral}.

Some C/C++ libraries are identified as thread-safe, meaning that each function
in that library is thread-safe. Of course, calling a thread-safe
function doesn't insure that the code with that function call is
thread-safe. For example, the function f() in our example, is
thread-safe, but the body of that loop is not thread-safe.


\chapter{Timing and Performance on Multicore machines}
\label{timingAndScalability/TimingOnMTLandscalability::doc}\label{timingAndScalability/TimingOnMTLandscalability:timing-and-performance-on-multicore-machines}

\section{Timing performance}
\label{timingAndScalability/TimingOnMTLandscalability:timing-performance}
We would like to know how long it takes to run various versions of our
programs so that we can determine if adding additional threads to our
computation is worth it.

There are several different ways that we can obtain the time it takes a
program to run (we typically like to get this time in milliseconds or
less).


\subsection{Simple, less accurate way: linux \textbf{time} program}
\label{timingAndScalability/TimingOnMTLandscalability:simple-less-accurate-way-linux-time-program}
We can obtain the running time for an
entire program using the \emph{time} Linux program. For example, the line

\begin{Verbatim}[commandchars=\\\{\}]
/usr/bin/time -p trap-omp
\end{Verbatim}

might display the following output:

\begin{Verbatim}[commandchars=\\\{\}]
OMP defined, threadct = 1
With n = 1048576 trapezoids, our estimate of the integral from 0 to 3.14159 is 2

real 0.04

user 0.04

sys 0.00
\end{Verbatim}

Here, we use the full path /usr/bin/time to insure that we are accessing
the time program instead of a shell built-in command.

The real time measures actual time elapsed during the running of your
command trap-omp. user measures the amount of time executing user code,
and sys measures the time executing in Linux kernel code.
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{To Do}

\medskip


Try the time command using your linux machine, and compare the results
for different thread counts. You should find that real time decreases
somewhat when changing from 1 thread to 2 threads; user time increases
somewhat. Can you think of reasons that might produce these results?

Also, real time and user time increase considerably on some machines
when increasing from 2 to 3 or more threads. What might explain that?
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\subsection{Additional accuracy: Using OpenMP functions for timing code}
\label{timingAndScalability/TimingOnMTLandscalability:additional-accuracy-using-openmp-functions-for-timing-code}
The for loop in your
trap-omp.C code represents the parallel portion of the code.  The other
parts are the ‘sequential parts’ where one thread is being used (these portions of code are quite small in this simple example).  Using
functions to get current time at points in your program, you can begin
to examine how long the sequential port takes in relation to the
parallel portion.  You can also use these functions around all the code
to determine how long it takes with varying numbers of processors.

We can use an OMP library function whose ‘signature’ looks like this:

\begin{Verbatim}[commandchars=\\\{\}]
\#include \textless{}omp.h\textgreater{}

double omp\_get\_wtime( void );
\end{Verbatim}

We can use this in the code in the following way:

\begin{Verbatim}[commandchars=\\\{\}]
// Starting the time measurement

double start = omp\_get\_wtime();

// Computations to be measured

...

// Measuring the elapsed time

double end = omp\_get\_wtime();

// Time calculation (in seconds)

double time1 = end - start;

//print out the resulting elapsed time
cout \textless{}\textless{} "Time for paralel computation section: "\textless{}\textless{} time1 \textless{}\textless{} "  seconds." \textless{}\textless{} endl;
...
\end{Verbatim}
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{To Do}

\medskip


Try inserting these pieces code and printing out the time it takes to execute
portions of your trap-omp.C code.  You will use this for later portions
of this activity.  You can do this on your local linux machine now to test it out and make sure it is working.
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}


\subsection{Basic C/C++ timing functions}
\label{timingAndScalability/TimingOnMTLandscalability:basic-c-c-timing-functions}
You could use basic linux/C/C++ timing functions: See the end of this
activity for an explanation of the way in which we can time any C/C++
code, even if you are not using OpenMP library functions or pragmas. This will come in handy in cases where you are not using OpenMP (such as CUDA, for example).


\section{Using the MTL}
\label{timingAndScalability/TimingOnMTLandscalability:using-the-mtl}
If you have access to Intel's Manycore Testing Lab (MTL), you can try some experimenting.  You may also use another machine that your instructor will give you access to.

Let’s try using many more threads and really experiment with multicore
programming! You will need to use a ‘terminal’ on Macs or ‘Putty’ on
PCs.  If you are off campus, you will need to ssh into a machine on your campus before
then logging into the MTL machine at Intel’s headquarters in Oregon.

\begin{notice}{note}{Note:}
Macalester's machine that you can use is nuggle.macalester.edu.
\end{notice}

You can login to the MTL computer, as follows

\begin{Verbatim}[commandchars=\\\{\}]
ssh accountname@207.108.8.131
\end{Verbatim}

Use one of the special MTL student account usernames provided to you, together with
the password distributed to the class.

Next, copy your program from your laptop or local linux machine to the MTL machine.
One way to do this is to use another window (to
keep for copying your code), then enter the following command \emph{from the directory where your code is located}:

\begin{Verbatim}[commandchars=\\\{\}]
scp trap-omp.C accountname@207.108.8.131:
\end{Verbatim}

After making this copy, login into the MTL machine 192.55.51.81 in another window.

On the MTL machine, compile and test run your program.

\begin{Verbatim}[commandchars=\\\{\}]
g++ -o trap-omp trap-omp.C -lm -fopenmp

./trap-omp

./trap-omp 2

./trap-omp 16
\end{Verbatim}

\textbf{Note:} Since the current directory . may not be in your default path, you
probably need to use the path name ./trap-omp to invoke your program.

Now, try some time trials of your code on the MTL machine. (The full
pathname for time and the -p flag are unnecessary.) For example, using
time:

\begin{Verbatim}[commandchars=\\\{\}]
time trap-omp

time trap-omp 2

time trap-omp 3

time trap-omp 4

time trap-omp 8

time trap-omp 16

time trap-omp 32
\end{Verbatim}

What patterns do you notice with the real and user times of various runs
of trap-omp with various values of threadct?

Also try it without using the time command on the command line and
instead using the OpenMP omp\_get\_wtime() function calls in your code.
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{To Do}

\medskip


It may be useful to change our problems size, n, to see how this affects the time and to observe the range of times that can occur for various problem sizes.  We therefore should eliminate `hard coding' of n.

Now update your code so that you submit the number of elements to compute as an additional command-line argument. Now the number of trapezoids, \code{n} should be set to the value in argv{[}2{]} (at the time that you set threadcnt to the value in argv{[}1{]}). \textbf{This also involves moving the declaration and assignment of the variable h, also}. The updated segment of code should look like this:
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}

\begin{Verbatim}[commandchars=\\\{\}]
  \PYG{c+cm}{/* parse command-line arg for number of threads, n */}
  \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{threadct} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{threadct} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{n} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}
  \PYG{k+kt}{double} \PYG{n}{h} \PYG{o}{=} \PYG{p}{(}\PYG{n}{b} \PYG{o}{-} \PYG{n}{a}\PYG{p}{)} \PYG{o}{/} \PYG{n}{n}\PYG{p}{;} \PYG{c+cm}{/* width of subdivision */}
\end{Verbatim}

\begin{notice}{note}{Note:}
The best way to work is to change your code on your local campus machine and copy it to the MTL using scp.  That way you have your own copy.
\end{notice}


\subsection{Submitting Batch Jobs for Timing Accurately}
\label{timingAndScalability/TimingOnMTLandscalability:submitting-batch-jobs-for-timing-accurately}
To submit a job on MTL and guarantee that you have exclusive access to a
nod for timing purposes, you submit your job to a queuing system.  You
do this by creating a ‘script’ file that will be read by the queuing
system.  Then you use the qsub command to run that script.  Here is an
example of the contents of a script file (save it as submit.sh on your
MTL account):

\begin{Verbatim}[commandchars=\\\{\}]
    \#!/bin/sh

    \#PBS -N LS\PYGZbs{}\_trap

    \#PBS -j oe

    \#here is how we can send parameters from job submission on the command
    line:

    \$HOME/240activities/trap-omp \$p \$n

\# this is a shell script comment
    \# the job gets submitted like this:
    \#   qsub -l select=1:ncpus=32 -v 'p=32, n=10485760'

    /home/mcls/240activities/submit.sh

    \#\#\#\#\#\#\#\#\# end of script file
\end{Verbatim}

Here is an example of how you run the script (change the path for your
user account):

\begin{Verbatim}[commandchars=\\\{\}]
qsub -l select=1:ncpus=32 -v 'p=32, n=10485760'
/home/mcls/240activities/submit.sh
\end{Verbatim}


\section{Investigating ‘scalability’}
\label{timingAndScalability/TimingOnMTLandscalability:investigating-scalability}
Scalability is the ability of a parallel program to run increasingly larger problems.  In our simple program, the \emph{problem size} is the number of trapazoids whose area are computed.  You will now conduct some investigations of two types of scalability of parallel programs:
\begin{itemize}
\item {} 
\textbf{stong scalability}

\item {} 
\textbf{weak scalbility}

\end{itemize}


\subsection{Strong Scalability}
\label{timingAndScalability/TimingOnMTLandscalability:strong-scalability}
As you keep the same ‘problem size’, i.e. the amount of work being done,
and increase the number of processors, you would hope that the time
drops proportionally to the number of processors used.  So in your case
of the problem size being the number of trapezoids computed, $2^{20}$, are
you able to halve the time as you double the number of threads?  When
does this stop being the case, if at all?  When this occurs, your
program is exhibiting \emph{strong scalability}, in that additional resources
(threads in this case) help you \textbf{obtain an answer faster}.  To truly
determine whether you have \emph{strong scalability}, you will likely need to
try a larger problem size on the MTL.


\subsection{Weak Scalability}
\label{timingAndScalability/TimingOnMTLandscalability:weak-scalability}
Another interesting set of experiments to try is to both increase the problem size
by changing the number of trapezoids to values higher than $2^{20}$ and to correspondingly increase the number of threads. Try
this: if you double the problem size and double the number of threads,
does the loop take the same amount of time?  In high performance
computation, this is known as \emph{weak scalability}:  you can keep using more
processors (to a point) to \textbf{tackle larger problems}.

\begin{notice}{note}{Note:}
Don't try more than the maximum 40 cores on the MTL for the above tests.
\end{notice}


\subsection{What happens when you have more threads than cores?}
\label{timingAndScalability/TimingOnMTLandscalability:what-happens-when-you-have-more-threads-than-cores}
Another interesting investigation is to consider what happens when you
‘oversubscribe’ the cores by using more threads than cores available.
 Try this experiment and write down your results and try to explain
them.


\section{An Alternative Method for Timing Code (optional; for reference)}
\label{timingAndScalability/TimingOnMTLandscalability:an-alternative-method-for-timing-code-optional-for-reference}
The following code snippets can be used in your program to time sections
of your program.  This is the traditional linux/C/C++ method, which is
most likely what the implementation of the OMP function get\_wtime() is
using.

\begin{Verbatim}[commandchars=\\\{\}]
/* Put this line at the top of the file: */
\#include \textless{}sys/time.h\textgreater{}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
/* Put this right before the code you want to time: */
struct timeval timer\_start, timer\PYGZbs{}\_end;
gettimeofday(\&timer\_start, NULL);
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
/* Put this right after the code you want to time: */
gettimeofday(\&timer\_end, NULL);
double time\_spent = timer\_end.tv\_sec - timer\_start.tv\_sec +
                      (timer\_end.tv\_usec - timer\_start.tv\_usec) / 1000000.0;
printf("Time spent: \%.6f\PYGZbs{}\PYGZbs{}n", time\_spent);
\end{Verbatim}

\begin{notice}{note}{Note:}
This example uses C printf statements; feel free to use C++ cout syntax, perhaps like this:
\end{notice}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{cout} \PYG{o}{\PYGZlt{}\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{Time for paralel computation section: }\PYG{l+s}{"}\PYG{o}{\PYGZlt{}\PYGZlt{}} \PYG{n}{time\PYGZus{}spent} \PYG{o}{\PYGZlt{}\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{  milliseconds.}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\end{Verbatim}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
