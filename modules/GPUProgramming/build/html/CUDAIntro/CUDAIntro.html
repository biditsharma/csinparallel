

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>CUDA Intro &mdash; GPU Programming</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="GPU Programming" href="../index.html" />
    <link rel="next" title="Thread Advance" href="../ThreadAdvance/ThreadAdvance.html" />
    <link rel="prev" title="Introduction" href="../Introduction/Introduction.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../ThreadAdvance/ThreadAdvance.html" title="Thread Advance"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../Introduction/Introduction.html" title="Introduction"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">GPU Programming</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="cuda-intro">
<h1>CUDA Intro<a class="headerlink" href="#cuda-intro" title="Permalink to this headline">¶</a></h1>
<p>Before you proceed to the next example, please download the following files and place them outside of your source code folder.</p>
<div class="section" id="an-example-of-vector-addition">
<h2>An Example of Vector Addition<a class="headerlink" href="#an-example-of-vector-addition" title="Permalink to this headline">¶</a></h2>
<p>Vector Addition source file:
<a class="reference download internal" href="../_downloads/VA-GPU-11.cu"><tt class="xref download docutils literal"><span class="pre">VA-GPU-11.cu</span></tt></a></p>
<div class="section" id="the-device-code">
<h3>The Device Code<a class="headerlink" href="#the-device-code" title="Permalink to this headline">¶</a></h3>
<p>As you may notice in your background reading about CUDA programming, the program executes in two separated places. One is called host, another is called device. In our example, the add() function executes on the device (our GPU) and the rest of the C program executes on our CPU.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span> <span class="p">)</span> <span class="p">{</span>    
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="c1">// loop over all the element in the vector</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">){</span>
        <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
        <span class="n">tid</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span> <span class="c1">// we are using one thread in one block</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>As shown in the code block above, we need to add a <strong>__global__</strong> qualifier to the function name of the original C code in order to let function add() execute on a device.</p>
</div>
<div class="section" id="the-host-code">
<h3>The Host Code<a class="headerlink" href="#the-host-code" title="Permalink to this headline">¶</a></h3>
<div class="highlight-c"><div class="highlight"><pre><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span> <span class="kt">void</span> <span class="p">)</span> <span class="p">{</span>

    <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="o">*</span><span class="n">c</span><span class="p">;</span>
    <span class="kt">int</span> <span class="o">*</span><span class="n">dev_a</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_b</span><span class="p">,</span> <span class="o">*</span><span class="n">dev_c</span><span class="p">;</span>

    <span class="c1">// allocate memory on the CPU</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>
    <span class="n">b</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>

    <span class="c1">// fill arrays &#39;a&#39; and &#39;b&#39; on the CPU</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">N</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">i</span><span class="p">;</span>
        <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>As shown in the code block above, we first need to declare pointers. Notice that we declared two sets of pointers, one set is used to store data on host memory, another is used to store data on the device memory.</p>
<div class="section" id="the-event-api">
<h4>The <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">Event API</a><a class="headerlink" href="#the-event-api" title="Permalink to this headline">¶</a></h4>
<p>Before we go any further, we need to first learn ways of measuring performance in CUDA runtime. The tool we use to measure the time GPU spends on a task is CUDA <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">Event API</a>. If you are C programming language veteran you may ask the question: why don&#8217;t we use the the timing functions in standard C, such as <em>clock()</em> or <em>timeval</em> structure, to perform this task? Well, this is a really good question.</p>
<p>The fundamental motivation of using <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">Event API</a> instead of timing functions in standard C lies on the difference between CPU and GPU computation. To be more specific, GPU is a companion computation device, which means every time CPU has to call GPU to do computations. However, when GPU is doing computation, CPU does not wait for it to finish its task, instead CPU will continue to execute next line of code while GPU is still working on previous call. This <em>asynchronous</em> feature of GPU computation structure leads to possible inaccuracy in standard C timing functions. Therefore, <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">Event API</a> become needed.</p>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// define events in the field</span>
    <span class="n">cudaEvent_t</span>     <span class="n">start</span><span class="p">,</span> <span class="n">stop</span><span class="p">;</span>
    <span class="c1">// create two events we need</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventCreate</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">start</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventCreate</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">stop</span> <span class="p">)</span> <span class="p">);</span>
    <span class="c1">// instruct the runtime to record the event start. </span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventRecord</span><span class="p">(</span> <span class="n">start</span><span class="p">,</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">);</span>
</pre></div>
</div>
<p>The first step of using event is declaring the event. In this example. we declared two events, one called start, which will record the start event and another called stop, which will record the stop event. After declaring the events, we can use the command <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__EVENT_ga324d5ce3fbf46899b15e5e42ff9cfa5.html#ga324d5ce3fbf46899b15e5e42ff9cfa5">cudaEventRecord()</a> to record a event. You can think of record a event as initializing it. You may noticed that we pass this command a second argument (0 in this case). In our example, this argument is actually useless. However, if you are really interested in this, you can read more about CUDA stream.</p>
</div>
<div class="section" id="the-cudamalloc-function">
<h4>The <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_gc63ffd93e344b939d6399199d8b12fef.html#gc63ffd93e344b939d6399199d8b12fef">cudaMalloc()</a> Function<a class="headerlink" href="#the-cudamalloc-function" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// allocate memory on the GPU</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaMalloc</span><span class="p">(</span> <span class="p">(</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_a</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaMalloc</span><span class="p">(</span> <span class="p">(</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_b</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaMalloc</span><span class="p">(</span> <span class="p">(</span><span class="kt">void</span><span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">dev_c</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">)</span> <span class="p">);</span>
</pre></div>
</div>
<p>Just like standard C programming language, you need to allocate memory for variables before you start to use them. The command <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_gc63ffd93e344b939d6399199d8b12fef.html#gc63ffd93e344b939d6399199d8b12fef">cudaMalloc()</a>, similar to <em>malloc()</em> command in standard C, tells the CUDA runtime to allocate the memory on the device (Memory of GPU), instead of on the host (Memory of CPU). The first argument is a pointer points to where you want to hold the address of the newly allocated memory.</p>
<p>For some reasons, you are not allowed to modify memory allocated on the device (GPU) from host directly in CUDA C programming language. Instead, you need to use two other method to access device memory. You can do it by either using device pointers in the device code, or you can use the <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy()</a> method.</p>
<p>The way to use pointers in the device code is exactly the same as we did in the host code. In other words, pointer in CUDA C is exactly the same as Standard C. However, there is one thing you need to pay attention to, host pointers can only access memory (usually CPU memory) from host code, you cannot access device memory directly. On the other hand, device pointers can only access memory (usually GPU memory) from device code as well.</p>
</div>
<div class="section" id="the-cudamemcpy-function">
<h4>The <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy()</a> Function<a class="headerlink" href="#the-cudamemcpy-function" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// copy arrays &#39;a&#39; and &#39;b&#39; to the GPU</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaMemcpy</span><span class="p">(</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span>
                              <span class="n">cudaMemcpyHostToDevice</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaMemcpy</span><span class="p">(</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span>
                              <span class="n">cudaMemcpyHostToDevice</span> <span class="p">)</span> <span class="p">);</span>
</pre></div>
</div>
<p>As mentioned in last section, We can also use <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy()</a> from host code to access memory on a device. <strong>This command is the typical way of transferring data between host and device.</strong> Again this call is similar to standard C call <em>memcpy()</em>, but requires more parameters. The first argument identifies the destination pointer; the second identifies the source pointer. The last parameter to the call is <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html#gg18fa99055ee694244a270e4d5101e95b1a03d03a676ea8ec51b9b1e193617568">cudaMemcpyHostToDevice</a>, telling the runtime that the source pointer is a host pointer and the destination pointer is a device pointer.</p>
</div>
<div class="section" id="the-kernel-invocation">
<h4>The Kernel Invocation<a class="headerlink" href="#the-kernel-invocation" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// kernel invocation code</span>
    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span> <span class="p">);</span>
</pre></div>
</div>
<p>The following line is the call for device code from host code. You may notice that this call is similar to a normal function call but has additional code in it. We will talk about what they represent in later examples. At this point all you need to know is that they are they are telling the GPU to use only one thread to execute the program.</p>
</div>
<div class="section" id="more-cudamemcpy-function">
<h4>More <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">cudaMemcpy()</a> Function<a class="headerlink" href="#more-cudamemcpy-function" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// copy array &#39;c&#39; back from the GPU to the CPU</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaMemcpy</span><span class="p">(</span> <span class="n">c</span><span class="p">,</span> <span class="n">dev_c</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">),</span>
                              <span class="n">cudaMemcpyDeviceToHost</span> <span class="p">)</span> <span class="p">);</span>
</pre></div>
</div>
<p>In previous section we have seen how CUDA runtime transfer data from Host to Device, this time we will see how to transfer data back to host. Notice that this time device memory is source and host memory is destination. Therefore, we are using argument <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html#gg18fa99055ee694244a270e4d5101e95b1a03d03a676ea8ec51b9b1e193617568">cudaMemcpyDeviceToHost</a>.</p>
</div>
<div class="section" id="timing-using-event-api">
<h4>Timing using <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">Event API</a><a class="headerlink" href="#timing-using-event-api" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// get stop time, and display the timing results</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventRecord</span><span class="p">(</span> <span class="n">stop</span><span class="p">,</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventSynchronize</span><span class="p">(</span> <span class="n">stop</span> <span class="p">)</span> <span class="p">);</span>
    <span class="kt">float</span>   <span class="n">elapsedTime</span><span class="p">;</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventElapsedTime</span><span class="p">(</span> <span class="o">&amp;</span><span class="n">elapsedTime</span><span class="p">,</span>
                                        <span class="n">start</span><span class="p">,</span> <span class="n">stop</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span> <span class="s">&quot;Time to generate:  %3.1f ms</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">elapsedTime</span> <span class="p">);</span>
</pre></div>
</div>
<p>We have seen how to declare and record a <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_g48efa06b81cc031b2aa6fdc2e9930741.html#g48efa06b81cc031b2aa6fdc2e9930741">Event API</a> in CUDA C, but have not elaborate how to use such tool to measure performance. The basic idea is that we first declare event start and event stop. Then at the beginning of the program we record event start and at the end of the program we record event stop. The last step is to calculate the elapsed time between two events.</p>
<p>As shown in the code block above, we again use command <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__EVENT_ga324d5ce3fbf46899b15e5e42ff9cfa5.html#ga324d5ce3fbf46899b15e5e42ff9cfa5">cudaEventRecord()</a> to instruct the runtime to record the event stop. Then we proceed to the last step, which is get elapsed time using command <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__EVENT_g14c387cc57ce2e328f6669854e6020a5.html#g14c387cc57ce2e328f6669854e6020a5">cudaEventElapsedTime()</a>.</p>
<p>However, there is still a problem with timing GPU code in this way. The CUDA C programming language, though is derived from standard C, has many characteristics that is different from standard C. We have mentioned in previous section that CUDA C is asynchronous. This is a example to jog your memory. Suppose we are running a program to do matrices multiplication, and host calls the GPU to do the computation. As GPU begins executing our code, the CPU proceeds to the next line of code instead of waiting GPU to finish its work. If we want the stop event to record the correct time, we need to make sure that our event is recorded after the GPU finishes everything prior to the call to <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__EVENT_ga324d5ce3fbf46899b15e5e42ff9cfa5.html#ga324d5ce3fbf46899b15e5e42ff9cfa5">cudaEventRecord()</a>. To address this problem, CUDA C calls the function <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__EVENT_g08241bcf5c5cb686b1882a8492f1e2d9.html#g08241bcf5c5cb686b1882a8492f1e2d9">cudaEventSynchronize()</a> to synchronize the stop event.</p>
<p>The <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__EVENT_g08241bcf5c5cb686b1882a8492f1e2d9.html#g08241bcf5c5cb686b1882a8492f1e2d9">cudaEventSynchronize()</a> function is essentially instructing the runtime to create a barrier to block CPU from executing further instructions until the GPU has reached the stop event.</p>
<p>Another caveat worth mentioning is that CUDA events are implemented directly on the GPU. Therefore they cannot be used for timing device code mixed with host code. In other words, you will get unreliable results if you attempt to use CUDA events to time more than kernel executions and memory copies involving the device.</p>
<p>you should include and only include kernel execution and memory copies involving the device in between start event and stop event. Anything more included could lead to unreliable results.</p>
</div>
<div class="section" id="the-cudafree-function">
<h4>The <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_gb17fef862d4d1fefb9dba35bd62a187e.html#gb17fef862d4d1fefb9dba35bd62a187e">cudaFree()</a> Function<a class="headerlink" href="#the-cudafree-function" title="Permalink to this headline">¶</a></h4>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// free memory allocated on the GPU</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaFree</span><span class="p">(</span> <span class="n">dev_a</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaFree</span><span class="p">(</span> <span class="n">dev_b</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaFree</span><span class="p">(</span> <span class="n">dev_c</span> <span class="p">)</span> <span class="p">);</span>

    <span class="c1">// destroy events to free memory</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventDestroy</span><span class="p">(</span> <span class="n">start</span> <span class="p">)</span> <span class="p">);</span>
    <span class="n">HANDLE_ERROR</span><span class="p">(</span> <span class="n">cudaEventDestroy</span><span class="p">(</span> <span class="n">stop</span> <span class="p">)</span> <span class="p">);</span>
</pre></div>
</div>
<p>When you are reading the section about <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_gc63ffd93e344b939d6399199d8b12fef.html#gc63ffd93e344b939d6399199d8b12fef">cudaMalloc()</a>, It may occur to you that we might a call different from the call <em>free()</em> to free memory on the device. You are absolutely right. To free memory allocated on the device, we need to use command <a class="reference external" href="http://developer.download.nvidia.com/compute/cuda/4_2/rel/toolkit/docs/online/group__CUDART__MEMORY_gb17fef862d4d1fefb9dba35bd62a187e.html#gb17fef862d4d1fefb9dba35bd62a187e">cudaFree()</a> instead of free().</p>
<p>To finish up the code, we need to free memory allocate on the CPU as well.</p>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// free memory allocated on the CPU</span>
    <span class="n">free</span><span class="p">(</span> <span class="n">a</span> <span class="p">);</span>
    <span class="n">free</span><span class="p">(</span> <span class="n">b</span> <span class="p">);</span>
    <span class="n">free</span><span class="p">(</span> <span class="n">c</span> <span class="p">);</span>
</pre></div>
</div>
<p>You can add the following code verify whether the GPU has done the task correctly or not. This time we are using CPU to verify GPU&#8217;s work. We can do this in this problem due to small data size and simple computation.</p>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="vector-addition-with-blocks">
<h2>Vector Addition with Blocks<a class="headerlink" href="#vector-addition-with-blocks" title="Permalink to this headline">¶</a></h2>
<p>We have learned some basic concepts in CUDA C in our last example. Starting from this example, you will begin to learn how to write CUDA language that will explore the potential of our GPU card how to measure the performance.</p>
<p>Vector Addition with Blocks source file:
<a class="reference download internal" href="../_downloads/VA-GPU-N1.cu"><tt class="xref download docutils literal"><span class="pre">VA-GPU-N1.cu</span></tt></a></p>
<div class="section" id="block">
<h3>Block<a class="headerlink" href="#block" title="Permalink to this headline">¶</a></h3>
<p>Recall that in the previous example, we use the code</p>
<div class="highlight-c"><div class="highlight"><pre>    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span> <span class="p">);</span>
</pre></div>
</div>
<p>to call for device kernels and we left those two numbers in the triple angle brackets unexplained. Well, the first number tells the kernel how many parallel blocks we would like to use to execute the instruction. For example, if we launch the kernel &lt;&lt;&lt;16,1&gt;&gt;&gt;, we are essentially creating 16 copies of the kernel and running them in parallel. We call each of these parallel invocations a block.</p>
<p>Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid. Why do we need two-dimensional or even three-dimensional grid? why can&#8217;t we just stick with one-dimensional? Well, it turned out that for problems with two or more dimensional domains, such as matrices multiplication or image processing (don&#8217;t forget the reason GPU been exist is to process image faster), it is often convenient and more efficient to use two or more dimensional indexing. Right now, nVidia GPUs that support CUDA structure can assign up to 65536 blocks in each dimension of the grid, that is in total 65536 * 65536 * 65536 blocks in a grid.</p>
<p>However, as we are doing vector addition, a one-dimensional process, we don&#8217;t need to use two-dimensional grid. However, don&#8217;t get disappointed, we will use higher dimensional grid in later examples.</p>
<p>Some of the books may refer grid in CUDA has only one and two-dimensions. This is incorrect because the official CUDA programming guide specifically addressed that grid can be three-dimensional.</p>
</div>
<div class="section" id="id1">
<h3>The Device Code<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="highlight-c"><div class="highlight"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span> <span class="p">)</span> <span class="p">{</span>

    <span class="c1">// keep track of the index</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

    <span class="k">while</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
        <span class="n">tid</span> <span class="o">+=</span> <span class="n">numBlock</span><span class="p">;</span> <span class="c1">// shift by the total number of blocks</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This is the complete device code.</p>
<p>We have mentioned that there are one, two and three-dimensional grids. To index different blocks in a grid, we use the built-in variables CUDA runtime defines for us: blockIdx. blockIdx is a three-component vector, so that threads can be identified using one-dimensional, two-dimensional or three-dimensional index. To access different component in this vector, we use blockIdx.x, blockIdx.y and blockIdx.z.</p>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// keep track of the index</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</pre></div>
</div>
<p>Since we have multiple blocks doing the same task, we need to keep track of these blocks so that the kernel can pass right data to them and bring right data back. Since we have only 1 thread in each block, we can simply use blockIdx to track index.</p>
<div class="highlight-c"><div class="highlight"><pre>        <span class="n">tid</span> <span class="o">+=</span> <span class="n">numBlock</span><span class="p">;</span> <span class="c1">// shift by the total number of blocks</span>
</pre></div>
</div>
<p>Although we have multiple blocks (1 thread per block) working simultaneously after one block finish one computation, this does not necessary mean block will only perform one time of computation. Normally, we could have problem size that is larger than the number of blocks we have. Therefore, we need each block to perform more than one time of computation. We do this by adding a stride to the tid after the while loop finish one round. In this example, we want tid to shift to the next data point by the total number of blocks.</p>
</div>
<div class="section" id="id2">
<h3>The Host Code<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<div class="highlight-c"><div class="highlight"><pre>    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlock</span><span class="p">,</span><span class="n">numThread</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span> <span class="p">);</span>
</pre></div>
</div>
<p>Except kernel invocation part of the host code, everything else is the same. However, as we are calling <strong>numBlock</strong> and <strong>numThread</strong> in the code, we need to define them at the very beginning of the source code file.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cp">#define numThread 1 </span><span class="c1">// in this example we keep one thread in one block</span>
<span class="cp">#define numBlock 128 </span><span class="c1">// in this example we use multiple blocks</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="vector-addition-with-blocks-and-threads">
<h2>Vector Addition with Blocks and Threads<a class="headerlink" href="#vector-addition-with-blocks-and-threads" title="Permalink to this headline">¶</a></h2>
<p>Vector Addition with Blocks and Threads source file:
<a class="reference download internal" href="../_downloads/VA-GPU-NN.cu"><tt class="xref download docutils literal"><span class="pre">VA-GPU-NN.cu</span></tt></a></p>
<div class="section" id="threads">
<h3>Threads<a class="headerlink" href="#threads" title="Permalink to this headline">¶</a></h3>
<p>In the last example, we learned how to launch multiple blocks in CUDA C programs. This time, we will see how to split parallel blocks. CUDA runtime allow us to split block into threads. Recall that in the previous example, we use the code</p>
<div class="highlight-c"><div class="highlight"><pre>    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlock</span><span class="p">,</span><span class="n">numThread</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span> <span class="p">);</span>
</pre></div>
</div>
<p>to call for device kernels where numBlock is 128 and numThread remain as 1, the second number represents how many threads we want in each block.</p>
<p>Here comes the question, why do we need two sets of parallel organization system? Why do we need not only blocks in grid, but also threads in blocks? Is there any advantages in one over the other? Well, there are advantages that we will cover in later examples, so for now, please bear with us.</p>
<p>Just like blocks is organized in up to three-dimensional grid, threads can also be organized in one, two or three-dimensional blocks. Just like there is a limit on number of blocks in a grid, there is also a limit on number of threads in a block. Right now, for most of the high-end nVidia GPUs, this limit is 1024. Be really careful here, 1024 is the total number of threads in a block, <strong>not</strong> the limit <strong>per dimension</strong> like in the grid. Most of the nVidia GPUs that is two or three year old, the limit might be 512. You can query the maxThreadsPerBlock field of the device properties structure to find out which number you have.</p>
</div>
<div class="section" id="id3">
<h3>The Device Code<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<div class="highlight-c"><div class="highlight"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="nf">add</span><span class="p">(</span> <span class="kt">int</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span><span class="n">c</span> <span class="p">)</span> <span class="p">{</span>

    <span class="c1">// keep track of the index</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">numBlock</span><span class="p">;</span>

    <span class="k">while</span> <span class="p">(</span><span class="n">tid</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">];</span>
        <span class="n">tid</span> <span class="o">+=</span> <span class="n">numThread</span> <span class="o">*</span> <span class="n">numBlock</span><span class="p">;</span><span class="c1">// shift by the total number of thread in a grid</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This is the complete device code.</p>
<p>Just like we use CUDA built-in variables to index blocks in a grid, we use variable threadIdx to index threads in a block. threadIdx is also a three-component vector and you can access each of its element using threadIdx.x, threadIdx,y and threadIdx.z.</p>
<div class="highlight-c"><div class="highlight"><pre>    <span class="c1">// keep track of the index</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">numBlock</span><span class="p">;</span>
</pre></div>
</div>
<p>The thread handles the data at its thread id. Recall that earlier we are using tid = blockIdx.x only. Now, as we are using multiple threads per block, we have to keep track of not only blockId, but also the threadId as well.</p>
<div class="highlight-c"><div class="highlight"><pre>        <span class="n">tid</span> <span class="o">+=</span> <span class="n">numThread</span> <span class="o">*</span> <span class="n">numBlock</span><span class="p">;</span><span class="c1">// shift by the total number of thread in a grid</span>
</pre></div>
</div>
<p>Since we have multiple threads in multiple blocks working simultaneously, after one thread in one block finish one computation, we want it to shift to the next data point by the total number of threads in the system. in this example, total number of threads is number of blocks times threads per block.</p>
</div>
<div class="section" id="id4">
<h3>The Host Code<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<div class="highlight-c"><div class="highlight"><pre>    <span class="n">add</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlock</span><span class="p">,</span><span class="n">numThread</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span> <span class="n">dev_a</span><span class="p">,</span> <span class="n">dev_b</span><span class="p">,</span> <span class="n">dev_c</span> <span class="p">);</span>
</pre></div>
</div>
<p>Except kernel invocation part of the host code, everything else is the same. However, as we are calling <strong>numBlock</strong> and <strong>numThread</strong> in the code, we need to define them at the very beginning of the source code file.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cp">#define numThread 128 </span><span class="c1">// in this example we use multiple threads</span>
<span class="cp">#define numBlock 128 </span><span class="c1">// in this example keep on using multiple blocks</span>
</pre></div>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">CUDA Intro</a><ul>
<li><a class="reference internal" href="#an-example-of-vector-addition">An Example of Vector Addition</a><ul>
<li><a class="reference internal" href="#the-device-code">The Device Code</a></li>
<li><a class="reference internal" href="#the-host-code">The Host Code</a><ul>
<li><a class="reference internal" href="#the-event-api">The Event API</a></li>
<li><a class="reference internal" href="#the-cudamalloc-function">The cudaMalloc() Function</a></li>
<li><a class="reference internal" href="#the-cudamemcpy-function">The cudaMemcpy() Function</a></li>
<li><a class="reference internal" href="#the-kernel-invocation">The Kernel Invocation</a></li>
<li><a class="reference internal" href="#more-cudamemcpy-function">More cudaMemcpy() Function</a></li>
<li><a class="reference internal" href="#timing-using-event-api">Timing using Event API</a></li>
<li><a class="reference internal" href="#the-cudafree-function">The cudaFree() Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#vector-addition-with-blocks">Vector Addition with Blocks</a><ul>
<li><a class="reference internal" href="#block">Block</a></li>
<li><a class="reference internal" href="#id1">The Device Code</a></li>
<li><a class="reference internal" href="#id2">The Host Code</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vector-addition-with-blocks-and-threads">Vector Addition with Blocks and Threads</a><ul>
<li><a class="reference internal" href="#threads">Threads</a></li>
<li><a class="reference internal" href="#id3">The Device Code</a></li>
<li><a class="reference internal" href="#id4">The Host Code</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../Introduction/Introduction.html"
                        title="previous chapter">Introduction</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../ThreadAdvance/ThreadAdvance.html"
                        title="next chapter">Thread Advance</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../ThreadAdvance/ThreadAdvance.html" title="Thread Advance"
             >next</a> |</li>
        <li class="right" >
          <a href="../Introduction/Introduction.html" title="Introduction"
             >previous</a> |</li>
        <li><a href="../index.html">GPU Programming</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>