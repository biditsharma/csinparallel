% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,openany,oneside]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}


\title{A Theory of Parallel Computation}
\date{July 26, 2012}
\release{}
\author{CSInParallel Project}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\def\PYG@tok@gd{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\def\PYG@tok@gu{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\def\PYG@tok@gt{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\def\PYG@tok@gs{\let\PYG@bf=\textbf}
\def\PYG@tok@gr{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\def\PYG@tok@cm{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@vg{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@m{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mh{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@cs{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\colorbox[rgb]{1.00,0.94,0.94}{##1}}}
\def\PYG@tok@ge{\let\PYG@it=\textit}
\def\PYG@tok@vc{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@il{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@go{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\def\PYG@tok@cp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@gi{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\def\PYG@tok@gh{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\def\PYG@tok@ni{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\def\PYG@tok@nl{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\def\PYG@tok@nn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@no{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\def\PYG@tok@na{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@nb{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\def\PYG@tok@nd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\def\PYG@tok@ne{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@nf{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\def\PYG@tok@si{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\def\PYG@tok@s2{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@vi{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@nt{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\def\PYG@tok@nv{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\def\PYG@tok@s1{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@gp{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@sh{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@ow{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@sx{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\def\PYG@tok@bp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c1{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@kc{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@c{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\def\PYG@tok@mf{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@err{\def\PYG@bc##1{\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{##1}}}
\def\PYG@tok@kd{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@ss{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\def\PYG@tok@sr{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\def\PYG@tok@mo{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@mi{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\def\PYG@tok@kn{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@o{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\def\PYG@tok@kr{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@s{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@kp{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@w{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\def\PYG@tok@kt{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\def\PYG@tok@sc{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sb{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@k{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\def\PYG@tok@se{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\def\PYG@tok@sd{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}



\chapter{Background}
\label{Background/Background::doc}\label{Background/Background:a-theory-of-parallel-computation}\label{Background/Background:background}\begin{itemize}
\item {} 
DFAs, NFAs, pushdown automata, Turing machines... All are mathematical entities that model computation. These abstract systems have concrete, practical applications in computer science (CS).

For example, deterministic finite automata (DFAs) are associated with regular expressions, which computer   programs that involve pattern matching frequently rely on. Also, knowing theoretical results such as the inability of any computation to determine whether or not another computation will stop (Halting Problem) can keeps programmers from attempting to write impossible computer programs.

\item {} 
Automata represent one approach to mathematically modeling computation. There are others.

For example, the mathematical logician Alonzo Church created a formalism of computation based on functions in the 1930s, called the \emph{:math:{}`lambda{}`-calculus}. The key notion in this approach is an operator (i.e., function) called $\lambda$ that is capable of generating other functions.

One of the earliest high-level programming languages, LISP (for LISt Processing language, 1959), is a practical computer implementation of the $\lambda$-calculus. LISP was designed originally for research in \emph{artificial intelligence (AI)}, a field in CS that perpetually seeks to extend the capabilities of computers to carry out tasks that humans can do. Scheme and Clojure are some contemporary programming languages descended from the original LISP, and and other widely used ``functional'' programming languages such as ML and Haskell are based on the $\lambda$-calculus. Programmers use these languages to develop useful applications, and researchers use them to explore new frontiers in computing.

\item {} 
From a theoretical viewpoint, the $\lambda$-calculus embodies all essential features of functional computation. This holds because the relationship between ``inputs'' (domain values in Mathematics, arguments/parameters in programming) and ``outputs'' (range values in Math, return values in programming) from functions expresses everything in a purely functional system of computations (no ``state changes''), and $\lambda$-calculus is the mathematical theory of functions considered entirely according to their ``inputs'' and ``outputs.''

In fact, it can be proven that any other foundation for \emph{functional} computation, such as Turing machines (which can express any type of computation), will have exactly the same expressive power for functional computation as the $\lambda$-calculus {[}Pierce 95{]}.

\item {} 
However, all of the computational models we've mentioned so far (Turing machines, $\lambda$-calculus, etc.) are for \emph{sequential computations} only. This means that we assume only a single computational entity. Until a few years ago, it was reasonable to assume that only one computational processor would be available for most computations, because most computers had only one computational circuit for carrying out instructions.

\item {} 
Nowadays, retailers sell only \emph{multi-core} computers (i.e., computers having multiple circuits for carrying out instructions) on the commodity market, and hardware manufacturers such as Intel and AMD no longer produce chips with only one computational processor. This results from computer engineering having reached certain limitations on performance for individual processors (related to electrical power consumption, access to computer memory, and parallel speedup capabilities with a single processor).

\item {} 
Consequently, the only way to continue improving the performance of computers going forward is to use \emph{parallel computing}, in which multiple computer actions are carried out physically at the same time. Parallel computing (or \emph{parallelism}) can be accomplished by writing programs that use multiple computational cores at the same time, and/or by running multiple cooperating programs on multiple computers.

\item {} 
Some computations are easy to parallelize. For example, a computation may involve applying exactly the same program steps to multiple independent input data sets, in which case we can perform parallel processing by executing that series of program steps on multiple processors (i.e., multiple cores and/or computers), and submitting different data sets to different processors. We call this strategy \emph{data parallelism}. Some authors refer to such computations as being \emph{embarassingly parallel}.

\item {} 
Other types of computations may be parallelizable without being data parallelizable. For example, matrix multiplication requires combining the rows and columns of rectangular arrays of numbers in ways that require accessing each number multiple times, in different groupings. Parallelization strategies for matrix multiplication exist, such as multiplying submatrices formed by subdividing the original matrices, then combining those results appropriately. However, those strategies are more complex than simple data parallelism.

\item {} 
Many computations require parallelizing according to the computational steps instead of (or in addition to) parallelizing according to the data. When a computation has multiple processors carrying out different sequences of computational steps in order to accomplish its work, we say that computation has \emph{task parallelism}.

For example, imagine a computation that extracts certain elements from a body of text (e.g., proper names), then sorts those elements, and finally removing duplications. With multiple processors, one might program one processor to extract those elements, another to perform the sorting operation, and a third to remove the duplications. In effect, we have an assembly line of processes, also called a \emph{pipeline} by computer scientists.

\item {} 
Computer scientists have found other computations exceedingly difficult to parallelize effectively. Notably, nobody knows how to parallelize finite state machines (FSMs) well, as a general class of computations. {[}View from Berkeley 06, p.16{]}

\item {} 
We can easily imagine how to construct a mathematical model of computation for simple data parallelism from a model of computation for the sequential case of that same computation, by replicating the sequential model. This approach seems promising as long as we can assume that those multiple parallel computations do not need to interact with each other in any way.

\item {} 
However, more complicated forms of parallelism that involve multiple processes interacting in various ways, such as the task parallelism example of pipelining, requires a mathematical model of parallel computation capable of expressing those interactions between processes.

\end{itemize}

The $\pi$-calculus, introduced in the next section, is an example of such a model of parallel computation.


\chapter{The pi-calculus, informally}
\label{ThePiCalculus/ThePiCalculus:the-pi-calculus-informally}\label{ThePiCalculus/ThePiCalculus::doc}\begin{itemize}
\item {} 
A \emph{calculus} is a method or computation based on symbolic manipulation.
\begin{itemize}
\item {} 
In \emph{differential calculus}, symbolic manipulations involve an operator $\frac{d}{dx}$ that satisfies rules such as the following:

\end{itemize}
\begin{gather}
\begin{split}{d\over dx}(f+g) = ({d\over dx}f) + ({d\over dx}g)\\
{d\over dx}(f\cdot g) = ({d\over dx}f) \cdot g + f \cdot ({d\over dx} g)\end{split}\notag
\end{gather}\begin{itemize}
\item {} 
In \emph{integral calculus}, symbolic manipulations involve an operator $\int ...\,dx$ that satisfies rules such as the following:

\end{itemize}
\begin{gather}
\begin{split}\int f+g\,dx = \int f\,dx + \int g\,dx \\
\int f\cdot ({d\over dx}g)\,dx = f\cdot g - \int ({d\over dx}f)\cdot g\,dx\end{split}\notag
\end{gather}\begin{itemize}
\item {} 
In the $\lambda$-calculus, symbolic manipulations involve an operator $\lambda$ that has its manipulation rules, involving operations such as substitution of variables and applying functions to particular ``input'' values (function calls).

\end{itemize}

\item {} 
The operators and manipulation rules for a calculus may have useful concrete applications. For example, the differential calculus rules are satisfied by certain continuous mathematical functions, where the operator $\frac{d}{dx}$ represents the rate of change of those functions.

We typically think of $\frac{d}{dx}$ as operating on those functions, although the differential calculus rules are actually abstract and might be applied to other entities than functions.

\item {} 
The $\pi$-calculus has six operators. We think of them as operating on \emph{sequential processes}, i.e., running computer programs, although they are abstract and can be used without any particular concrete application.
\begin{itemize}
\item {} 
The \emph{concurrency operator} $P|Q$ (pronounced \emph{``P par Q''}) may be thought of as two processes \emph{P} and \emph{Q} executing in parallel (e.g., simultaneously on separate cores or on different computers).

\item {} 
The \emph{communication operators} may be thought of as sending and receiving messages from one process to another, across a communication channel that is used only by those two processes (i.e., a \emph{dedicated} communication channel in the language of CS).
\begin{itemize}
\item {} 
The output prefixing operator $\bar{c} \langle x \rangle . P$ (pronounced ``output x along c (then proceed with P)'') may be thought of as send a message \emph{x} across a channel \emph{c}, then proceeding to carry out process \emph{P}. Here, the channel c may be thought of as starting from this process to another.

Channels such as c may be set up between any two processes, but those two processes are then uniquely determined for c, and may not be changed later. Channels provide for a single communication in one direction only, specified when the channel is created.

The ``dot'' that appears in this notation indicates the boundary between one step and a next step in a process.

\item {} 
The \emph{input prefixing} operator $c(y).P$ (pronounced ``Input y along c'') may be thought of as waiting to receive a value from the channel \emph{c}, and once a value is received, storing that value in \emph{y} and proceeding to carry out process \emph{P}.

\end{itemize}

\item {} 
The replication operator $!P$ (``bang P'') may be thought of as creating a new process that is a duplicate of \emph{P}.

This sort of an operation is quite realistic in parallel computing. For example, a \emph{web server} is a program that receives requests for particular web pages and responds by sending those web pages. Web servers must be capable of handling multiple responses at the same time, because some web pages may take a significant amount of time to prepare and deliver, and it would be undesirable for one user to be delayed by another user's request. Therefore, a web server system may start up a new duplicate process for handling each request it receives. (Students who have studied operating systems will also see an analogy between the system call fork() and this replication operator.)

In the $\pi$-calculus, arbitrarily many duplicate processes are created by a single application of the replication operator.

\item {} 
The name allocation operator $(\nu{\it c}).{P}$ (``new c in P'') may be thought of as allocating a new constant communication channel \emph{c} within the process \emph{P}. The symbol $\nu$ is the Greek letter nu, pronounced like ``new''.

\item {} 
The alternative operator $P+Q$ (``P plus Q'') represents a process capable of taking part in exactly one alternative for communication. That process cannot make the choice among its alternatives; that selection among alternatives cannot be determined until it occurs, and once determined, any remaining alternatives have lost their chance and will never occur. (These restrictions on the alternative operator are not strictly necessary for $\pi$-calculus to work, but they simplify the theory.)

\end{itemize}

\item {} 
Besides these operations, there is one constant process 0 that does nothing. For example, we might write $\bar{c} \langle x \rangle . 0$ for a process that sends one message across a channel \emph{c}, then does nothing more.

\item {} 
Observe that all of the operations have to do with entire processes or with communication among processes. For example, there are no arithmetic operations such as multiplication, nor any operations related to applying (i.e., calling) functions, nor a way to store values in memory (assignment). The $\pi$-calculus is entirely concerned with communication among processes that are executing in parallel.

However, a theory of sequential processes, such as automata or the $\lambda$-calculus, can be used in conjunction with $\pi$-calculus in order to model both the parallelism of communication and sequential algorithms that take place between communication events.

In our examples, we will use an informal notation for the sequential aspects of a process for readability and convenience, but we will use the $\pi$-calculus formalism carefully in matters of parallelism and communication between processes.

Here is an example that models parallel computation using the $\pi$-calculus operators.

A \emph{client-server application} is a parallel system in which a program running on one computer, called the \emph{server} program, responds to requests that may be sent by programs that may be running on other computers, called \emph{client} programs. One example of a client-server application consists of web browsers (as clients) communicating with a web server (as server). However, there are other possibilities.

Consider a client-server application in which clients send requests to a server to apply a particular function to arguments that a client provides. In CS, this type of service is called \emph{remote procedure call (RPC)} (where ``procedure'' is another term for ``function''). RPC can enable clients to obtain the results of computations that those clients may be unable to compute on their own ``local'' hardware.

We will model RPC using a simple incrementing function.
\begin{itemize}
\item {} 
Here is C++ language code for the desired function.

\begin{Verbatim}[commandchars=\\\{\}]
int incr(int x)\PYGZob{}
  return x+1;
\PYGZcb{}
\end{Verbatim}

In case you are not a programmer: The first line indicates that the name of this function is incr, and that incr accepts one integer input (argument) named x and returns an integer value (as indicated by the int at the beginning of the line). The second line is a return statement, which specifies the output (``return value'') in terms of the input \emph{x}. This incrementing function returns the value x+1.

\item {} 
Here is a model for the server process:
\begin{gather}
\begin{split}!{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\end{split}\notag
\end{gather}
Here, the expression x+1 indicates sequential code, but the remainder of the expression uses $\pi$-calculus formalism. Observe that \emph{incr} is a channel for communicating to the server.

The use of the replication operator \emph{!} means that the entire remainder of the expression will be duplicated as many times as needed (in order to serve as many RPC requests as may arrive over time). We will consider the operator ! to have higher precedence that \textbar{} and + but lower precedence than the other $\pi$-calculus operators; this means that the expression above is equivalent to

\end{itemize}
\begin{gather}
\begin{split}!\big({\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\big)\end{split}\notag
\end{gather}\begin{itemize}
\item {} 
Here is C++ code for part of a client process:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{n}{y} \PYG{o}{=} \PYG{n}{incr}\PYG{p}{(}\PYG{l+m+mi}{17}\PYG{p}{)}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{Verbatim}

The dots represent steps to be taken after accomplishing a remote procedure call of \emph{incr}.

\begin{notice}{note}{Note:}
for non-programmers

In this C++ context, the symbol = is an \emph{assignment operator}, not an equality relation. The effect is to compute the result of applying the function incr with input value 17, and to store the output (return value) into computer memory under the name y.
\end{notice}

\begin{notice}{note}{Note:}
for everyone

The mathematical effect of making an assignment is substitution. In other words, the assignment of 18 to \emph{y} means that every occurrence of \emph{y} should be replaced by 18 throughout the program steps indicated by dots above.
\end{notice}

\item {} 
Here is a model for that client process, starting from the assignment above:
\begin{gather}
\begin{split}(\nu{\it a})\big(\overline{\it incr}\langle a,17 \rangle.{0}|{\it a}(y).{P}\big)\end{split}\notag
\end{gather}
Here, we create a new channel a and send that \emph{channel}, together with the value 17 that we want to increment, to the server, using the \emph{incr} channel from client to server. The channel a is for communicating from the server back to the same client. Observe that the output along \emph{incr} requesting the service takes place in parallel with the input along \emph{a} for delivering the result. (Of course, the first of these will necessarily occur before the second in this particular situation.) The entire client model consists of $\pi$-calculus expressions, except for the integer 17.

In this expression, the process \emph{P} represents steps the client will take after the remote procedure call of \emph{incr}. In other words, \emph{P} represents the dots in the client code above. We want RPC to cause \emph{y} to be replaced by 18 throughout \emph{P}.

\item {} 
We can now express a model for the entire client-server application.
\begin{gather}
\begin{split}!{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad (\nu{\it a})\big(\overline{\it incr}\langle a,17 \rangle.{0}\ |\ {\it a}(y).{P}\big)\end{split}\notag
\end{gather}
\end{itemize}

\item {} 
\href{http://en.wikipedia.org/wiki/Pi-calculus\#Structural\_congruence}{Structural congruence}, an equivalence relation on $\pi$-calculus expressions

\item {} 
\href{http://en.wikipedia.org/wiki/Pi-calculus\#Reduction\_semantics}{Reduction}, the ``calculus rules'' for $\pi$-calculus.

\item {} 
We can now use the definition of structural congruence and the reduction rules to give a formal proof that our $\pi$-calculus model of an \emph{incr} remote procedure call service produces the results we desired for it.

\end{itemize}
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{Verify}

\medskip

\begin{gather}
\begin{split}!{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad (\nu{\it a})\big(\overline{\it incr}\langle a,17 \rangle.{0}\ |\ {\it a}(y).{P}\big)\end{split}\notag\\\begin{split}\equiv \quad {\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad (\nu{\it a})\big(\overline{\it incr}\langle a,17 \rangle.{0}\ |\ {\it a}(y).{P}\big)\end{split}\notag\\\begin{split}\text{by structural congruence axiom for !}\end{split}\notag\\\begin{split}\text{(this dispenses a copy of the server process to use)}\end{split}\notag\\\begin{split}\equiv \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad (\nu{\it a})\big(\overline{\it incr}\langle a,17 \rangle.{0}\ |\ {\it a}(y).{P}\big)\end{split}\notag\\\begin{split}\text{by commutative law for }|\end{split}\notag\\\begin{split}\longrightarrow \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad \overline{\it c}\langle x+1 \rangle.{0}[c,x/a,17]\quad |\quad (\nu{\it a})\big(0\ |\ {\it a}(y).{P}\big)\end{split}\notag\\\begin{split}\text{by main reduction rule (this corresponds to sending a message)}\end{split}\notag\\\begin{split}\text{\textit{Note:} the notation [c,x/a,17] means to replace c by a and replace x by 17.}\end{split}\notag\\\begin{split}= \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad \overline{\it a}\langle 18 \rangle.{0}\quad |\quad (\nu{\it a})\big(0\ |\ {\it a}(y).{P}\big)\end{split}\notag\\\begin{split}\text{by definition of substitution and arithmetic}\end{split}\notag\\\begin{split}\equiv \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad \overline{\it a}\langle 18 \rangle.{0}\quad |\quad (\nu{\it a})\big({\it a}(y).{P}\ |\ 0\big)\end{split}\notag\\\begin{split}\text{by commutativity axiom for }|\end{split}\notag\\\begin{split}\equiv \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad \overline{\it a}\langle 18 \rangle.{0}\quad |\quad (\nu{\it a})\big({\it a}(y).{P}\big)\end{split}\notag\\\begin{split}\text{by identity axiom for }|\end{split}\notag\\\begin{split}\longrightarrow \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad 0 \quad |\quad (\nu{\it a})\big(P[y/18]\big)\end{split}\notag\\\begin{split}\text{by main reduction rule}\end{split}\notag\\\begin{split}\equiv \quad !{\it incr}(c,x).\overline{\it c}\langle x+1 \rangle.{0}\quad |\quad (\nu{\it a})\big(P[y/18]\big)\end{split}\notag\\\begin{split}\text{by associativity and identity for }|\end{split}\notag
\end{gather}\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}

In this proof, we started with the $\pi$-calculus expression for the server and the $\pi$-calculus expression for the client \emph{before} RPC, running in parallel. We ended with that same server we began with, and with a client process P \emph{after} RPC that has every occurrence of y replaced by 18 -- as desired.


\section{Exercises}
\label{ThePiCalculus/ThePiCalculus:exercises}\begin{enumerate}
\item {} 
If \emph{a} does not appear in \emph{P}, show that the last line above is structurally congruent to $!\textit{incr}(c,x).\bar{c}\langle x+1 \rangle.0 \quad | \quad P[y/18]$. Give a formal proof segment using the axioms and reduction rules.

\item {} 
Prove the following facts, using formal proofs from axiom and reduction rules, as in the verification of the RPC server above.

\end{enumerate}
\begin{gather}
\begin{split}0|P \equiv P \\
!P \equiv !P|P\end{split}\notag
\end{gather}\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Write a $\pi$-calculus expression that models an RPC system for an echo function, whose return value (output) is the same as its argument (input).

\end{enumerate}
\setbox0\vbox{
\begin{minipage}{0.95\linewidth}
\textbf{Hints:}

\medskip


Modify the RPC example for incr to serve echo instead. You can use the same client expression as before, but you will need to alter the server expression. Since the problem asks for a \emph{system} instead of only a server, your final answer should be a $\pi$-calculus expression for both the client and the server.

Here's a C++ programming language definition of echo, in case it's helpful.

\begin{Verbatim}[commandchars=\\\{\}]
int echo(int x)\PYGZob{}
  return x;
\PYGZcb{}
\end{Verbatim}
\end{minipage}}
\begin{center}\setlength{\fboxsep}{5pt}\shadowbox{\box0}\end{center}
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Examine the formal proof of the $\pi$-calculus model of an incr RPC service above, and indicate how to transform it to a proof of your $\pi$-calculus model of an echo RPC service in the previous problem.

\end{enumerate}
\begin{enumerate}
\setcounter{enumi}{4}
\item {} 
Consider the following $\pi$-calculus model.

\end{enumerate}
\begin{quote}
\begin{gather}
\begin{split}!\ {\it a}(v).\overline{\it v}\langle \hbox{\tt p()} \rangle.{0}\quad | \quad !\ (\nu{\it c})\overline{\it a}\langle c \rangle.{\it c}(y).{\hbox{\tt q($y$)}}\end{split}\notag
\end{gather}
Here, the notations p() and q(x) represent \emph{sequential} computer functions, and are not part of the $\pi$-calculus notation.
\begin{quote}

The function p() requires no arguments and sequentially produces a return value (output) when called (applied).

The function q(x) requires one argument (input) \emph{x} and performs some sequential operation with that argument when called.
\end{quote}

Answer the following questions:
\begin{enumerate}
\item {} 
This model formally describes an interaction between two programs running in parallel. Give an informal verbal description of what those two programs do and how they interact, according to the $\pi$-calculus expression above.

\item {} 
Perform $\pi$-calculus reduction and structural congruence to work through one interaction between these two programs.

\item {} 
You may give a thorough formal computation as in the proof of the incr RPC system, or you may skip or combine steps you feel comfortable with, as long as your work is accurate and expresses the calculation clearly.

\end{enumerate}
\end{quote}
\begin{enumerate}
\setcounter{enumi}{5}
\item {} 
Write your own $\pi$-calculus expressions for modeling each of the following parallel computations. (Each itemized sentence describes a separate problem to solve.) Note: No $\pi$-calculus replication operations are necessary for these problems, although you may optionally include it.

\end{enumerate}
\begin{enumerate}
\item {} 
One program uses channel \emph{a} to send an integer value 5 and a new channel to another program, and that latter program sends twice that integer value back to the first program along that new channel.

\item {} 
One program uses channel \emph{b} to send an integer value 10 and a new channel to another program; that second program uses channel \emph{c} to send twice that integer value and that same new channel to a third program; and that third program outputs three times the integer it receives along the channel it receives to the first program.

\end{enumerate}


\chapter{Examples and applications}
\label{ExamplesAndApplications/ExamplesAndApplications::doc}\label{ExamplesAndApplications/ExamplesAndApplications:examples-and-applications}\begin{itemize}
\item {} 
An exercise: Reduce the following $\pi$-calculus expression:

\end{itemize}
\begin{gather}
\begin{split}(\nu{\it x})\big(\overline{\it x}\langle z \rangle.{0}\ |\ {\it x}(y).\overline{\it y}\langle x \rangle.{\it x}(y).{0}\big) \quad|\quad {\it z}(v).\overline{\it v}\langle v \rangle.{0}\end{split}\notag
\end{gather}\begin{itemize}
\item {} 
Mobile communications {[}Milner 91{]}

\end{itemize}


\chapter{History; other formalisms}
\label{History/History::doc}\label{History/History:history-other-formalisms}\begin{itemize}
\item {} 
The $\pi$-calculus is an example of a \emph{process calculus}, i.e., a mathematical structure with a set of values and operations on those values in which processes are among the values and parallel composition (``running processes in parallel'') is a commutative and associative operation on processes.

\item {} 
The $\pi$-calculus was created in 1992 by Robin Milner, Joachim Parrow, and David Walker. Milner (1934-2010) was a famous British computer scientist known for inventing one of the early systems for automatic theorem proving (LCF) and for creating the functional programming language ML, in addition to the $\pi$-calculus.

\item {} 
The $\pi$-calculus extends Milner's earlier process calculus system called CCS (Calculus of Concurrent Systems) {[}Milner 80{]}.

\item {} 
Another famous British computer scientist, Tony Hoare, created a simlar system called CSP (Communicating Sequential Processes) {[}Hoare 85{]}, starting about 1978. CSP is the theoretical basis for the Occam language for parallel programming.

\item {} 
A major distinction between the $\pi$-calculus and predecessor systems is the $\pi$-calculus's ability to pass a channels from one process to another, along some other communication channel. This feature enables the system to model mobility, (e.g., cell phones) and changes in process structure.

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
