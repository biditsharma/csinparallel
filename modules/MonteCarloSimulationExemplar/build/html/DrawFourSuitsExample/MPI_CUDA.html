

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Monte Carlo Simulations on Other Parallel and Distributed Platforms &mdash; Monte Carlo Simulation Exemplar</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="top" title="Monte Carlo Simulation Exemplar" href="../index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li><a href="../index.html">Monte Carlo Simulation Exemplar</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="monte-carlo-simulations-on-other-parallel-and-distributed-platforms">
<h1>Monte Carlo Simulations on Other Parallel and Distributed Platforms<a class="headerlink" href="#monte-carlo-simulations-on-other-parallel-and-distributed-platforms" title="Permalink to this headline">¶</a></h1>
<div class="section" id="mpich-version">
<h2>MPICH Version<a class="headerlink" href="#mpich-version" title="Permalink to this headline">¶</a></h2>
<p>Converting the sequential code to use MPICH instead is only slightly more difficult
than using OpenMp. First we have to initialize MPI and calculate the number of tests
each instance needs to run:</p>
<div class="highlight-python"><pre>MPI_Init(&amp;argc, &amp;argv);
MPI_Comm_size(MPI_COMM_WORLD, &amp;size);
MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank);
...
testsToRun = numTests/size;
if(rank == size - 1) testsToRun += numTest % size //assign remaining tests to last worker</pre>
</div>
<p>Then, each instance runs a separate simulation loop as before. The other difference
is we need to explictly send the partial results from each worker to the Master instance.
The Master node sums the partial results and displays the answer:</p>
<div class="highlight-c++"><div class="highlight"><pre>		<span class="c1">//The Master collects results and sums them</span>
		<span class="k">if</span><span class="p">(</span><span class="n">rank</span> <span class="o">==</span> <span class="n">MASTER</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">mtype</span> <span class="o">=</span> <span class="n">FROM_MASTER</span><span class="p">;</span>
			<span class="n">total</span> <span class="o">=</span> <span class="n">partialSum</span><span class="p">;</span>
			<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">source</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">source</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="n">dest</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
				<span class="n">MPI_Recv</span><span class="p">(</span><span class="o">&amp;</span><span class="n">partialSum</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">mtype</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
				<span class="n">total</span> <span class="o">+=</span> <span class="n">partialSum</span><span class="p">;</span>
			<span class="p">}</span>
		<span class="c1">//Each work sends its partial sum to the Master</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="n">mtype</span> <span class="o">=</span> <span class="n">FROM_WORKER</span><span class="p">;</span>
			<span class="n">MPI_Send</span><span class="p">(</span><span class="o">&amp;</span><span class="n">partialSum</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MASTER</span><span class="p">,</span> <span class="n">mtype</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>

		<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="cuda-version">
<h2>Cuda Version<a class="headerlink" href="#cuda-version" title="Permalink to this headline">¶</a></h2>
<p>The changes needed to run the code with CUDA are slightly more involved. We have several
issues we have to deal with. First, we have to copy our results to and from the the device.
The other issue is that we can&#8217;t use the standard random library from the device.
Fortunately, Nvidia provides build in libraries for generating random numbers on a GPU.
You can get more information about the CUDA random library at:
<a class="reference external" href="http://docs.nvidia.com/cuda/curand/introduction.html">http://docs.nvidia.com/cuda/curand/introduction.html</a> We allocate space on the GPU for a
random number generator for each thread and the result:</p>
<div class="highlight-python"><pre>cudaMalloc((void **)&amp;devStates, sizeof(curandState)*BLOCK_SIZE);
cudaMalloc((void **)&amp;dev_result, sizeof(unsigned int));
cudaDeviceSynchronize();</pre>
</div>
<p>We define a new function, run_simulations(), which runs the simulations on the GPU:</p>
<div class="highlight-c++"><div class="highlight"><pre><span class="n">__global__</span> <span class="kt">void</span> <span class="n">run_simulations</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="n">curandState</span> <span class="o">*</span><span class="n">state</span><span class="p">,</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="o">*</span><span class="n">result</span><span class="p">)</span> <span class="p">{</span>
	<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">tot</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">thread2</span><span class="p">,</span> <span class="n">halfPoint</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">nTotalThreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
	<span class="n">__shared__</span> <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">sum</span><span class="p">[</span><span class="n">BLOCK_SIZE</span><span class="p">];</span> 
	<span class="kt">int</span> <span class="n">nTrials</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="n">nTotalThreads</span><span class="p">;</span> 
	<span class="k">if</span><span class="p">(</span><span class="n">id</span> <span class="o">==</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">nTrials</span> <span class="o">+=</span> <span class="n">n</span> <span class="o">%</span> <span class="n">nTotalThreads</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="n">curand_init</span><span class="p">(</span><span class="mi">1235</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">state</span><span class="p">[</span><span class="n">id</span><span class="p">]);</span>
	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">nTrials</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">if</span><span class="p">(</span><span class="n">testOneHand</span><span class="p">(</span><span class="o">&amp;</span><span class="n">state</span><span class="p">[</span><span class="n">id</span><span class="p">]))</span> <span class="p">{</span> 
			<span class="n">tot</span><span class="o">++</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span>
	
	<span class="n">sum</span><span class="p">[</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">tot</span><span class="p">;</span>
	<span class="n">__syncthreads</span><span class="p">();</span>
	<span class="k">while</span><span class="p">(</span><span class="n">nTotalThreads</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">halfPoint</span> <span class="o">=</span> <span class="p">(</span><span class="n">nTotalThreads</span> <span class="o">&gt;&gt;</span> <span class="mi">1</span><span class="p">);</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="n">halfPoint</span><span class="p">)</span> <span class="p">{</span>
			<span class="n">thread2</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">halfPoint</span><span class="p">;</span>
			<span class="n">sum</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sum</span><span class="p">[</span><span class="n">thread2</span><span class="p">];</span>
		<span class="p">}</span>
		<span class="n">__syncthreads</span><span class="p">();</span>
		<span class="n">nTotalThreads</span> <span class="o">=</span> <span class="n">halfPoint</span><span class="p">;</span>
	<span class="p">}</span>
	
	<span class="k">if</span><span class="p">(</span><span class="n">id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="o">*</span><span class="n">result</span> <span class="o">=</span> <span class="n">sum</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
	<span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>From this function, we call the other helper function, so we add the __device__
directive to them to allow them to be called on the GPU:</p>
<p>Here is the source code for this section:</p>
<p><a class="reference download internal" href="../_downloads/drawFourSuitsMPI.cpp"><tt class="xref download docutils literal"><span class="pre">drawFourSuitsMPI.cpp</span></tt></a>
<a class="reference download internal" href="../_downloads/drawFourSuitsCuda.cu"><tt class="xref download docutils literal"><span class="pre">drawFourSuitsCuda.cu</span></tt></a></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Monte Carlo Simulations on Other Parallel and Distributed Platforms</a><ul>
<li><a class="reference internal" href="#mpich-version">MPICH Version</a></li>
<li><a class="reference internal" href="#cuda-version">Cuda Version</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li><a href="../index.html">Monte Carlo Simulation Exemplar</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>