

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Data Decomposition Algorithm Strategies and Related Coordination Strategies &mdash; Parallel Patternlets</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="top" title="Parallel Patternlets" href="../index.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li><a href="../index.html">Parallel Patternlets</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="data-decomposition-algorithm-strategies-and-related-coordination-strategies">
<h1>Data Decomposition Algorithm Strategies and Related Coordination Strategies<a class="headerlink" href="#data-decomposition-algorithm-strategies-and-related-coordination-strategies" title="Permalink to this headline">¶</a></h1>
<div class="section" id="shared-data-decomposition-algorithm-strategy-chunks-of-data-per-thread-using-a-parallel-for-loop-implementation-strategy">
<h2>6. Shared Data Decomposition Algorithm Strategy:  chunks of data per thread using a parallel for loop implementation strategy<a class="headerlink" href="#shared-data-decomposition-algorithm-strategy-chunks-of-data-per-thread-using-a-parallel-for-loop-implementation-strategy" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/06.parallelLoop-equalChunks/parallelLoopEqualChunks.c</em></p>
<p><em>Build inside 06.parallelLoop-equalChunks directory:</em></p>
<div class="highlight-python"><pre>make parallelLoopEqualChunks</pre>
</div>
<p><em>Execute on the command line inside 06.parallelLoop-equalChunks directory:</em></p>
<div class="highlight-python"><pre>./parallelLoopEqualChunks 4
Replace 4 with other values for the number of threads, or leave off</pre>
</div>
<p>An iterative for loop is a remarkably common pattern in all programming, primarily used to
perform a calculation N times, often over a set of data containing N elements, using each
element in turn inside the for loop.  If there are no dependencies between the calculations
(i.e. the order of them is not important), then the code inside the loop can be split
between forked threads.  When doing this, a decision the programmer needs to make is to
decide how to partition the work between the threads by answering this question:</p>
<ul class="simple">
<li>How many and which iterations of the loop will each thread complete on its own?</li>
</ul>
<p>We refer to this as the <strong>data decomposition</strong> pattern because we are decomposing the
amount of work to be done (typically on a set of data) across multiple threads.
In the following code, this is done in OpenMP using the <em>omp parallel for</em> pragma
just in front of the for statement (line 27) in the following code.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* parallelLoopEqualChunks.c</span>
<span class="cm"> * ... illustrates the use of OpenMP&#39;s default parallel for loop in which</span>
<span class="cm"> *  	threads iterate through equal sized chunks of the index range</span>
<span class="cm"> *	(cache-beneficial when accessing adjacent memory locations).</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./parallelLoopEqualChunks [numThreads]</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise</span>
<span class="cm"> * - Compile and run, comparing output to source code</span>
<span class="cm"> * - try with different numbers of threads, e.g.: 2, 3, 4, 6, 8</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;    </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;stdlib.h&gt;   </span><span class="c1">// atoi()</span>
<span class="cp">#include &lt;omp.h&gt;      </span><span class="c1">// OpenMP</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">omp_set_num_threads</span><span class="p">(</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">);</span>
    <span class="p">}</span>

    <span class="cp">#pragma omp parallel for  </span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">REPS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Thread %d performed iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> 
                 <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
<p>Once you run this code, verify that the default behavior for this pragma is this
sort of decomposition of iterations of the loop to threads, when you set the
number of threads to 4 on the command line:</p>
<img alt="../_images/ParalleFor_Chunks-4_threads-1.png" src="../_images/ParalleFor_Chunks-4_threads-1.png" />
<p>What happens when the number of iterations (16 in this code) is not evenly divisible by the number of threads?  Try several cases to be certain how the compiler splits up the work.
This type of decomposition is commonly used when accessing data that is stored in
consecutive memory locations (such as an array) that might be cached by each thread.</p>
</div>
<div class="section" id="shared-data-decomposition-algorithm-strategy-one-iteration-per-thread-in-a-parallel-for-loop-implementation-strategy">
<h2>7. Shared Data Decomposition Algorithm Strategy:  one iteration per thread in a parallel for loop implementation strategy<a class="headerlink" href="#shared-data-decomposition-algorithm-strategy-one-iteration-per-thread-in-a-parallel-for-loop-implementation-strategy" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/07.parallelLoop-chunksOf1/parallelLoopChunksOf1.c</em></p>
<p><em>Build inside 07.parallelLoop-chunksOf1 directory:</em></p>
<div class="highlight-python"><pre>make parallelLoopChunksOf1</pre>
</div>
<p><em>Execute on the command line inside 07.parallelLoop-chunksOf1 directory:</em></p>
<div class="highlight-python"><pre>./parallelLoopChunksOf1 4
Replace 4 with other values for the number of threads, or leave off</pre>
</div>
<p>You can imagine other ways of assigning threads to iterations of a loop besides that
shown above for four threads and 16 iterations.  A simple decomposition sometimes used
when your loop is not accessing consecutive memory locations would be to let each
thread do one iteration, up to N threads, then
start again with thread 0 taking the next iteration.  This is declared in OpenMP
using the pragma on line 31 of the following code.  Also note that the commented code
below it is an alternative explicit way of doing it.  The schedule clause is the preferred style
when using OpenMP and is more versatile, because you can easily change the <cite>chunk size</cite>
that each thread will work on.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* parallelLoopChunksOf1.c</span>
<span class="cm"> * ... illustrates how to make OpenMP map threads to </span>
<span class="cm"> *	parallel loop iterations in chunks of size 1</span>
<span class="cm"> *	(use when not accesssing memory).</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./parallelLoopChunksOf1 [numThreads]</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * 1. Compile and run, comparing output to source code,</span>
<span class="cm"> *    and to the output of the &#39;equal chunks&#39; version.</span>
<span class="cm"> * 2. Uncomment the &quot;commented out&quot; code below,</span>
<span class="cm"> *    and verify that both loops produce the same output.</span>
<span class="cm"> *    The first loop is simpler but more restrictive;</span>
<span class="cm"> *    the second loop is more complex but less restrictive.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;omp.h&gt;</span>
<span class="cp">#include &lt;stdlib.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">16</span><span class="p">;</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">omp_set_num_threads</span><span class="p">(</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">);</span>
    <span class="p">}</span>

    <span class="cp">#pragma omp parallel for schedule(static,1)</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">REPS</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Thread %d performed iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> 
                 <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

<span class="cm">/*</span>
<span class="cm">    printf(&quot;\n---\n\n&quot;);</span>

<span class="cm">    #pragma omp parallel</span>
<span class="cm">    {</span>
<span class="cm">        int id = omp_get_thread_num();</span>
<span class="cm">        int numThreads = omp_get_num_threads();</span>
<span class="cm">        for (int i = id; i &lt; REPS; i += numThreads) {</span>
<span class="cm">            printf(&quot;Thread %d performed iteration %d\n&quot;, </span>
<span class="cm">                     id, i);</span>
<span class="cm">        }</span>
<span class="cm">    }</span>
<span class="cm">*/</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
<p>This can be made even more
efficient if the next available thread simply takes the next iteration.
In OpenMP, this is done by using <em>dynamic</em> scheduling instead of the static scheduling shown
in the above code.  Also note that the number of iterations, or chunk size, could
be greater than 1 inside the schedule clause.</p>
</div>
<div class="section" id="coordination-using-collective-communication-reduction">
<h2>8. Coordination Using Collective Communication: Reduction<a class="headerlink" href="#coordination-using-collective-communication-reduction" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/08.reduction/reduction.c</em></p>
<p><em>Build inside 08.reduction directory:</em></p>
<div class="highlight-python"><pre>make reduction</pre>
</div>
<p><em>Execute on the command line inside 08.reduction directory:</em></p>
<div class="highlight-python"><pre>./reduction 4
Replace 4 with other values for the number of threads, or leave off</pre>
</div>
<p>Once threads have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite common to then <em>reduce</em>
those individual computations into one value. This type of operation is
called a <strong>collective communication</strong> pattern because the threads must somehow
work together to create the final desired single value.</p>
<p>In this example, an array of randomly assigned integers represents a set of shared data (a more realistic program would perform a computation
that creates meaningful data values; this is just an example).
Note the common sequential code pattern found in the function called <em>sequentialSum</em> in the code
below (starting line 51): a for loop is used to sum up all the values in the array.</p>
<p>Next let&#8217;s consider how this can be done in parallel with threads.
Somehow the threads must implicitly <cite>communicate</cite> to keep the overall sum updated
as each of them works on a portion of the array.
In the <em>parallelSum</em> function, line 64 shows a special clause that
can be used with the parallel for pragma in OpenMP for this. All values
in the array are summed together by using the OpenMP
parallel for pragma with the <cite>reduction(+:sum)</cite> clause on the variable <strong>sum</strong>,
which is computed in line 66.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* reduction.c</span>
<span class="cm"> * ... illustrates the OpenMP parallel-for loop&#39;s reduction clause</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./reduction </span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run.  Note that correct output is produced.</span>
<span class="cm"> * - Uncomment #pragma in function parallelSum(), </span>
<span class="cm"> *    but leave its reduction clause commented out</span>
<span class="cm"> * - Recompile and rerun.  Note that correct output is NOT produced.</span>
<span class="cm"> * - Uncomment &#39;reduction(+:sum)&#39; clause of #pragma in parallelSum()</span>
<span class="cm"> * - Recompile and rerun.  Note that correct output is produced again.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;omp.h&gt;     </span><span class="c1">// OpenMP</span>
<span class="cp">#include &lt;stdlib.h&gt;  </span><span class="c1">// rand()</span>

<span class="kt">void</span> <span class="n">initialize</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">sequentialSum</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">parallelSum</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">);</span>

<span class="cp">#define SIZE 1000000</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">int</span> <span class="n">array</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>

   <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">omp_set_num_threads</span><span class="p">(</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">);</span>
   <span class="p">}</span>

   <span class="n">initialize</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">);</span>
   <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Sequential sum: </span><span class="se">\t</span><span class="s">%d</span><span class="se">\n</span><span class="s">Parallel sum: </span><span class="se">\t</span><span class="s">%d</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">,</span>
            <span class="n">sequentialSum</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">),</span>
            <span class="n">parallelSum</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">)</span> <span class="p">);</span>

   <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span> 

<span class="cm">/* fill array with random values */</span>
<span class="kt">void</span> <span class="nf">initialize</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
   <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rand</span><span class="p">()</span> <span class="o">%</span> <span class="mi">1000</span><span class="p">;</span>
   <span class="p">}</span>
<span class="p">}</span>

<span class="cm">/* sum the array sequentially */</span>
<span class="kt">int</span> <span class="nf">sequentialSum</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
   <span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
   <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">sum</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
   <span class="p">}</span>
   <span class="k">return</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/* sum the array using multiple threads */</span>
<span class="kt">int</span> <span class="nf">parallelSum</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">int</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
   <span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
<span class="c1">//   #pragma omp parallel for // reduction(+:sum)</span>
   <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">sum</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
   <span class="p">}</span>
   <span class="k">return</span> <span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
<div class="section" id="something-to-think-about">
<h3>Something to think about<a class="headerlink" href="#something-to-think-about" title="Permalink to this headline">¶</a></h3>
<p>Do you have an ideas about why the parallel for pragma without the reduction clause did not
produce the correct result?  The next few examples will hopefully shed some light on this.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Data Decomposition Algorithm Strategies and Related Coordination Strategies</a><ul>
<li><a class="reference internal" href="#shared-data-decomposition-algorithm-strategy-chunks-of-data-per-thread-using-a-parallel-for-loop-implementation-strategy">6. Shared Data Decomposition Algorithm Strategy:  chunks of data per thread using a parallel for loop implementation strategy</a></li>
<li><a class="reference internal" href="#shared-data-decomposition-algorithm-strategy-one-iteration-per-thread-in-a-parallel-for-loop-implementation-strategy">7. Shared Data Decomposition Algorithm Strategy:  one iteration per thread in a parallel for loop implementation strategy</a></li>
<li><a class="reference internal" href="#coordination-using-collective-communication-reduction">8. Coordination Using Collective Communication: Reduction</a><ul>
<li><a class="reference internal" href="#something-to-think-about">Something to think about</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li><a href="../index.html">Parallel Patternlets</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>