

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Shared Memory Program Structure and Coordination Patterns &mdash; Parallel Patternlets</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="Parallel Patternlets" href="../index.html" />
    <link rel="up" title="Shared Memory Parallel Patternlets in OpenMP" href="OpenMP_Patternlets.html" />
    <link rel="next" title="Data Decomposition Algorithm Strategies and Related Coordination Strategies" href="DataDecomp_Reduction.html" />
    <link rel="prev" title="Shared Memory Parallel Patternlets in OpenMP" href="OpenMP_Patternlets.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="DataDecomp_Reduction.html" title="Data Decomposition Algorithm Strategies and Related Coordination Strategies"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="OpenMP_Patternlets.html" title="Shared Memory Parallel Patternlets in OpenMP"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Parallel Patternlets</a> &raquo;</li>
          <li><a href="OpenMP_Patternlets.html" accesskey="U">Shared Memory Parallel Patternlets in OpenMP</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="shared-memory-program-structure-and-coordination-patterns">
<h1>Shared Memory Program Structure and Coordination Patterns<a class="headerlink" href="#shared-memory-program-structure-and-coordination-patterns" title="Permalink to this headline">¶</a></h1>
<div class="section" id="program-structure-implementation-strategy-the-basic-fork-join-pattern">
<h2>0. Program Structure Implementation Strategy: The basic fork-join pattern<a class="headerlink" href="#program-structure-implementation-strategy-the-basic-fork-join-pattern" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/00.forkJoin/forkJoin.c</em></p>
<p><em>Build inside 00.forkJoin directory:</em></p>
<div class="highlight-python"><pre>make forkjoin</pre>
</div>
<p><em>Execute on the command line inside 00.forkJoin directory:</em></p>
<div class="highlight-python"><pre>./forkjoin</pre>
</div>
<p>The <em>omp parallel</em> pragma on line 21, when uncommented, tells the compiler to
fork a set of threads to execute the next line of code (later you will see how this is done for a block of code).  You can conceptualize how this works using the following diagram, where time is moving from left to right:</p>
<img alt="../_images/ForkJoin.png" src="../_images/ForkJoin.png" style="width: 800px;" />
<p>Observe what happens on the machine
where you are running this code, both when you have the pragma commented (no fork) and when you uncomment it (adding a fork).</p>
<p>Note that in OpenMP the join is implicit and does not require a pragma directive.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* forkJoin.c</span>
<span class="cm"> * ... illustrates the fork-join pattern </span>
<span class="cm"> *      using OpenMP&#39;s parallel directive.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./forkJoin</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile &amp; run, uncomment the pragma,</span>
<span class="cm"> *    recompile &amp; run, compare results.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;     </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;omp.h&gt;       </span><span class="c1">// OpenMP</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Before...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

<span class="c1">//    #pragma omp parallel </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">During...&quot;</span><span class="p">);</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n\n</span><span class="s">After...</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="program-structure-implementation-strategy-fork-join-with-setting-the-number-of-threads">
<h2>1. Program Structure Implementation Strategy: Fork-join with setting the number of threads<a class="headerlink" href="#program-structure-implementation-strategy-fork-join-with-setting-the-number-of-threads" title="Permalink to this headline">¶</a></h2>
<p><em>file openMP/01.forkJoin2/forkJoin2.c</em></p>
<p><em>Build inside 01.forkJoin2 directory:</em></p>
<div class="highlight-python"><pre>make forkjoin2</pre>
</div>
<p><em>Execute on the command line inside 01.forkJoin2 directory:</em></p>
<div class="highlight-python"><pre>./forkjoin2</pre>
</div>
<p>This code illustrates that one program can fork and join more than once
and that programmers can set the number of threads to use in the parallel forked code.</p>
<p>Note on line 28 there is an OpenMP function called <em>omp_set_num_threads</em>
for setting the number of threads to use for each
<em>fork</em>, which occur when the omp_parallel pragma is used.
Also note on line 35 that you can set the number of threads for the very next
fork indicated by an omp_parallel pragma by augmenting the pragma as shown in line 35.
Follow the instructions in the header of the code file to understand the difference
between these.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* forkJoin2.c</span>
<span class="cm"> * ... illustrates the fork-join pattern </span>
<span class="cm"> *      using multiple OpenMP parallel directives,</span>
<span class="cm"> *      and changing the number of threads two ways.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, May 2013.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./forkJoin2</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile &amp; run, compare results to source.</span>
<span class="cm"> * - Predict how many threads will be used in &#39;Part IV&#39;?</span>
<span class="cm"> * - Uncomment &#39;Part IV&#39;, recompile, rerun.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;    </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;omp.h&gt;      </span><span class="c1">// OpenMP</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Beginning</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

    <span class="cp">#pragma omp parallel </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Part I&quot;</span><span class="p">);</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n\n</span><span class="s">Between I and II...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

    <span class="n">omp_set_num_threads</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>

    <span class="cp">#pragma omp parallel </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Part II...&quot;</span><span class="p">);</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n\n</span><span class="s">Between II and III...</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

    <span class="cp">#pragma omp parallel num_threads(5)</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Part III...&quot;</span><span class="p">);</span>
<span class="cm">/*</span>
<span class="cm">    printf(&quot;\n\nBetween III and IV...\n&quot;);</span>

<span class="cm">    #pragma omp parallel </span>
<span class="cm">    printf(&quot;\nPart IV...&quot;);</span>
<span class="cm">*/</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n\n</span><span class="s">End</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="program-structure-implementation-strategy-single-program-multiple-data">
<h2>2. Program Structure Implementation Strategy: Single Program, multiple data<a class="headerlink" href="#program-structure-implementation-strategy-single-program-multiple-data" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/02.spmd/spmd.c</em></p>
<p><em>Build inside 02.spmd directory:</em></p>
<div class="highlight-python"><pre>make spmd</pre>
</div>
<p><em>Execute on the command line inside 02.spmd directory:</em></p>
<div class="highlight-python"><pre>./spmd</pre>
</div>
<p>Note how there are OpenMP functions to
obtain a thread number and the total number of threads.
We have one program, but multiple threads executing in the forked section,
each with a copy of the id and num_threads variables.
Programmers write one program, but write it in such a way that
each thread has its own data values for particular variables.
This is why this is called the <em>single program, multiple data</em> (SPMD) pattern.</p>
<p>Most parallel programs use this SPMD pattern, because writing one program
is ultimately the most efficient method for programmers.  It does require you
as a programmer to understand how this works, however.  Think carefully about
how each thread executing in parallel has its own set of variables.  Conceptually,
it looks like this, where each thread has its own memory for the variables id and numThreads:</p>
<img alt="../_images/ForkJoin_SPMD.png" src="../_images/ForkJoin_SPMD.png" style="width: 800px;" />
<p>When the pragma is uncommented in the code below, note what the default number of threads
is.  Here the threads are forked and execute the block of code insode the
curly braces on lines 22 through 26.  This is how we can have a block of code executed
concurrently on each thread.</p>
<p>When you execute the parallel version containing the pragma (uncommenting line 20),
what do you observe about the order of the printed lines?  Run the program multiple times&#8211;
does the ordering change?  This illustrates an important point about threaded programs:
<em>the ordering of execution of statements between threads is not guaranteed.</em>  This is also
illustrated in the diagram above.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* spmd.c</span>
<span class="cm"> * ... illustrates the single-program-multiple-data (SPMD)</span>
<span class="cm"> *      pattern using two basic OpenMP commands...</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./spmd</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile &amp; run </span>
<span class="cm"> * - Uncomment pragma, recompile &amp; run, compare results</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;omp.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

<span class="c1">//    #pragma omp parallel </span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="kt">int</span> <span class="n">numThreads</span> <span class="o">=</span> <span class="n">omp_get_num_threads</span><span class="p">();</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello from thread %d of %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="program-structure-implementation-strategy-single-program-multiple-data-with-user-defined-number-of-threads">
<h2>3. Program Structure Implementation Strategy: Single Program, multiple data with user-defined number of threads<a class="headerlink" href="#program-structure-implementation-strategy-single-program-multiple-data-with-user-defined-number-of-threads" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/03.spmd2/spmd2.c</em></p>
<p><em>Build inside 03.spmd2 directory:</em></p>
<div class="highlight-python"><pre>make spmd2</pre>
</div>
<p><em>Execute on the command line inside 03.spmd2 directory:</em></p>
<div class="highlight-python"><pre>./spmd2 4
Replace 4 with other values for the number of threads</pre>
</div>
<p>Here we enter the number of threads to use on the command line.  This is a useful way to
make your code versatile so that you can use as many threads as you would like.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* spmd2.c</span>
<span class="cm"> * ... illustrates the SPMD pattern in OpenMP,</span>
<span class="cm"> * 	using the commandline arguments </span>
<span class="cm"> *      to control the number of threads.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./spmd2 [numThreads]</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile &amp; run with no commandline args </span>
<span class="cm"> * - Rerun with different commandline args</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;omp.h&gt;</span>
<span class="cp">#include &lt;stdlib.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">;</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">omp_set_num_threads</span><span class="p">(</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">);</span>
    <span class="p">}</span>

    <span class="cp">#pragma omp parallel </span>
    <span class="p">{</span>
        <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="n">numThreads</span> <span class="o">=</span> <span class="n">omp_get_num_threads</span><span class="p">();</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello from thread %d of %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="coordination-synchronization-with-a-barrier">
<h2>4. Coordination: Synchronization with a Barrier<a class="headerlink" href="#coordination-synchronization-with-a-barrier" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/04.barrier/barrier.c</em></p>
<p><em>Build inside 04.barrier directory:</em></p>
<div class="highlight-python"><pre>make barrier</pre>
</div>
<p><em>Execute on the command line inside 04.barrier directory:</em></p>
<div class="highlight-python"><pre>./barrier 4
Replace 4 with other values for the number of threads</pre>
</div>
<p>The barrier pattern is used in parallel programs to ensure that all threads complete
a parallel section of code before execution continues. This can be necessary when
threads are generating computed data (in an array, for example) that needs to be
completed for use in another computation.</p>
<p>Conceptually, the running code is excuting like this:</p>
<img alt="../_images/ForkJoin_Barrier.png" src="../_images/ForkJoin_Barrier.png" style="width: 850px;" />
<p>Note what happens with and without the commented pragma on line 35.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* barrier.c</span>
<span class="cm"> * ... illustrates the use of the OpenMP barrier command,</span>
<span class="cm"> * 	using the commandline to control the number of threads...</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, May 2013.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./barrier [numThreads]</span>
<span class="cm"> * </span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile &amp; run several times, noting interleaving of outputs.</span>
<span class="cm"> * - Uncomment the barrier directive, recompile, rerun,</span>
<span class="cm"> *    and note the change in the outputs.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;omp.h&gt;</span>
<span class="cp">#include &lt;stdlib.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">omp_set_num_threads</span><span class="p">(</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">);</span>
    <span class="p">}</span>

    <span class="cp">#pragma omp parallel </span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="kt">int</span> <span class="n">numThreads</span> <span class="o">=</span> <span class="n">omp_get_num_threads</span><span class="p">();</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Thread %d of %d is BEFORE the barrier.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>

<span class="c1">//        #pragma omp barrier </span>

        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Thread %d of %d is AFTER the barrier.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="program-structure-the-master-worker-implementation-strategy">
<h2>5. Program Structure: The Master-Worker Implementation Strategy<a class="headerlink" href="#program-structure-the-master-worker-implementation-strategy" title="Permalink to this headline">¶</a></h2>
<p><em>file: openMP/05.masterWorker/masterWorker.c</em></p>
<p><em>Build inside 05.masterWorker directory:</em></p>
<div class="highlight-python"><pre>make masterWorker</pre>
</div>
<p><em>Execute on the command line inside 05.masterWorker directory:</em></p>
<div class="highlight-python"><pre>./masterWorker 4
Replace 4 with other values for the number of threads</pre>
</div>
<p>Once you have mastered the notion of fork-join and single-program, multiple data,
the next common pattern that programmers use in association with these patterns
is to have one thread, called the master, execute one block of code when it forks while the rest
of the threads, called workers, execute a different block of code when they fork.
This is illustrated in this simple example (useful code would be more complicated).</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* masterWorker.c</span>
<span class="cm"> * ... illustrates the master-worker pattern in OpenMP</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: ./masterWorker</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> * - Compile and run as is.</span>
<span class="cm"> * - Uncomment #pragma directive, re-compile and re-run</span>
<span class="cm"> * - Compare and trace the different executions.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;stdlib.h&gt;   </span><span class="c1">// atoi()</span>
<span class="cp">#include &lt;omp.h&gt;     </span><span class="c1">// OpenMP</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">argc</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">omp_set_num_threads</span><span class="p">(</span> <span class="n">atoi</span><span class="p">(</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">);</span>
    <span class="p">}</span>

<span class="c1">//    #pragma omp parallel </span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="kt">int</span> <span class="n">numThreads</span> <span class="o">=</span> <span class="n">omp_get_num_threads</span><span class="p">();</span>

        <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">{</span>  <span class="c1">// thread with ID 0 is master</span>
            <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from the master, # %d of %d threads</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
			    <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>          <span class="c1">// threads with IDs &gt; 0 are workers </span>
            <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from a worker, # %d of %d threads</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
			    <span class="n">id</span><span class="p">,</span> <span class="n">numThreads</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Shared Memory Program Structure and Coordination Patterns</a><ul>
<li><a class="reference internal" href="#program-structure-implementation-strategy-the-basic-fork-join-pattern">0. Program Structure Implementation Strategy: The basic fork-join pattern</a></li>
<li><a class="reference internal" href="#program-structure-implementation-strategy-fork-join-with-setting-the-number-of-threads">1. Program Structure Implementation Strategy: Fork-join with setting the number of threads</a></li>
<li><a class="reference internal" href="#program-structure-implementation-strategy-single-program-multiple-data">2. Program Structure Implementation Strategy: Single Program, multiple data</a></li>
<li><a class="reference internal" href="#program-structure-implementation-strategy-single-program-multiple-data-with-user-defined-number-of-threads">3. Program Structure Implementation Strategy: Single Program, multiple data with user-defined number of threads</a></li>
<li><a class="reference internal" href="#coordination-synchronization-with-a-barrier">4. Coordination: Synchronization with a Barrier</a></li>
<li><a class="reference internal" href="#program-structure-the-master-worker-implementation-strategy">5. Program Structure: The Master-Worker Implementation Strategy</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="OpenMP_Patternlets.html"
                        title="previous chapter">Shared Memory Parallel Patternlets in OpenMP</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="DataDecomp_Reduction.html"
                        title="next chapter">Data Decomposition Algorithm Strategies and Related Coordination Strategies</a></p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="DataDecomp_Reduction.html" title="Data Decomposition Algorithm Strategies and Related Coordination Strategies"
             >next</a> |</li>
        <li class="right" >
          <a href="OpenMP_Patternlets.html" title="Shared Memory Parallel Patternlets in OpenMP"
             >previous</a> |</li>
        <li><a href="../index.html">Parallel Patternlets</a> &raquo;</li>
          <li><a href="OpenMP_Patternlets.html" >Shared Memory Parallel Patternlets in OpenMP</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>