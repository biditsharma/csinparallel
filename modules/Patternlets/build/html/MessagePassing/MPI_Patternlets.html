

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Message Passing Parallel Patternlets &mdash; Parallel Patternlets</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="top" title="Parallel Patternlets" href="../index.html" />
    <link rel="next" title="Shared Memory Parallel Patternlets in OpenMP" href="../SharedMemory/OpenMP_Patternlets.html" />
    <link rel="prev" title="Parallel Programming Patterns" href="../PatternsIntro.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../SharedMemory/OpenMP_Patternlets.html" title="Shared Memory Parallel Patternlets in OpenMP"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="../PatternsIntro.html" title="Parallel Programming Patterns"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">Parallel Patternlets</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="message-passing-parallel-patternlets">
<h1>Message Passing Parallel Patternlets<a class="headerlink" href="#message-passing-parallel-patternlets" title="Permalink to this headline">¶</a></h1>
<p>Parallel programs contain <em>patterns</em>:  code that recurs over and over again
in solutions to many problems.  The following examples show very simple
examples of small portions of
these patterns that can be combined to solve a problem.  These C code examples use the
Message Passing Interface (MPI) library, which is suitable for use on either a
single multiprocessor machine or a cluster
of machines.</p>
<div class="section" id="source-code">
<h2>Source Code<a class="headerlink" href="#source-code" title="Permalink to this headline">¶</a></h2>
<p>Please download all examples from this tarball:
<a class="reference download internal" href="../_downloads/MPI.tgz"><tt class="xref download docutils literal"><span class="pre">MPI.tgz</span></tt></a></p>
<p>A C code file for each example below can be found in subdirectories of the MPI directory,
along with a makefile and an example of how to execute the program.</p>
</div>
<div class="section" id="single-program-multiple-data">
<h2>00. Single Program, Multiple Data<a class="headerlink" href="#single-program-multiple-data" title="Permalink to this headline">¶</a></h2>
<p>First let us illustrate the basic components of an MPI program,
which by its nature uses a single program that runs on each process.
Note what gets printed is different for each process, thus the
processes using this one single program can have different data values
for its variables.  This is why we call it single program, multiple data.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* spmd.c</span>
<span class="cm"> * ... illustrates the single program multiple data</span>
<span class="cm"> *      (SPMD) pattern using basic MPI commands.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 4 ./spmd</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run.</span>
<span class="cm"> * - Compare source code to output.</span>
<span class="cm"> * - Rerun, using varying numbers of processes</span>
<span class="cm"> *    (i.e., vary the argument to &#39;mpirun -np&#39;).</span>
<span class="cm"> * - Explain what &quot;multiple data&quot; values this</span>
<span class="cm"> *    &quot;single program&quot; is generating.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;     </span><span class="c1">// MPI functions</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">myHostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">];</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>
    <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">myHostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from process #%d of %d on %s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
             <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">myHostName</span><span class="p">);</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/00.spmd/spmd.c</em></p>
</div>
<div class="section" id="the-master-worker-implementation-strategy-pattern">
<h2>01. The Master-Worker Implementation Strategy Pattern<a class="headerlink" href="#the-master-worker-implementation-strategy-pattern" title="Permalink to this headline">¶</a></h2>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* masterWorker.c</span>
<span class="cm"> * ... illustrates the basic master-worker pattern in MPI ...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./masterWorker</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run the program, varying N from 1 through 8.</span>
<span class="cm"> * - Explain what stays the same and what changes as the</span>
<span class="cm"> *    number of processes changes.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;mpi.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numWorkers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
  <span class="kt">char</span> <span class="n">hostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">];</span>

  <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
  <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
  <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numWorkers</span><span class="p">);</span>
  <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">hostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">{</span>  <span class="c1">// process 0 is the master </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from the master, #%d (%s) of %d processes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
             <span class="n">id</span><span class="p">,</span> <span class="n">hostName</span><span class="p">,</span> <span class="n">numWorkers</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>          <span class="c1">// processes with ids &gt; 0 are workers </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from a worker, #%d (%s) of %d processes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
             <span class="n">id</span><span class="p">,</span> <span class="n">hostName</span><span class="p">,</span> <span class="n">numWorkers</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">MPI_Finalize</span><span class="p">();</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/01.masterWorker/masterWorker.c</em></p>
</div>
<div class="section" id="message-passing-1-using-send-receive-of-a-single-value">
<h2>02. Message passing 1, using Send-Receive of a single value<a class="headerlink" href="#message-passing-1-using-send-receive-of-a-single-value" title="Permalink to this headline">¶</a></h2>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* messagePassing.c</span>
<span class="cm"> * ... illustrates the use of the MPI_Send() and MPI_Recv() commands...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./messagePassing</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, using N = 4, 6, 8, and 10 processes.</span>
<span class="cm"> * - Use source code to trace execution.</span>
<span class="cm"> * - Explain what each process:</span>
<span class="cm"> * -- computes</span>
<span class="cm"> * -- sends</span>
<span class="cm"> * -- receives</span>
<span class="cm"> * -- outputs.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;mpi.h&gt;</span>
<span class="cp">#include &lt;math.h&gt;   </span><span class="c1">// sqrt()</span>

<span class="kt">int</span> <span class="nf">odd</span><span class="p">(</span><span class="kt">int</span> <span class="n">number</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">number</span> <span class="o">%</span> <span class="mi">2</span><span class="p">;</span> <span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span> 
    <span class="kt">float</span> <span class="n">sendValue</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">receivedValue</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="n">MPI_Status</span> <span class="n">status</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">numProcesses</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">odd</span><span class="p">(</span><span class="n">numProcesses</span><span class="p">)</span> <span class="p">)</span> <span class="p">{</span>
        <span class="n">sendValue</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">id</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span> <span class="n">odd</span><span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="p">)</span> <span class="p">{</span>  <span class="c1">// odd processors send, then receive </span>
            <span class="n">MPI_Send</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sendValue</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_FLOAT</span><span class="p">,</span> <span class="n">id</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
            <span class="n">MPI_Recv</span><span class="p">(</span><span class="o">&amp;</span><span class="n">receivedValue</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_FLOAT</span><span class="p">,</span> <span class="n">id</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> 
                       <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>          <span class="c1">// even processors receive, then send </span>
            <span class="n">MPI_Recv</span><span class="p">(</span><span class="o">&amp;</span><span class="n">receivedValue</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_FLOAT</span><span class="p">,</span> <span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                       <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
            <span class="n">MPI_Send</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sendValue</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_FLOAT</span><span class="p">,</span> <span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d of %d computed %f and received %f</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">sendValue</span><span class="p">,</span> <span class="n">receivedValue</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span> <span class="o">!</span><span class="n">id</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// only process 0 does this part </span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Please run this program using -np N where N is positive and even.</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/02.messagePassing/messagePassing.c</em></p>
</div>
<div class="section" id="message-passing-2-using-send-receive-of-an-array-of-values">
<h2>03. Message passing 2,  using Send-Receive of an array of values<a class="headerlink" href="#message-passing-2-using-send-receive-of-an-array-of-values" title="Permalink to this headline">¶</a></h2>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* messagePassing2.c</span>
<span class="cm"> * ... illustrates using MPI_Send() and MPI_Recv() commands on arrays...</span>
<span class="cm"> * While this example sends and receives char arrays (strings),</span>
<span class="cm"> *  the same approach works on arrays of numbers or other types.</span>
<span class="cm"> * Joel Adams, Calvin College, September 2013.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./messagePassing2</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 4, 8.</span>
<span class="cm"> * - Trace execution using source code.</span>
<span class="cm"> * - Compare to messagePassing1.c; note send/receive differences.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;     </span><span class="c1">// MPI</span>
<span class="cp">#include &lt;stdlib.h&gt;  </span><span class="c1">// malloc()</span>
<span class="cp">#include &lt;string.h&gt;  </span><span class="c1">// strlen()</span>

<span class="kt">int</span> <span class="nf">odd</span><span class="p">(</span><span class="kt">int</span> <span class="n">number</span><span class="p">)</span> <span class="p">{</span> <span class="k">return</span> <span class="n">number</span> <span class="o">%</span> <span class="mi">2</span><span class="p">;</span> <span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span> 
    <span class="kt">char</span> <span class="o">*</span> <span class="n">sendString</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="kt">char</span> <span class="o">*</span> <span class="n">receivedString</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">hostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">];</span>
    <span class="n">MPI_Status</span> <span class="n">status</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span><span class="o">+</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">char</span><span class="p">);</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>
    <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">hostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">numProcesses</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">odd</span><span class="p">(</span><span class="n">numProcesses</span><span class="p">)</span> <span class="p">)</span> <span class="p">{</span>
        <span class="n">sendString</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">SIZE</span> <span class="p">);</span>
        <span class="n">receivedString</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">SIZE</span> <span class="p">);</span>
        <span class="n">sprintf</span><span class="p">(</span><span class="n">sendString</span><span class="p">,</span> <span class="s">&quot;Process %d is on host </span><span class="se">\&quot;</span><span class="s">%s</span><span class="se">\&quot;</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">hostName</span><span class="p">);</span>

        <span class="k">if</span> <span class="p">(</span> <span class="n">odd</span><span class="p">(</span><span class="n">id</span><span class="p">)</span> <span class="p">)</span> <span class="p">{</span>  <span class="c1">// odd processes send, then receive </span>
            <span class="n">MPI_Send</span><span class="p">(</span><span class="n">sendString</span><span class="p">,</span> <span class="n">strlen</span><span class="p">(</span><span class="n">sendString</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> 
                       <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="n">id</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
            <span class="n">MPI_Recv</span><span class="p">(</span><span class="n">receivedString</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="n">id</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> 
                       <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>          <span class="c1">// even processes receive, then send </span>
            <span class="n">MPI_Recv</span><span class="p">(</span><span class="n">receivedString</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                       <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
            <span class="n">MPI_Send</span><span class="p">(</span><span class="n">sendString</span><span class="p">,</span> <span class="n">strlen</span><span class="p">(</span><span class="n">sendString</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> 
                       <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Process %d of %d received the message:</span><span class="se">\n\t</span><span class="s">&#39;%s&#39;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">receivedString</span><span class="p">);</span>

        <span class="n">free</span><span class="p">(</span><span class="n">sendString</span><span class="p">);</span>
        <span class="n">free</span><span class="p">(</span><span class="n">receivedString</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span> <span class="o">!</span><span class="n">id</span><span class="p">)</span> <span class="p">{</span>  <span class="c1">// only process 0 does this part </span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Please run this program using -np N where N is positive and even.</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/03.messagePassing2/messagePassing2.c</em></p>
</div>
<div class="section" id="message-passing-3-using-send-receive-with-master-worker-pattern">
<h2>04. Message passing 3,  using Send-Receive with master-worker pattern<a class="headerlink" href="#message-passing-3-using-send-receive-with-master-worker-pattern" title="Permalink to this headline">¶</a></h2>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* messagePassing3.c</span>
<span class="cm"> * ... illustrates the use of MPI_Send() and MPI_Recv(),</span>
<span class="cm"> *      in combination with the master-worker pattern. </span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./messagePassing3</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> * - Run the program, varying the value of N from 1-8.</span>
<span class="cm"> * - Explain the behavior you observe.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;    </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;string.h&gt;   </span><span class="c1">// strlen()</span>
<span class="cp">#include &lt;mpi.h&gt;      </span><span class="c1">// MPI</span>

<span class="cp">#define MAX 256</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span> 
    <span class="kt">char</span> <span class="n">sendBuffer</span><span class="p">[</span><span class="n">MAX</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sc">&#39;\0&#39;</span><span class="p">};</span>
    <span class="kt">char</span> <span class="n">recvBuffer</span><span class="p">[</span><span class="n">MAX</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sc">&#39;\0&#39;</span><span class="p">};</span>
    <span class="n">MPI_Status</span> <span class="n">status</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">numProcesses</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">{</span>                              <span class="c1">// master:</span>
            <span class="n">sprintf</span><span class="p">(</span><span class="n">sendBuffer</span><span class="p">,</span> <span class="s">&quot;%d&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>            <span class="c1">//  create msg</span>

            <span class="n">MPI_Send</span><span class="p">(</span><span class="n">sendBuffer</span><span class="p">,</span>                      <span class="c1">//  msg sent</span>
                      <span class="n">strlen</span><span class="p">(</span><span class="n">sendBuffer</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>         <span class="c1">//  num chars + NULL</span>
                      <span class="n">MPI_CHAR</span><span class="p">,</span>                       <span class="c1">//  type</span>
                      <span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span>                           <span class="c1">//  destination</span>
                      <span class="mi">1</span><span class="p">,</span>                              <span class="c1">//  tag</span>
                      <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>                <span class="c1">//  communicator</span>

            <span class="n">MPI_Recv</span><span class="p">(</span><span class="n">recvBuffer</span><span class="p">,</span>                      <span class="c1">//  msg received</span>
                      <span class="n">MAX</span><span class="p">,</span>                            <span class="c1">//  buffer size</span>
                      <span class="n">MPI_CHAR</span><span class="p">,</span>                       <span class="c1">//  type</span>
                      <span class="n">numProcesses</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span>                 <span class="c1">//  sender</span>
                      <span class="mi">1</span><span class="p">,</span>                              <span class="c1">//  tag</span>
                      <span class="n">MPI_COMM_WORLD</span><span class="p">,</span>                 <span class="c1">//  communicator</span>
                      <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>                       <span class="c1">//  recv status</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>                                      <span class="c1">// workers:</span>
            <span class="n">MPI_Recv</span><span class="p">(</span><span class="n">recvBuffer</span><span class="p">,</span>                      <span class="c1">//  msg received</span>
                      <span class="n">MAX</span><span class="p">,</span>                            <span class="c1">//  buffer size</span>
                      <span class="n">MPI_CHAR</span><span class="p">,</span>                       <span class="c1">//  type</span>
                      <span class="n">MPI_ANY_SOURCE</span><span class="p">,</span>                 <span class="c1">//  sender (anyone)</span>
                      <span class="mi">1</span><span class="p">,</span>                              <span class="c1">//  tag</span>
                      <span class="n">MPI_COMM_WORLD</span><span class="p">,</span>                 <span class="c1">//  communicator</span>
                      <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>                       <span class="c1">//  recv status</span>

            <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process #%d of %d received %s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="c1">// show msg</span>
                    <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">recvBuffer</span><span class="p">);</span>

            <span class="c1">// build msg to send by appending id to msg received</span>
            <span class="n">sprintf</span><span class="p">(</span><span class="n">sendBuffer</span><span class="p">,</span> <span class="s">&quot;%s %d&quot;</span><span class="p">,</span> <span class="n">recvBuffer</span><span class="p">,</span> <span class="n">id</span><span class="p">);</span>

            <span class="n">MPI_Send</span><span class="p">(</span><span class="n">sendBuffer</span><span class="p">,</span>                      <span class="c1">//  msg to send</span>
                      <span class="n">strlen</span><span class="p">(</span><span class="n">sendBuffer</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>         <span class="c1">//  num chars + NULL</span>
                      <span class="n">MPI_CHAR</span><span class="p">,</span>                       <span class="c1">//  type</span>
                      <span class="p">(</span><span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">numProcesses</span><span class="p">,</span>          <span class="c1">//  destination</span>
                      <span class="mi">1</span><span class="p">,</span>                              <span class="c1">//  tag</span>
                      <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>                <span class="c1">//  communicator</span>
        <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Please run this program with at least 2 processes</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/04.messagePassing3/messagePassing3.c</em></p>
</div>
<div class="section" id="data-decomposition-on-equal-sized-chunks-using-parallel-for">
<h2>05. Data Decomposition: on <em>equal-sized chunks</em> using parallel-for<a class="headerlink" href="#data-decomposition-on-equal-sized-chunks-using-parallel-for" title="Permalink to this headline">¶</a></h2>
<p>In this example, the data being decomposed is simply the set of integers
from zero to REPS * numProcesses, which are used in the for loop.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* parallelLoopEqualChunks.c</span>
<span class="cm"> * ... illustrates the parallel for loop pattern in MPI </span>
<span class="cm"> *	in which processes perform the loop&#39;s iterations in equal-sized &#39;chunks&#39; </span>
<span class="cm"> *	(preferable when loop iterations access memory/cache locations) ...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./parallelForEqualChunks</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8</span>
<span class="cm"> * - Change REPS to 16, save, recompile, rerun, varying N again.</span>
<span class="cm"> * - Explain how this pattern divides the iterations of the loop</span>
<span class="cm"> *    among the processes.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt; </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;   </span><span class="c1">// MPI</span>
<span class="cp">#include &lt;math.h&gt;  </span><span class="c1">// ceil()</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">start</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">chunkSize</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="n">chunkSize</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">ceil</span><span class="p">(((</span><span class="kt">double</span><span class="p">)</span><span class="n">REPS</span><span class="p">)</span> <span class="o">/</span> <span class="n">numProcesses</span><span class="p">);</span> <span class="c1">// find chunk size</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">id</span> <span class="o">*</span> <span class="n">chunkSize</span><span class="p">;</span>                               <span class="c1">// find starting index</span>
                                                          <span class="c1">// find stopping index:</span>
    <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">&lt;</span> <span class="n">numProcesses</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">)</span> <span class="p">{</span>                        <span class="c1">//  if not the last process</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="p">(</span><span class="n">id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunkSize</span><span class="p">;</span>                      <span class="c1">//   stop where next process starts</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>                                              <span class="c1">//  else </span>
        <span class="n">stop</span> <span class="o">=</span> <span class="n">REPS</span><span class="p">;</span>                                      <span class="c1">//   last process does leftovers </span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">stop</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>                      <span class="c1">// iterate through our range </span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d is performing iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/05.parallelLoop-equalChunks/parallelLoopEqualChunks.c</em></p>
</div>
<div class="section" id="data-decomposition-on-chunks-of-size-1-using-parallel-for">
<h2>06. Data Decomposition: on <em>chunks of size 1</em> using parallel-for<a class="headerlink" href="#data-decomposition-on-chunks-of-size-1-using-parallel-for" title="Permalink to this headline">¶</a></h2>
<p>This is a basic example that does not yet include a data array, though
it would typically be used when each process would be working on a portion
of an array that could have been looped over in a sequential solution.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* parallelLoopChunksOf1.c</span>
<span class="cm"> * ... illustrates the parallel for loop pattern in MPI </span>
<span class="cm"> *	in which processes perform the loop&#39;s iterations in &#39;chunks&#39; </span>
<span class="cm"> *      of size 1 (simple, and useful when loop iterations </span>
<span class="cm"> *      do not access memory/cache locations) ...</span>
<span class="cm"> * Note this is much simpler than the &#39;equal chunks&#39; loop.</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./parallelForSlices</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8</span>
<span class="cm"> * - Change REPS to 16, save, recompile, rerun, varying N again.</span>
<span class="cm"> * - Explain how this pattern divides the iterations of the loop</span>
<span class="cm"> *    among the processes.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;  </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;    </span><span class="c1">// MPI</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">REPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">numProcesses</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d is performing iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/06.parallelLoop-chunksOf1/parallelLoopChunksOf1.c</em></p>
</div>
<div class="section" id="broadcast-a-special-form-of-message-passing">
<h2>07. Broadcast: a special form of message passing<a class="headerlink" href="#broadcast-a-special-form-of-message-passing" title="Permalink to this headline">¶</a></h2>
<p>This example shows how a data item read from a file can be sent to all the processes.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* broadcast.c</span>
<span class="cm"> * ... illustrates the use of MPI_Bcast() with a scalar value...</span>
<span class="cm"> *      (compare to array version).</span>
<span class="cm"> * Joel Adams, Calvin College, April 2016.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./broadcast</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run several times,</span>
<span class="cm"> *     using 2, 4, and 8 processes</span>
<span class="cm"> * - Use source code to trace execution and output</span>
<span class="cm"> *     (noting contents of file &quot;data.txt&quot;);</span>
<span class="cm"> * - Explain behavior/effect of MPI_Bcast().</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;mpi.h&gt;</span>
<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;stdlib.h&gt;</span>
<span class="cp">#include &lt;assert.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
	<span class="kt">int</span> <span class="n">answer</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">numProcs</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
        <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
        <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
               <span class="kt">FILE</span> <span class="o">*</span><span class="n">filePtr</span> <span class="o">=</span> <span class="n">fopen</span><span class="p">(</span><span class="s">&quot;data.txt&quot;</span><span class="p">,</span> <span class="s">&quot;r&quot;</span><span class="p">);</span> 
               <span class="n">assert</span><span class="p">(</span> <span class="n">filePtr</span> <span class="o">!=</span> <span class="nb">NULL</span> <span class="p">);</span>
               <span class="n">fscanf</span><span class="p">(</span><span class="n">filePtr</span><span class="p">,</span> <span class="s">&quot; %d&quot;</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">answer</span><span class="p">);</span>
               <span class="n">fclose</span><span class="p">(</span><span class="n">filePtr</span><span class="p">);</span>
        <span class="p">}</span>

	<span class="n">printf</span><span class="p">(</span><span class="s">&quot;BEFORE the broadcast, process %d&#39;s answer = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                 <span class="n">myRank</span><span class="p">,</span> <span class="n">answer</span><span class="p">);</span>

        <span class="n">MPI_Bcast</span><span class="p">(</span><span class="o">&amp;</span><span class="n">answer</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

	<span class="n">printf</span><span class="p">(</span><span class="s">&quot;AFTER the broadcast, process %d&#39;s answer = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                 <span class="n">myRank</span><span class="p">,</span> <span class="n">answer</span><span class="p">);</span>

 	<span class="n">MPI_Finalize</span><span class="p">();</span>

	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/07.broadcast/broadcast.c</em></p>
</div>
<div class="section" id="broadcast-send-data-to-all-processes">
<h2>08. Broadcast: send data to all processes<a class="headerlink" href="#broadcast-send-data-to-all-processes" title="Permalink to this headline">¶</a></h2>
<p>This example shows how to ensure that all processes have a copy of an array
created by a single <em>master</em> node.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* broadcast2.c</span>
<span class="cm"> * ... illustrates the use of MPI_Bcast() for arrays...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./broadcast</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, using 2, 4, and 8 processes</span>
<span class="cm"> * - Use source code to trace execution and output</span>
<span class="cm"> * - Explain behavior/effect of MPI_Bcast().</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;mpi.h&gt;</span>
<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;stdlib.h&gt;</span>

<span class="cm">/* fill an array with some arbitrary values </span>
<span class="cm"> * @param: a, an int*.</span>
<span class="cm"> * @param: size, an int.</span>
<span class="cm"> * Precondition: a is the address of an array of ints.</span>
<span class="cm"> *              &amp;&amp; size is the number of ints a can hold.</span>
<span class="cm"> * Postcondition: a has been filled with arbitrary values </span>
<span class="cm"> *                { 11, 12, 13, ... }.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">fill</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">int</span> <span class="n">size</span><span class="p">)</span> <span class="p">{</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">size</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span><span class="o">+</span><span class="mi">11</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="cm">/* display a string, a process id, and its array values </span>
<span class="cm"> * @param: str, a char*</span>
<span class="cm"> * @param: id, an int</span>
<span class="cm"> * @param: a, an int*.</span>
<span class="cm"> * Precondition: str points to either &quot;BEFORE&quot; or &quot;AFTER&quot;</span>
<span class="cm"> *              &amp;&amp; id is the rank of this MPI process</span>
<span class="cm"> *              &amp;&amp; a is the address of an 8-element int array.</span>
<span class="cm"> * Postcondition: str, id, and a have all been written to stdout.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span> <span class="n">str</span><span class="p">,</span> <span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">&quot;%s broadcast, process %d has: {%d, %d, %d, %d, %d, %d, %d, %d}</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
	   <span class="n">str</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">a</span><span class="p">[</span><span class="mi">7</span><span class="p">]);</span>
<span class="p">}</span>

<span class="cp">#define MAX 8</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
	<span class="kt">int</span> <span class="n">array</span><span class="p">[</span><span class="n">MAX</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
        <span class="kt">int</span> <span class="n">numProcs</span><span class="p">,</span> <span class="n">myRank</span><span class="p">;</span>

	<span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
        <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
        <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="n">fill</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">MAX</span><span class="p">);</span>
     
	<span class="n">print</span><span class="p">(</span><span class="s">&quot;BEFORE&quot;</span><span class="p">,</span> <span class="n">myRank</span><span class="p">,</span> <span class="n">array</span><span class="p">);</span>

        <span class="n">MPI_Bcast</span><span class="p">(</span><span class="n">array</span><span class="p">,</span> <span class="n">MAX</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

	<span class="n">print</span><span class="p">(</span><span class="s">&quot;AFTER&quot;</span><span class="p">,</span> <span class="n">myRank</span><span class="p">,</span> <span class="n">array</span><span class="p">);</span>

 	<span class="n">MPI_Finalize</span><span class="p">();</span>

	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/08.broadcast2/broadcast2.c</em></p>
</div>
<div class="section" id="collective-communication-reduction">
<h2>09. Collective Communication: Reduction<a class="headerlink" href="#collective-communication-reduction" title="Permalink to this headline">¶</a></h2>
<p>Once processes have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite common to then <em>reduce</em>
those individual computations into one value.  This example shows a simple
calculation done by each process being reduced to a sum and a maximum.
In this example, MPI, has built-in computations, indicated by MPI_SUM and
MPI_MAX in the following code.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* reduction.c</span>
<span class="cm"> * ... illustrates the use of MPI_Reduce()...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./reduction</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> * - Compile and run, varying N: 4, 6, 8, 10.</span>
<span class="cm"> * - Explain behavior of MPI_Reduce().</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;mpi.h&gt;</span>
<span class="cp">#include &lt;stdio.h&gt;</span>
<span class="cp">#include &lt;stdlib.h&gt;</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">numProcs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
        <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
        <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

	<span class="n">square</span> <span class="o">=</span> <span class="p">(</span><span class="n">myRank</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">myRank</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
     
	<span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d computed %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">myRank</span><span class="p">,</span> <span class="n">square</span><span class="p">);</span>

        <span class="n">MPI_Reduce</span><span class="p">(</span><span class="o">&amp;</span><span class="n">square</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sum</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_SUM</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

        <span class="n">MPI_Reduce</span><span class="p">(</span><span class="o">&amp;</span><span class="n">square</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">max</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_MAX</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">The sum of the squares is %d</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">&quot;The max of the squares is %d</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">max</span><span class="p">);</span>
	<span class="p">}</span>

 	<span class="n">MPI_Finalize</span><span class="p">();</span>

	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/09.reduction/reduction.c</em></p>
</div>
<div class="section" id="id1">
<h2>10. Collective Communication: Reduction<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Here is a second reduction example using arrays of data.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* reduction2.c</span>
<span class="cm"> * ... illustrates the use of MPI_Reduce() using arrays...</span>
<span class="cm"> * Joel Adams, Calvin College, January 2015.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 4 ./reduction2</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, comparing output to source code.</span>
<span class="cm"> * - Uncomment the &#39;commented out&#39; call to printArray.</span>
<span class="cm"> * - Save, recompile, rerun, comparing output to source code.</span>
<span class="cm"> * - Explain behavior of MPI_Reduce() in terms of </span>
<span class="cm"> *     srcArr and destArr.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;mpi.h&gt;</span>
<span class="cp">#include &lt;stdio.h&gt;</span>

<span class="cp">#define ARRAY_SIZE 5</span>

<span class="kt">void</span> <span class="n">printArray</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrayName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="kt">int</span> <span class="n">SIZE</span><span class="p">);</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">srcArr</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">int</span> <span class="n">destArr</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Before reduction: &quot;</span><span class="p">);</span>
        <span class="n">printArray</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;destArr&quot;</span><span class="p">,</span> <span class="n">destArr</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="p">);</span>
    <span class="p">}</span> 

    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">srcArr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">myRank</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>

<span class="c1">//    printArray(myRank, &quot;srcArr&quot;, srcArr, ARRAY_SIZE);</span>

    <span class="n">MPI_Reduce</span><span class="p">(</span><span class="n">srcArr</span><span class="p">,</span> <span class="n">destArr</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_SUM</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">After reduction:  &quot;</span><span class="p">);</span>
        <span class="n">printArray</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;destArr&quot;</span><span class="p">,</span> <span class="n">destArr</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="p">);</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="p">}</span> 

    <span class="n">MPI_Finalize</span><span class="p">();</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/* utility to display an array</span>
<span class="cm"> * params: id, the rank of the current process</span>
<span class="cm"> *         arrayName, the name of the array being displayed</span>
<span class="cm"> *         array, the array being displayed</span>
<span class="cm"> *         SIZE, the number of items in array.</span>
<span class="cm"> * postcondition:</span>
<span class="cm"> *         the id, name, and items in array have been printed to stdout.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">printArray</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrayName</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="kt">int</span> <span class="n">SIZE</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d, %s: [&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">arrayName</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%3d&quot;</span><span class="p">,</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="n">printf</span><span class="p">(</span><span class="s">&quot;,&quot;</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;]</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/10.reduction2/reduction2.c</em></p>
</div>
<div class="section" id="collective-communication-scatter-for-message-passing-data-decomposition">
<h2>11. Collective communication: Scatter for message-passing data decomposition<a class="headerlink" href="#collective-communication-scatter-for-message-passing-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p>If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the scatter pattern can be used to ensure that each process receives
a copy of its portion of the array.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* scatter.c</span>
<span class="cm"> * ... illustrates the use of MPI_Scatter()...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./scatter</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 4, 8</span>
<span class="cm"> * - Trace execution through source code.</span>
<span class="cm"> * - Explain behavior/effect of MPI_Scatter().</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;mpi.h&gt;      </span><span class="c1">// MPI</span>
<span class="cp">#include &lt;stdio.h&gt;    </span><span class="c1">// printf(), etc.</span>
<span class="cp">#include &lt;stdlib.h&gt;   </span><span class="c1">// malloc()</span>

<span class="kt">void</span> <span class="n">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">);</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">MAX</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">arrSend</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">arrRcv</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">numProcs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numSent</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>                            <span class="c1">// initialize</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>                                 <span class="c1">// master process:</span>
        <span class="n">arrSend</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">MAX</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>  <span class="c1">//  allocate array1</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">MAX</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>                <span class="c1">//  load with values</span>
            <span class="n">arrSend</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">11</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;arrSend&quot;</span><span class="p">,</span> <span class="n">arrSend</span><span class="p">,</span> <span class="n">MAX</span><span class="p">);</span>        <span class="c1">//  display array1</span>
    <span class="p">}</span>
     
    <span class="n">numSent</span> <span class="o">=</span> <span class="n">MAX</span> <span class="o">/</span> <span class="n">numProcs</span><span class="p">;</span>                          <span class="c1">// all processes:</span>
    <span class="n">arrRcv</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">numSent</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>   <span class="c1">//  allocate array2</span>

    <span class="n">MPI_Scatter</span><span class="p">(</span><span class="n">arrSend</span><span class="p">,</span> <span class="n">numSent</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">arrRcv</span><span class="p">,</span>     <span class="c1">//  scatter array1 </span>
                 <span class="n">numSent</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span> <span class="c1">//   into array2</span>

    <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;arrRcv&quot;</span><span class="p">,</span> <span class="n">arrRcv</span><span class="p">,</span> <span class="n">numSent</span><span class="p">);</span>          <span class="c1">// display array2</span>

    <span class="n">free</span><span class="p">(</span><span class="n">arrSend</span><span class="p">);</span>                                     <span class="c1">// clean up</span>
    <span class="n">free</span><span class="p">(</span><span class="n">arrRcv</span><span class="p">);</span>
    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d, %s: &quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">arrName</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">arrSize</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot; %d&quot;</span><span class="p">,</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/11.scatter/scatter.c</em></p>
</div>
<div class="section" id="collective-communication-gather-for-message-passing-data-decomposition">
<h2>12. Collective communication: Gather for message-passing data decomposition<a class="headerlink" href="#collective-communication-gather-for-message-passing-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p>If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the gather pattern can be used to ensure that each process sends
a copy of its portion of the array back to the root, or master process.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* gather.c</span>
<span class="cm"> * ... illustrates the use of MPI_Gather()...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./gather</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 4, 8.</span>
<span class="cm"> * - Trace execution through source.</span>
<span class="cm"> * - Explain behavior of MPI_Gather().</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;mpi.h&gt;       </span><span class="c1">// MPI</span>
<span class="cp">#include &lt;stdio.h&gt;     </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;stdlib.h&gt;    </span><span class="c1">// malloc()</span>

<span class="kt">void</span> <span class="n">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">);</span>

<span class="cp">#define SIZE 3</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">int</span>  <span class="n">computeArray</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>                          <span class="c1">// array1</span>
   <span class="kt">int</span><span class="o">*</span> <span class="n">gatherArray</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>                          <span class="c1">// array2</span>
   <span class="kt">int</span>  <span class="n">numProcs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">totalGatheredVals</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

   <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>                           <span class="c1">// initialize</span>
   <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
   <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>
                                                     <span class="c1">// all processes:</span>
   <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>                  <span class="c1">//  load array1 with</span>
      <span class="n">computeArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">myRank</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>             <span class="c1">//   3 distinct values</span>
   <span class="p">}</span>

   <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;computeArray&quot;</span><span class="p">,</span> <span class="n">computeArray</span><span class="p">,</span>       <span class="c1">//  show array1</span>
           <span class="n">SIZE</span><span class="p">);</span>

   <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>                                <span class="c1">// master:</span>
      <span class="n">totalGatheredVals</span> <span class="o">=</span> <span class="n">SIZE</span> <span class="o">*</span> <span class="n">numProcs</span><span class="p">;</span>           <span class="c1">//  allocate array2</span>
      <span class="n">gatherArray</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">totalGatheredVals</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>
   <span class="p">}</span>

   <span class="n">MPI_Gather</span><span class="p">(</span><span class="n">computeArray</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span>           <span class="c1">//  gather array1 vals</span>
               <span class="n">gatherArray</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span>           <span class="c1">//   into array2</span>
               <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>                   <span class="c1">//   at master process               </span>

   <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>                                <span class="c1">// master process:</span>
      <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;gatherArray&quot;</span><span class="p">,</span>                   <span class="c1">//  show array2</span>
             <span class="n">gatherArray</span><span class="p">,</span> <span class="n">totalGatheredVals</span><span class="p">);</span> 
   <span class="p">}</span>

   <span class="n">free</span><span class="p">(</span><span class="n">gatherArray</span><span class="p">);</span>                                <span class="c1">// clean up</span>
   <span class="n">MPI_Finalize</span><span class="p">();</span>
   <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d, %s: &quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">arrName</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">arrSize</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot; %d&quot;</span><span class="p">,</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/12.gather/gather.c</em></p>
</div>
<div class="section" id="the-barrier-coordination-pattern">
<h2>13. The Barrier Coordination Pattern<a class="headerlink" href="#the-barrier-coordination-pattern" title="Permalink to this headline">¶</a></h2>
<p>A barrier is used when you want all the processes to complete a portion of
code before continuing. Use this exercise to verify that is is ocurring when
you add the call to the MPI_Barrier funtion.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* barrier.c </span>
<span class="cm"> *  ... illustrates the behavior of MPI_Barrier() ...</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, May 2013.</span>
<span class="cm"> * Bill Siever, April 2016 </span>
<span class="cm"> *   (Converted to master/worker pattern).</span>
<span class="cm"> * Joel Adams, April 2016</span>
<span class="cm"> *   (Refactored code so that just one barrier needed).</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 8 ./barrier</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> *  - Compile; then run the program several times, </span>
<span class="cm"> *     noting the interleaved outputs.</span>
<span class="cm"> *  - Uncomment the MPI_Barrier() call; then recompile and rerun,</span>
<span class="cm"> *     noting how the output changes.</span>
<span class="cm"> *  - Explain what effect MPI_Barrier() has on process behavior.</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;     </span><span class="c1">// MPI</span>

<span class="cm">/* Have workers send messages to the master, which prints them.</span>
<span class="cm"> * @param: id, an int</span>
<span class="cm"> * @param: numProcesses, an int</span>
<span class="cm"> * @param: hostName, a char*</span>
<span class="cm"> * @param: position, a char*</span>
<span class="cm"> *</span>
<span class="cm"> * Precondition: this function is being called by an MPI process</span>
<span class="cm"> *               &amp;&amp; id is the MPI rank of that process </span>
<span class="cm"> *               &amp;&amp; numProcesses is the number of processes in the computation </span>
<span class="cm"> *               &amp;&amp; hostName points to a char array containing the name of the</span>
<span class="cm"> *                    host on which this MPI process is running </span>
<span class="cm"> *               &amp;&amp; position points to &quot;BEFORE&quot; or &quot;AFTER&quot;.</span>
<span class="cm"> *</span>
<span class="cm"> * Postcondition: each process whose id &gt; 0 has sent a message to process 0</span>
<span class="cm"> *                     containing id, numProcesses, hostName, and position </span>
<span class="cm"> *                &amp;&amp; process 0 has received and output each message.</span>
<span class="cm"> */</span>

<span class="cp">#define BUFFER_SIZE 200</span>
<span class="cp">#define MASTER      0</span>

<span class="kt">void</span> <span class="nf">sendReceivePrint</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">hostName</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">position</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">char</span> <span class="n">buffer</span><span class="p">[</span><span class="n">BUFFER_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sc">&#39;\0&#39;</span><span class="p">};;</span>
    <span class="n">MPI_Status</span> <span class="n">status</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">!=</span> <span class="n">MASTER</span><span class="p">)</span> <span class="p">{</span> 
        <span class="c1">// Worker: Build a message and send it to the Master</span>
        <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">sprintf</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span>
                              <span class="s">&quot;Process #%d of %d on %s is %s the barrier.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                                <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">hostName</span><span class="p">,</span> <span class="n">position</span><span class="p">);</span>       
        <span class="n">MPI_Send</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">length</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="c1">// Master: Receive and print the messages from all Workers</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numProcesses</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="n">MPI_Recv</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">BUFFER_SIZE</span><span class="p">,</span> <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="n">MPI_ANY_SOURCE</span><span class="p">,</span>
                     <span class="n">MPI_ANY_TAG</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
           <span class="n">printf</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">myHostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sc">&#39;\0&#39;</span><span class="p">};</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>
    <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">myHostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

    <span class="n">sendReceivePrint</span><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">myHostName</span><span class="p">,</span> <span class="s">&quot;BEFORE&quot;</span><span class="p">);</span>

<span class="c1">//    MPI_Barrier(MPI_COMM_WORLD);</span>

    <span class="n">sendReceivePrint</span><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">myHostName</span><span class="p">,</span> <span class="s">&quot;AFTER&quot;</span><span class="p">);</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/13.barrier/barrier.c</em></p>
</div>
<div class="section" id="timing-code-using-the-barrier-coordination-pattern">
<h2>14. Timing code using the Barrier Coordination Pattern<a class="headerlink" href="#timing-code-using-the-barrier-coordination-pattern" title="Permalink to this headline">¶</a></h2>
<p>In this example you can run the code several times and determine the average, median, and minimum
execution time when the code has a barrier and when it does not. The primary purpose of this exercise
is to illustrate that one of the most useful uses of a barrier is to ensure that you are getting legitimate
timings for your code examples. By using a barrier, you ensure that all processes have finished before
recording the time using the master node.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* barrier+timing.c </span>
<span class="cm"> *  ... illustrates the behavior of MPI_Barrier() </span>
<span class="cm"> *       to coordinate process-timing.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, April 2016</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 8 ./barrier+timing</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> *  - Compile; then run the program five times, </span>
<span class="cm"> *  - In a spreadsheet, compute the average,</span>
<span class="cm"> *     median, and minimum of the five times.</span>
<span class="cm"> *  - Uncomment the two MPI_Barrier() calls;</span>
<span class="cm"> *     then recompile, rerun five times, and</span>
<span class="cm"> *     compute the new average, median, and min</span>
<span class="cm"> *     times.</span>
<span class="cm"> *  - Why did uncommenting the barrier calls</span>
<span class="cm"> *     produce the change you observed?</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;     </span><span class="c1">// MPI</span>
<span class="cp">#include &lt;unistd.h&gt;  </span><span class="c1">// sleep()</span>

<span class="cp">#define  MASTER 0</span>

<span class="cm">/* answer the ultimate question of life, the universe, </span>
<span class="cm"> *  and everything, based on id and numProcs.</span>
<span class="cm"> * @param: id, an int</span>
<span class="cm"> * @param: numProcs, an int</span>
<span class="cm"> * Precondition: id is the MPI rank of this process</span>
<span class="cm"> *             &amp;&amp; numProcs is the number of MPI processes.</span>
<span class="cm"> * Postcondition: The return value is 42.</span>
<span class="cm"> */</span>
<span class="kt">int</span> <span class="nf">solveProblem</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numProcs</span><span class="p">)</span> <span class="p">{</span>

    <span class="n">sleep</span><span class="p">(</span> <span class="p">((</span><span class="kt">double</span><span class="p">)</span><span class="n">id</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">numProcs</span><span class="p">);</span>

    <span class="k">return</span> <span class="mi">42</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">double</span> <span class="n">startTime</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">totalTime</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">answer</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

<span class="c1">//    MPI_Barrier(MPI_COMM_WORLD);</span>
    <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">==</span> <span class="n">MASTER</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">startTime</span> <span class="o">=</span> <span class="n">MPI_Wtime</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="n">answer</span> <span class="o">=</span> <span class="n">solveProblem</span><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">);</span>

<span class="c1">//    MPI_Barrier(MPI_COMM_WORLD);</span>
    <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">==</span> <span class="n">MASTER</span> <span class="p">)</span> <span class="p">{</span>
        <span class="n">totalTime</span> <span class="o">=</span> <span class="n">MPI_Wtime</span><span class="p">()</span> <span class="o">-</span> <span class="n">startTime</span><span class="p">;</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">The answer is %d; computing it took %f secs.</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">,</span>
                   <span class="n">answer</span><span class="p">,</span> <span class="n">totalTime</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/14.barrier+Timing/barrier+timing.c</em></p>
</div>
<div class="section" id="sequence-numbers">
<h2>15. Sequence Numbers<a class="headerlink" href="#sequence-numbers" title="Permalink to this headline">¶</a></h2>
<p>Tags can be placed on messages that are sent from a non-master node and received by the master node.
Using tags is an alternative form of simulating the barrier example in example 13 above.</p>
<div class="highlight-c"><div class="highlight"><pre><span class="cm">/* sequenceNumbers.c</span>
<span class="cm"> *  ... shows how to acheive barrier-like behavior</span>
<span class="cm"> *      by using MPI message tags as sequence numbers.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, April 2016.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 8 ./sequenceNumbers</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> * 1. Compile; then run the program several times, </span>
<span class="cm"> *     noting the intermixed outputs</span>
<span class="cm"> * 2. Comment out the sendReceivePrint(..., &quot;SECOND&quot;, 1); call;</span>
<span class="cm"> *      uncomment the sendReceivePrint(..., &quot;SECOND&quot;, 2); call;</span>
<span class="cm"> *      then recompile and rerun, noting how the output changes.</span>
<span class="cm"> * 3. Uncomment the sendReceivePrint(..., &quot;THIRD&quot;, 3); </span>
<span class="cm"> *      and sendReceivePrint(..., &quot;FOURTH&quot;, 4); calls,</span>
<span class="cm"> *      then recompile and rerun, noting how the output changes.</span>
<span class="cm"> * 4. Explain the differences: what has caused the changes</span>
<span class="cm"> *      in the program&#39;s behavior, and why?</span>
<span class="cm"> */</span>

<span class="cp">#include &lt;stdio.h&gt;   </span><span class="c1">// printf()</span>
<span class="cp">#include &lt;mpi.h&gt;     </span><span class="c1">// MPI</span>

<span class="cm">/* Have workers send messages to the master, which prints them.</span>
<span class="cm"> * @param: id, an int</span>
<span class="cm"> * @param: numProcesses, an int</span>
<span class="cm"> * @param: hostName, a char*</span>
<span class="cm"> * @param: messageNum, a char*</span>
<span class="cm"> * @param: tagValue, an int</span>
<span class="cm"> *</span>
<span class="cm"> * Precondition: this routine is being called by an MPI process </span>
<span class="cm"> *               &amp;&amp; id is the MPI rank of that process </span>
<span class="cm"> *               &amp;&amp; numProcesses is the number of processes in the computation </span>
<span class="cm"> *               &amp;&amp; hostName points to a char array containing the name of the</span>
<span class="cm"> *                    host on which this MPI process is running </span>
<span class="cm"> *               &amp;&amp; messageNum is &quot;FIRST&quot;, &quot;SECOND&quot;, &quot;THIRD&quot;, ...</span>
<span class="cm"> *               &amp;&amp; tagValue is the value for the tags of the message</span>
<span class="cm"> *                    being sent and received this invocation of the function.</span>
<span class="cm"> *</span>
<span class="cm"> * Postcondition: each process whose id &gt; 0 has sent a message to process 0</span>
<span class="cm"> *                    containing id, numProcesses, hostName, messageNum,</span>
<span class="cm"> *                    and tagValue </span>
<span class="cm"> *                &amp;&amp; process 0 has received and output each message.</span>
<span class="cm"> */</span>

<span class="cp">#define BUFFER_SIZE 200</span>
<span class="cp">#define MASTER      0</span>

<span class="kt">void</span> <span class="nf">sendReceivePrint</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">hostName</span><span class="p">,</span>
                        <span class="kt">char</span><span class="o">*</span> <span class="n">messageNum</span><span class="p">,</span> <span class="kt">int</span> <span class="n">tagValue</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">char</span> <span class="n">buffer</span><span class="p">[</span><span class="n">BUFFER_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sc">&#39;\0&#39;</span><span class="p">};;</span>
    <span class="n">MPI_Status</span> <span class="n">status</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">!=</span> <span class="n">MASTER</span><span class="p">)</span> <span class="p">{</span> 
        <span class="c1">// Worker: Build a message and send it to the Master</span>
        <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">sprintf</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span>
                              <span class="s">&quot;This is the %s message from process #%d of %d on %s.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                                <span class="n">messageNum</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">hostName</span><span class="p">);</span>       
        <span class="n">MPI_Send</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">length</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tagValue</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="c1">// Master: Receive and print the messages from all Workers</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numProcesses</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
           <span class="n">MPI_Recv</span><span class="p">(</span><span class="n">buffer</span><span class="p">,</span> <span class="n">BUFFER_SIZE</span><span class="p">,</span> <span class="n">MPI_CHAR</span><span class="p">,</span> <span class="n">MPI_ANY_SOURCE</span><span class="p">,</span>
                     <span class="n">tagValue</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">status</span><span class="p">);</span>
           <span class="n">printf</span><span class="p">(</span><span class="n">buffer</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>


<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">myHostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sc">&#39;\0&#39;</span><span class="p">};</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>
    <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">myHostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

    <span class="n">sendReceivePrint</span><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">myHostName</span><span class="p">,</span> <span class="s">&quot;FIRST&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

    <span class="n">sendReceivePrint</span><span class="p">(</span><span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">myHostName</span><span class="p">,</span> <span class="s">&quot;SECOND&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="c1">//    sendReceivePrint(id, numProcesses, myHostName, &quot;SECOND&quot;, 2);</span>
<span class="c1">//    sendReceivePrint(id, numProcesses, myHostName, &quot;THIRD&quot;, 3);</span>
<span class="c1">//    sendReceivePrint(id, numProcesses, myHostName, &quot;FOURTH&quot;, 4);</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><em>file: patternlets/MPI/15.sequenceNumbers/sequenceNumbers.c</em></p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Message Passing Parallel Patternlets</a><ul>
<li><a class="reference internal" href="#source-code">Source Code</a></li>
<li><a class="reference internal" href="#single-program-multiple-data">00. Single Program, Multiple Data</a></li>
<li><a class="reference internal" href="#the-master-worker-implementation-strategy-pattern">01. The Master-Worker Implementation Strategy Pattern</a></li>
<li><a class="reference internal" href="#message-passing-1-using-send-receive-of-a-single-value">02. Message passing 1, using Send-Receive of a single value</a></li>
<li><a class="reference internal" href="#message-passing-2-using-send-receive-of-an-array-of-values">03. Message passing 2,  using Send-Receive of an array of values</a></li>
<li><a class="reference internal" href="#message-passing-3-using-send-receive-with-master-worker-pattern">04. Message passing 3,  using Send-Receive with master-worker pattern</a></li>
<li><a class="reference internal" href="#data-decomposition-on-equal-sized-chunks-using-parallel-for">05. Data Decomposition: on <em>equal-sized chunks</em> using parallel-for</a></li>
<li><a class="reference internal" href="#data-decomposition-on-chunks-of-size-1-using-parallel-for">06. Data Decomposition: on <em>chunks of size 1</em> using parallel-for</a></li>
<li><a class="reference internal" href="#broadcast-a-special-form-of-message-passing">07. Broadcast: a special form of message passing</a></li>
<li><a class="reference internal" href="#broadcast-send-data-to-all-processes">08. Broadcast: send data to all processes</a></li>
<li><a class="reference internal" href="#collective-communication-reduction">09. Collective Communication: Reduction</a></li>
<li><a class="reference internal" href="#id1">10. Collective Communication: Reduction</a></li>
<li><a class="reference internal" href="#collective-communication-scatter-for-message-passing-data-decomposition">11. Collective communication: Scatter for message-passing data decomposition</a></li>
<li><a class="reference internal" href="#collective-communication-gather-for-message-passing-data-decomposition">12. Collective communication: Gather for message-passing data decomposition</a></li>
<li><a class="reference internal" href="#the-barrier-coordination-pattern">13. The Barrier Coordination Pattern</a></li>
<li><a class="reference internal" href="#timing-code-using-the-barrier-coordination-pattern">14. Timing code using the Barrier Coordination Pattern</a></li>
<li><a class="reference internal" href="#sequence-numbers">15. Sequence Numbers</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../PatternsIntro.html"
                        title="previous chapter">Parallel Programming Patterns</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../SharedMemory/OpenMP_Patternlets.html"
                        title="next chapter">Shared Memory Parallel Patternlets in OpenMP</a></p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../SharedMemory/OpenMP_Patternlets.html" title="Shared Memory Parallel Patternlets in OpenMP"
             >next</a></li>
        <li class="right" >
          <a href="../PatternsIntro.html" title="Parallel Programming Patterns"
             >previous</a> |</li>
        <li><a href="../index.html">Parallel Patternlets</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3.
    </div>
  </body>
</html>