
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Data Decomposition, Reduction, Scatter and Gather &#8212; Parallel Patternlets</title>
    
    <link rel="stylesheet" href="../_static/csip.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Barrier Synchronization and Tags" href="Barrier_Tags.html" />
    <link rel="prev" title="Broadcast" href="Broadcast.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="Barrier_Tags.html" title="Barrier Synchronization and Tags"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="Broadcast.html" title="Broadcast"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Parallel Patternlets</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="MPI_Patternlets.html" accesskey="U">Message Passing Parallel Patternlets</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="data-decomposition-reduction-scatter-and-gather">
<h1>Data Decomposition, Reduction, Scatter and Gather<a class="headerlink" href="#data-decomposition-reduction-scatter-and-gather" title="Permalink to this headline">¶</a></h1>
<div class="section" id="collective-communication-reduction">
<h2>09. Collective Communication: Reduction<a class="headerlink" href="#collective-communication-reduction" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/10.reduction/reduction.c</em></p>
<p><em>Build inside 10.reduction directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">make</span> <span class="n">reduction</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 10.reduction directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">reduction</span>
</pre></div>
</div>
<p>Once processes have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite common to then <em>reduce</em>
those individual computations into one value. This example shows a simple
calculation done by each process being reduced to a sum and a maximum.
In this example, MPI, has built-in computations, indicated by MPI_SUM and
MPI_MAX in the following code. With four processes, the code is implemented
like this:</p>
<a class="reference internal image-reference" href="../_images/Reduction.png"><img alt="../_images/Reduction.png" src="../_images/Reduction.png" style="width: 800px;" /></a>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* reduction.c</span>
<span class="cm"> * ... illustrates the use of MPI_Reduce()...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./reduction</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise: </span>
<span class="cm"> * - Compile and run, varying N: 4, 6, 8, 10.</span>
<span class="cm"> * - Explain behavior of MPI_Reduce().</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdlib.h&gt;</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">numProcs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">max</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

	<span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
        <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
        <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

	<span class="n">square</span> <span class="o">=</span> <span class="p">(</span><span class="n">myRank</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">myRank</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
     
	<span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d computed %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">myRank</span><span class="p">,</span> <span class="n">square</span><span class="p">);</span>

        <span class="n">MPI_Reduce</span><span class="p">(</span><span class="o">&amp;</span><span class="n">square</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">sum</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_SUM</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

        <span class="n">MPI_Reduce</span><span class="p">(</span><span class="o">&amp;</span><span class="n">square</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">max</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_MAX</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

	<span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">The sum of the squares is %d</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">&quot;The max of the squares is %d</span><span class="se">\n\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">max</span><span class="p">);</span>
	<span class="p">}</span>

 	<span class="n">MPI_Finalize</span><span class="p">();</span>

	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="id1">
<h2>11. Collective Communication: Reduction<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/11.reduction2/reduction2.c</em></p>
<p><em>Build inside 11.reduction2 directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">make</span> <span class="n">reduction2</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 11.reduction2 directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">reduction2</span>
</pre></div>
</div>
<p>Here is a second reduction example using arrays of data.</p>
<div class="topic">
<p class="topic-title first">To do:</p>
<p>Can you explain the behavior of the reduction, MPI_reduce(), in terms of
srcArr and destArr?</p>
</div>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* reduction2.c</span>
<span class="cm"> * ... illustrates the use of MPI_Reduce() using arrays...</span>
<span class="cm"> * Joel Adams, Calvin College, January 2015.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 4 ./reduction2</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, comparing output to source code.</span>
<span class="cm"> * - Uncomment the &#39;commented out&#39; call to printArray.</span>
<span class="cm"> * - Save, recompile, rerun, comparing output to source code.</span>
<span class="cm"> * - Explain behavior of MPI_Reduce() in terms of </span>
<span class="cm"> *     srcArr and destArr.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>

<span class="cp">#define ARRAY_SIZE 5</span>

<span class="kt">void</span> <span class="nf">printArray</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrayName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="kt">int</span> <span class="n">SIZE</span><span class="p">);</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">srcArr</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">int</span> <span class="n">destArr</span><span class="p">[</span><span class="n">ARRAY_SIZE</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">Before reduction: &quot;</span><span class="p">);</span>
        <span class="n">printArray</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;destArr&quot;</span><span class="p">,</span> <span class="n">destArr</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="p">);</span>
    <span class="p">}</span> 

    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ARRAY_SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">srcArr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">myRank</span> <span class="o">*</span> <span class="n">i</span><span class="p">;</span>
    <span class="p">}</span>

<span class="c1">//    printArray(myRank, &quot;srcArr&quot;, srcArr, ARRAY_SIZE);</span>

    <span class="n">MPI_Reduce</span><span class="p">(</span><span class="n">srcArr</span><span class="p">,</span> <span class="n">destArr</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">MPI_SUM</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">After reduction:  &quot;</span><span class="p">);</span>
        <span class="n">printArray</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;destArr&quot;</span><span class="p">,</span> <span class="n">destArr</span><span class="p">,</span> <span class="n">ARRAY_SIZE</span><span class="p">);</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="p">}</span> 

    <span class="n">MPI_Finalize</span><span class="p">();</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="cm">/* utility to display an array</span>
<span class="cm"> * params: id, the rank of the current process</span>
<span class="cm"> *         arrayName, the name of the array being displayed</span>
<span class="cm"> *         array, the array being displayed</span>
<span class="cm"> *         SIZE, the number of items in array.</span>
<span class="cm"> * postcondition:</span>
<span class="cm"> *         the id, name, and items in array have been printed to stdout.</span>
<span class="cm"> */</span>
<span class="kt">void</span> <span class="nf">printArray</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrayName</span><span class="p">,</span> <span class="kt">int</span> <span class="o">*</span> <span class="n">array</span><span class="p">,</span> <span class="kt">int</span> <span class="n">SIZE</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d, %s: [&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">arrayName</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%3d&quot;</span><span class="p">,</span> <span class="n">array</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">SIZE</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="n">printf</span><span class="p">(</span><span class="s">&quot;,&quot;</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;]</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>

</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="data-decomposition-on-equal-sized-chunks-using-parallel-for">
<h2>12. Data Decomposition: on <em>equal-sized chunks</em> using parallel-for<a class="headerlink" href="#data-decomposition-on-equal-sized-chunks-using-parallel-for" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/12.parallelLoop-equalChunks/parallelLoopEqualChunks.c</em></p>
<p><em>Build inside 12.parallelLoop-equalChunks directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">make</span> <span class="n">parallelLoopEqualChunks</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 12.parallelLoop-equalChunks directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">parallelLoopEqualChunks</span>
</pre></div>
</div>
<p>In this example, the data being decomposed is simply the set of integers
from zero to REPS * numProcesses, which are used in the for loop. Thus,
each process receives REPS / numProcesses iterations to perform, thereby
enforcing the <em>equal-sized chunks</em> pattern. This type of decomposition is
commonly used when accessing data that is stored in consecutive memory locations
(such as an array). Verify that the program behavior is:</p>
<a class="reference internal image-reference" href="../_images/EqualChunks.png"><img alt="../_images/EqualChunks.png" src="../_images/EqualChunks.png" style="width: 800px;" /></a>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* parallelLoopEqualChunks.c</span>
<span class="cm"> * ... illustrates the parallel for loop pattern in MPI </span>
<span class="cm"> *	in which processes perform the loop&#39;s iterations in equal-sized &#39;chunks&#39; </span>
<span class="cm"> *	(preferable when loop iterations access memory/cache locations) ...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./parallelForEqualChunks</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8</span>
<span class="cm"> * - Change REPS to 16, save, recompile, rerun, varying N again.</span>
<span class="cm"> * - Explain how this pattern divides the iterations of the loop</span>
<span class="cm"> *    among the processes.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt; // printf()</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;   // MPI</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;math.h&gt;  // ceil()</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">start</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">stop</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">chunkSize</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="n">chunkSize</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="n">ceil</span><span class="p">(((</span><span class="kt">double</span><span class="p">)</span><span class="n">REPS</span><span class="p">)</span> <span class="o">/</span> <span class="n">numProcesses</span><span class="p">);</span> <span class="c1">// find chunk size</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">id</span> <span class="o">*</span> <span class="n">chunkSize</span><span class="p">;</span>                               <span class="c1">// find starting index</span>
                                                          <span class="c1">// find stopping index:</span>
    <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">&lt;</span> <span class="n">numProcesses</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">)</span> <span class="p">{</span>                        <span class="c1">//  if not the last process</span>
        <span class="n">stop</span> <span class="o">=</span> <span class="p">(</span><span class="n">id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunkSize</span><span class="p">;</span>                      <span class="c1">//   stop where next process starts</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>                                              <span class="c1">//  else </span>
        <span class="n">stop</span> <span class="o">=</span> <span class="n">REPS</span><span class="p">;</span>                                      <span class="c1">//   last process does leftovers </span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">stop</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>                      <span class="c1">// iterate through our range </span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d is performing iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="data-decomposition-on-chunks-of-size-1-using-parallel-for">
<h2>13. Data Decomposition: on <em>chunks of size 1</em> using parallel-for<a class="headerlink" href="#data-decomposition-on-chunks-of-size-1-using-parallel-for" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/13.parallelLoop-chunksOf1/parallelLoopChunksOf1.c</em></p>
<p><em>Build inside 13.parallelLoop-chunksOf1 directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">make</span> <span class="n">parallelLoopChunksOf1</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 13.parallelLoop-chunksOf1 directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">parallelLoopChunksOf1</span>
</pre></div>
</div>
<p>A simple decomposition sometimes used when your loop is not accessing consecutive
memory locations would be to let each process do one iteration, up to N processes,
then start again with process 0 taking the next iteration. A for loop on line 29
is used to implement this type of data decomposition.</p>
<a class="reference internal image-reference" href="../_images/ChunksOf1.png"><img alt="../_images/ChunksOf1.png" src="../_images/ChunksOf1.png" style="width: 800px;" /></a>
<p>This is a basic example that does not yet include a data array, though
it would typically be used when each process would be working on a portion
of an array that could have been looped over in a sequential solution.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* parallelLoopChunksOf1.c</span>
<span class="cm"> * ... illustrates the parallel for loop pattern in MPI</span>
<span class="cm"> *	in which processes perform the loop&#39;s iterations in &#39;chunks&#39;</span>
<span class="cm"> *      of size 1 (simple, and useful when loop iterations</span>
<span class="cm"> *      do not access memory/cache locations) ...</span>
<span class="cm"> * Note this is much simpler than the &#39;equal chunks&#39; loop.</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./parallelLoopChunksOf1</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8</span>
<span class="cm"> * - Change REPS to 16, save, recompile, rerun, varying N again.</span>
<span class="cm"> * - Explain how this pattern divides the iterations of the loop</span>
<span class="cm"> *    among the processes.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;  // printf()</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;    // MPI</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">REPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">numProcesses</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d is performing iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="collective-communication-scatter-for-message-passing-data-decomposition">
<h2>14. Collective communication: Scatter for message-passing data decomposition<a class="headerlink" href="#collective-communication-scatter-for-message-passing-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/14.scatter/scatter.c</em></p>
<p><em>Build inside 14.scatter directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">make</span> <span class="n">scatter</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 14.scatter directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">scatter</span>
</pre></div>
</div>
<p>If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern, the scatter pattern can be
used to ensure that each process receives a copy of its portion of the array.
Process 0 gets the first chunk, process 1 gets the second chunk and so on until
the entire array has been distributed.</p>
<div class="topic">
<p class="topic-title first">To do:</p>
<p>What previous data decomposition pattern is this similar to?</p>
</div>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* scatter.c</span>
<span class="cm"> * ... illustrates the use of MPI_Scatter()...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./scatter</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 4, 8</span>
<span class="cm"> * - Trace execution through source code.</span>
<span class="cm"> * - Explain behavior/effect of MPI_Scatter().</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;      // MPI</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;    // printf(), etc.</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdlib.h&gt;   // malloc()</span><span class="cp"></span>

<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">);</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">MAX</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">arrSend</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="kt">int</span><span class="o">*</span> <span class="n">arrRcv</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">numProcs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numSent</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>                            <span class="c1">// initialize</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>                                 <span class="c1">// master process:</span>
        <span class="n">arrSend</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">MAX</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>  <span class="c1">//  allocate array1</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">MAX</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>                <span class="c1">//  load with values</span>
            <span class="n">arrSend</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">11</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;arrSend&quot;</span><span class="p">,</span> <span class="n">arrSend</span><span class="p">,</span> <span class="n">MAX</span><span class="p">);</span>        <span class="c1">//  display array1</span>
    <span class="p">}</span>
     
    <span class="n">numSent</span> <span class="o">=</span> <span class="n">MAX</span> <span class="o">/</span> <span class="n">numProcs</span><span class="p">;</span>                          <span class="c1">// all processes:</span>
    <span class="n">arrRcv</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">numSent</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>   <span class="c1">//  allocate array2</span>

    <span class="n">MPI_Scatter</span><span class="p">(</span><span class="n">arrSend</span><span class="p">,</span> <span class="n">numSent</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="n">arrRcv</span><span class="p">,</span>     <span class="c1">//  scatter array1 </span>
                 <span class="n">numSent</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span> <span class="c1">//   into array2</span>

    <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;arrRcv&quot;</span><span class="p">,</span> <span class="n">arrRcv</span><span class="p">,</span> <span class="n">numSent</span><span class="p">);</span>          <span class="c1">// display array2</span>

    <span class="n">free</span><span class="p">(</span><span class="n">arrSend</span><span class="p">);</span>                                     <span class="c1">// clean up</span>
    <span class="n">free</span><span class="p">(</span><span class="n">arrRcv</span><span class="p">);</span>
    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d, %s: &quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">arrName</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">arrSize</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot; %d&quot;</span><span class="p">,</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>

</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="collective-communication-gather-for-message-passing-data-decomposition">
<h2>15. Collective communication: Gather for message-passing data decomposition<a class="headerlink" href="#collective-communication-gather-for-message-passing-data-decomposition" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/15.gather/gather.c</em></p>
<p><em>Build inside 15.gather directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">make</span> <span class="n">gather</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 15.gather directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">gather</span>
</pre></div>
</div>
<p>If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the gather pattern can be used to ensure that each process sends
a copy of its portion of the array back to the root, or master process.
Thus, gather is the reverse of scatter. Here is the idea:</p>
<a class="reference internal image-reference" href="../_images/Gather.png"><img alt="../_images/Gather.png" src="../_images/Gather.png" style="width: 750px;" /></a>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65</pre></div></td><td class="code"><div class="highlight"><pre><span class="cm">/* gather.c</span>
<span class="cm"> * ... illustrates the use of MPI_Gather()...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./gather</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 4, 8.</span>
<span class="cm"> * - Trace execution through source.</span>
<span class="cm"> * - Explain behavior of MPI_Gather().</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;       // MPI</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;     // printf()</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdlib.h&gt;    // malloc()</span><span class="cp"></span>

<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">);</span>

<span class="cp">#define SIZE 3</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
   <span class="kt">int</span>  <span class="n">computeArray</span><span class="p">[</span><span class="n">SIZE</span><span class="p">];</span>                          <span class="c1">// array1</span>
   <span class="kt">int</span><span class="o">*</span> <span class="n">gatherArray</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>                          <span class="c1">// array2</span>
   <span class="kt">int</span>  <span class="n">numProcs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">myRank</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> 
        <span class="n">totalGatheredVals</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

   <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>                           <span class="c1">// initialize</span>
   <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcs</span><span class="p">);</span>
   <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">myRank</span><span class="p">);</span>
                                                     <span class="c1">// all processes:</span>
   <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">SIZE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>                  <span class="c1">//  load array1 with</span>
      <span class="n">computeArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">myRank</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">i</span><span class="p">;</span>             <span class="c1">//   3 distinct values</span>
   <span class="p">}</span>

   <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;computeArray&quot;</span><span class="p">,</span> <span class="n">computeArray</span><span class="p">,</span>       <span class="c1">//  show array1</span>
           <span class="n">SIZE</span><span class="p">);</span>

   <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>                                <span class="c1">// master:</span>
      <span class="n">totalGatheredVals</span> <span class="o">=</span> <span class="n">SIZE</span> <span class="o">*</span> <span class="n">numProcs</span><span class="p">;</span>           <span class="c1">//  allocate array2</span>
      <span class="n">gatherArray</span> <span class="o">=</span> <span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">)</span> <span class="n">malloc</span><span class="p">(</span> <span class="n">totalGatheredVals</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">)</span> <span class="p">);</span>
   <span class="p">}</span>

   <span class="n">MPI_Gather</span><span class="p">(</span><span class="n">computeArray</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span>           <span class="c1">//  gather array1 vals</span>
               <span class="n">gatherArray</span><span class="p">,</span> <span class="n">SIZE</span><span class="p">,</span> <span class="n">MPI_INT</span><span class="p">,</span>           <span class="c1">//   into array2</span>
               <span class="mi">0</span><span class="p">,</span> <span class="n">MPI_COMM_WORLD</span><span class="p">);</span>                   <span class="c1">//   at master process               </span>

   <span class="k">if</span> <span class="p">(</span><span class="n">myRank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>                                <span class="c1">// master process:</span>
      <span class="n">print</span><span class="p">(</span><span class="n">myRank</span><span class="p">,</span> <span class="s">&quot;gatherArray&quot;</span><span class="p">,</span>                   <span class="c1">//  show array2</span>
             <span class="n">gatherArray</span><span class="p">,</span> <span class="n">totalGatheredVals</span><span class="p">);</span> 
   <span class="p">}</span>

   <span class="n">free</span><span class="p">(</span><span class="n">gatherArray</span><span class="p">);</span>                                <span class="c1">// clean up</span>
   <span class="n">MPI_Finalize</span><span class="p">();</span>
   <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">print</span><span class="p">(</span><span class="kt">int</span> <span class="n">id</span><span class="p">,</span> <span class="kt">char</span><span class="o">*</span> <span class="n">arrName</span><span class="p">,</span> <span class="kt">int</span><span class="o">*</span> <span class="n">arr</span><span class="p">,</span> <span class="kt">int</span> <span class="n">arrSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d, %s: &quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">arrName</span><span class="p">);</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">arrSize</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot; %d&quot;</span><span class="p">,</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>


</pre></div>
</td></tr></table></div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Data Decomposition, Reduction, Scatter and Gather</a><ul>
<li><a class="reference internal" href="#collective-communication-reduction">09. Collective Communication: Reduction</a></li>
<li><a class="reference internal" href="#id1">11. Collective Communication: Reduction</a></li>
<li><a class="reference internal" href="#data-decomposition-on-equal-sized-chunks-using-parallel-for">12. Data Decomposition: on <em>equal-sized chunks</em> using parallel-for</a></li>
<li><a class="reference internal" href="#data-decomposition-on-chunks-of-size-1-using-parallel-for">13. Data Decomposition: on <em>chunks of size 1</em> using parallel-for</a></li>
<li><a class="reference internal" href="#collective-communication-scatter-for-message-passing-data-decomposition">14. Collective communication: Scatter for message-passing data decomposition</a></li>
<li><a class="reference internal" href="#collective-communication-gather-for-message-passing-data-decomposition">15. Collective communication: Gather for message-passing data decomposition</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="Broadcast.html"
                        title="previous chapter">Broadcast</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="Barrier_Tags.html"
                        title="next chapter">Barrier Synchronization and Tags</a></p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="Barrier_Tags.html" title="Barrier Synchronization and Tags"
             >next</a></li>
        <li class="right" >
          <a href="Broadcast.html" title="Broadcast"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Parallel Patternlets</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="MPI_Patternlets.html" >Message Passing Parallel Patternlets</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.2.
    </div>
  </body>
</html>