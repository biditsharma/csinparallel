
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Program Structure: SPMD, Master Worker, and Parallel Loops &#8212; Parallel Patternlets</title>
    <link rel="stylesheet" href="../_static/csip.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Communication Patterns in MPI: Basic message passing" href="Communication.html" />
    <link rel="prev" title="Running the examples on your cluster" href="RunningMPI.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="Communication.html" title="Communication Patterns in MPI: Basic message passing"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="RunningMPI.html" title="Running the examples on your cluster"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Parallel Patternlets</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="MPI_Patternlets.html" accesskey="U">Message Passing Parallel Patternlets</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="program-structure-spmd-master-worker-and-parallel-loops">
<h1>Program Structure: SPMD, Master Worker, and Parallel Loops<a class="headerlink" href="#program-structure-spmd-master-worker-and-parallel-loops" title="Permalink to this headline">¶</a></h1>
<p>This initial set of MPI pattern examples illustrates how
many distributed processing programs are <strong>structured</strong>.
For this examples it is useful to look at the overall
organization of the program and become comfortable with the idea
that multiple processes are all running this code simultaneously,
in no particular guaranteed order.</p>
<div class="section" id="single-program-multiple-data">
<h2>00. Single Program, Multiple Data<a class="headerlink" href="#single-program-multiple-data" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/00.spmd/spmd.c</em></p>
<p><em>Build inside 00.spmd directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">spmd</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 00.spmd directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">spmd</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This command is going to run all processes on the machine on which you
type it.
See <a class="reference internal" href="RunningMPI.html"><span class="doc">Running the examples on your cluster</span></a> for more information about running the code
on a cluster of machines. This note applies for all the examples below.</p>
</div>
<p>First let us illustrate the basic components of an MPI program,
which by its nature uses a single program that runs on each process.
Note what gets printed is different for each process, thus the
processes using this one single program can have different data values
for its variables.  This is why we call it single program, multiple data.</p>
<p>On the command line, <em>mpirun</em> tells the system to start &lt;number of processes&gt;
instances of the program. The call to <em>MPI_INIT</em> on line 25 tells the MPI
system to setup. This includes allocating storage for message buffers and
deciding the rank each process receives. <em>MPI_INIT</em> also defines a communicator
called <em>MPI_COMM_WORLD</em>. A communicator is a group of processes that can
communicate with each other by sending messages. The <em>MPI_Finalize</em> command
tells the MPI system that we are finished and it deallocates MPI resources.</p>
<div class="topic">
<p class="topic-title first">To do:</p>
<p>Can you determine the purpose of the <em>MPI_Comm_rank</em> function and
<em>MPI_Comm_size</em> function? How is the communicator related to these functions?</p>
</div>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="cm">/* spmd.c</span>
<span class="cm"> * ... illustrates the single program multiple data</span>
<span class="cm"> *      (SPMD) pattern using basic MPI commands.</span>
<span class="cm"> *</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np 4 ./spmd</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run.</span>
<span class="cm"> * - Compare source code to output.</span>
<span class="cm"> * - Rerun, using varying numbers of processes</span>
<span class="cm"> *    (i.e., vary the argument to &#39;mpirun -np&#39;).</span>
<span class="cm"> * - Explain what &quot;multiple data&quot; values this</span>
<span class="cm"> *    &quot;single program&quot; is generating.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;   // printf()</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;     // MPI functions</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
    <span class="kt">char</span> <span class="n">myHostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">];</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>
    <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">myHostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from process #%d of %d on %s</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
             <span class="n">id</span><span class="p">,</span> <span class="n">numProcesses</span><span class="p">,</span> <span class="n">myHostName</span><span class="p">);</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="the-master-worker-implementation-strategy-pattern">
<h2>01. The Master-Worker Implementation Strategy Pattern<a class="headerlink" href="#the-master-worker-implementation-strategy-pattern" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/01.masterWorker/masterWorker.c</em></p>
<p><em>Build inside 01.masterWorker directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">masterWorker</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 01.masterWorker directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">masterWorker</span>
</pre></div>
</div>
<p>The master worker pattern is illustrated in this simple example.  The pattern
consists of one process, called the master, executing one block of code while
the rest of the processes, called workers, are executing a different block of code.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="cm">/* masterWorker.c</span>
<span class="cm"> * ... illustrates the basic master-worker pattern in MPI ...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./masterWorker</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run the program, varying N from 1 through 8.</span>
<span class="cm"> * - Explain what stays the same and what changes as the</span>
<span class="cm"> *    number of processes changes.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numWorkers</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">length</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>
  <span class="kt">char</span> <span class="n">hostName</span><span class="p">[</span><span class="n">MPI_MAX_PROCESSOR_NAME</span><span class="p">];</span>

  <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
  <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
  <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numWorkers</span><span class="p">);</span>
  <span class="n">MPI_Get_processor_name</span> <span class="p">(</span><span class="n">hostName</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">length</span><span class="p">);</span>

  <span class="k">if</span> <span class="p">(</span> <span class="n">id</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span> <span class="p">{</span>  <span class="c1">// process 0 is the master </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from the master, #%d (%s) of %d processes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
             <span class="n">id</span><span class="p">,</span> <span class="n">hostName</span><span class="p">,</span> <span class="n">numWorkers</span><span class="p">);</span>
  <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>          <span class="c1">// processes with ids &gt; 0 are workers </span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Greetings from a worker, #%d (%s) of %d processes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
             <span class="n">id</span><span class="p">,</span> <span class="n">hostName</span><span class="p">,</span> <span class="n">numWorkers</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="n">MPI_Finalize</span><span class="p">();</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="data-decomposition-on-equal-sized-chunks-using-parallel-for">
<h2>02. Data Decomposition: on <em>equal-sized chunks</em> using parallel-for<a class="headerlink" href="#data-decomposition-on-equal-sized-chunks-using-parallel-for" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/02.parallelLoop-equalChunks/parallelLoopEqualChunks.c</em></p>
<p><em>Build inside 02.parallelLoop-equalChunks directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">parallelLoopEqualChunks</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 02.parallelLoop-equalChunks directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">parallelLoopEqualChunks</span>
</pre></div>
</div>
<p>The <strong>data decomposition</strong> pattern occurs in code in two ways:</p>
<blockquote>
<div><p>1. a for loop that traverses  many data elements stored in an array
(1-dimensional or more). If each element in an array needs some sort of
computation to be done on it, that work could be split between processes.
This classic data decomposition pattern divides the array into equal-sized
pieces, where each process works on a subset of the array assigned to it.</p>
<p>2. a for loop that simply has a total of N independent iterations to perform
a data calculation of some type. The work can be split into N/P ‘chunks’ of
work, which can be performed on each of P processes.</p>
</div></blockquote>
<p>In this example, we illustrate the second of these two. The total iterations to
perform are numbered from 0 to REPS in the code below. Each process will
complete REPS / numProcesses iterations, and will <cite>start</cite> and <cite>stop</cite> on its
chunk from 0 to, but not including REPS. Since each process receives REPS /
numProcesses consecutive iterations to perform, we declare this an <em>equal-sized
chunks</em> decomposition pattern. This type of decomposition is commonly used when
accessing data that is stored in consecutive memory locations (such as an
array).</p>
<div class="topic">
<p class="topic-title first">To do:</p>
<p>Verify that the program behavior is as follows for 4 processes:</p>
<a class="reference internal image-reference" href="../_images/EqualChunks.png"><img alt="../_images/EqualChunks.png" src="../_images/EqualChunks.png" style="width: 800px;" /></a>
<p>Run it more than once- what do you observe about the order in which the
processes print their iterations? Try it for other numbers of processes from
1 through 8. As you can guess, we cannot always get equal-sized chunks for
all processes, but we can distribute chunks that differ by no more than one
in size. When the REPS are equally divisible by the number of processes (e.g.
2, 4, or 8 processes), the work is equally distributed among the processes.
What happens when this is not the case (3, 5, 6, 7 processes)?</p>
</div>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="cm">/* parallelLoopEqualChunks.c</span>
<span class="cm"> * ... illustrates the parallel for loop pattern in MPI</span>
<span class="cm"> *	in which processes perform the loop&#39;s iterations in equal-sized &#39;chunks&#39;</span>
<span class="cm"> *	(preferable when loop iterations access memory/cache locations) ...</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *    updated by Libby Shoop, Macalester College, 2017</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./parallelForEqualChunks</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 4, and 8</span>
<span class="cm"> * - Change REPS to 16, save, recompile, rerun, varying N again.</span>
<span class="cm"> * - Explain how this pattern divides the iterations of the loop</span>
<span class="cm"> *    among the processes.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt; // printf()</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;   // MPI</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>                      <span class="c1">// repetitions in a loop</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="c1">// In this example, ensure that the REPS can ben evenly divided by the</span>
    <span class="c1">// number of processors and that the number of processes doesn&#39;t exceed REPS.</span>
    <span class="c1">// If either is the case, stop.</span>
    <span class="k">if</span> <span class="p">((</span><span class="n">REPS</span> <span class="o">%</span> <span class="n">numProcesses</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">numProcesses</span> <span class="o">&lt;=</span> <span class="n">REPS</span><span class="p">)</span> <span class="p">{</span>

      <span class="kt">int</span> <span class="n">chunkSize</span> <span class="o">=</span> <span class="n">REPS</span> <span class="o">/</span> <span class="n">numProcesses</span><span class="p">;</span>      <span class="c1">// find chunk size</span>
      <span class="kt">int</span> <span class="n">start</span> <span class="o">=</span> <span class="n">id</span> <span class="o">*</span> <span class="n">chunkSize</span><span class="p">;</span>               <span class="c1">// find starting index</span>
      <span class="kt">int</span> <span class="n">stop</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">chunkSize</span><span class="p">;</span>             <span class="c1">// find stopping index</span>

      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">start</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">stop</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>      <span class="c1">// iterate through our range</span>
          <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d is performing iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
      <span class="p">}</span>

    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Please run with -np divisible by and less than or equal to %d</span><span class="se">\n</span><span class="s">.&quot;</span><span class="p">,</span> <span class="n">REPS</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="data-decomposition-on-chunks-of-size-1-using-parallel-for">
<h2>03. Data Decomposition: on <em>chunks of size 1</em> using parallel-for<a class="headerlink" href="#data-decomposition-on-chunks-of-size-1-using-parallel-for" title="Permalink to this headline">¶</a></h2>
<p><em>file: patternlets/MPI/03.parallelLoop-chunksOf1/parallelLoopChunksOf1.c</em></p>
<p><em>Build inside 03.parallelLoop-chunksOf1 directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">parallelLoopChunksOf1</span>
</pre></div>
</div>
<p><em>Execute on the command line inside 03.parallelLoop-chunksOf1 directory:</em></p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">mpirun</span> <span class="o">-</span><span class="n">np</span> <span class="o">&lt;</span><span class="n">number</span> <span class="n">of</span> <span class="n">processes</span><span class="o">&gt;</span> <span class="o">./</span><span class="n">parallelLoopChunksOf1</span>
</pre></div>
</div>
<p>A simple decomposition sometimes used when your loop is not accessing consecutive
memory locations would be to let each process do one iteration, up to N processes,
then start again with process 0 taking the next iteration. A for loop on line 35
is used to implement this type of data decomposition.</p>
<a class="reference internal image-reference" href="../_images/ChunksOf1.png"><img alt="../_images/ChunksOf1.png" src="../_images/ChunksOf1.png" style="width: 800px;" /></a>
<p>This is a basic example that does not yet include a data array, though
it would typically be used when each process would be working on a portion
of an array that could have been looped over in a sequential solution.</p>
<div class="highlight-c"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="cm">/* parallelLoopChunksOf1.c</span>
<span class="cm"> * ... illustrates the parallel for loop pattern in MPI</span>
<span class="cm"> *	in which processes perform the loop&#39;s iterations in &#39;chunks&#39;</span>
<span class="cm"> *      of size 1 (simple, and useful when loop iterations</span>
<span class="cm"> *      do not access memory/cache locations) ...</span>
<span class="cm"> * Note this is much simpler than the &#39;equal chunks&#39; loop.</span>
<span class="cm"> * Joel Adams, Calvin College, November 2009.</span>
<span class="cm"> *   updated by Libby Shoop, Macalester College, July, 2017</span>
<span class="cm"> *</span>
<span class="cm"> * Usage: mpirun -np N ./parallelLoopChunksOf1</span>
<span class="cm"> *</span>
<span class="cm"> * Exercise:</span>
<span class="cm"> * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8</span>
<span class="cm"> * - Change REPS to 16, save, recompile, rerun, varying N again.</span>
<span class="cm"> * - Explain how this pattern divides the iterations of the loop</span>
<span class="cm"> *    among the processes.</span>
<span class="cm"> */</span>

<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;  // printf()</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mpi.h&gt;    // MPI</span><span class="cp"></span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span><span class="o">**</span> <span class="n">argv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">REPS</span> <span class="o">=</span> <span class="mi">8</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">id</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">numProcesses</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">;</span>

    <span class="n">MPI_Init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">argc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">argv</span><span class="p">);</span>
    <span class="n">MPI_Comm_rank</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">id</span><span class="p">);</span>
    <span class="n">MPI_Comm_size</span><span class="p">(</span><span class="n">MPI_COMM_WORLD</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">numProcesses</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">numProcesses</span> <span class="o">&gt;</span> <span class="n">REPS</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">id</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Please run with -np less than or equal to %d</span><span class="se">\n</span><span class="s">.&quot;</span><span class="p">,</span> <span class="n">REPS</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">id</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">REPS</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">numProcesses</span><span class="p">)</span> <span class="p">{</span>
          <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Process %d is performing iteration %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">id</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">MPI_Finalize</span><span class="p">();</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</td></tr></table></div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Program Structure: SPMD, Master Worker, and Parallel Loops</a><ul>
<li><a class="reference internal" href="#single-program-multiple-data">00. Single Program, Multiple Data</a></li>
<li><a class="reference internal" href="#the-master-worker-implementation-strategy-pattern">01. The Master-Worker Implementation Strategy Pattern</a></li>
<li><a class="reference internal" href="#data-decomposition-on-equal-sized-chunks-using-parallel-for">02. Data Decomposition: on <em>equal-sized chunks</em> using parallel-for</a></li>
<li><a class="reference internal" href="#data-decomposition-on-chunks-of-size-1-using-parallel-for">03. Data Decomposition: on <em>chunks of size 1</em> using parallel-for</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="RunningMPI.html"
                        title="previous chapter">Running the examples on your cluster</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="Communication.html"
                        title="next chapter">Communication Patterns in MPI: Basic message passing</a></p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="Communication.html" title="Communication Patterns in MPI: Basic message passing"
             >next</a></li>
        <li class="right" >
          <a href="RunningMPI.html" title="Running the examples on your cluster"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Parallel Patternlets</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="MPI_Patternlets.html" >Message Passing Parallel Patternlets</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.3.
    </div>
  </body>
</html>