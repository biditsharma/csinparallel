% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,openany,oneside]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}


\title{Parallel Patternlets}
\date{January 21, 2013}
\release{}
\author{CSinParallel Project}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


This document contains simple examples of basic elements that are combined to form
patterns often used in
programs employing parallelism.  The examples are separated between
two major \emph{coordination patterns}:
\begin{enumerate}
\item {} 
message passing used on single multiprocessor machines or clusters of distributed computers, and

\item {} 
mutual exclusion between threads executing concurrently on a single shared memory system.

\end{enumerate}

Both sets of examples are illustrated
with the C programming language, using standard popular available libraries.
The message passing example uses
a C library called MPI (Message Passing Interface).  The mutual Exclusion/shared memory
examples use the OpenMP library.


\chapter{Source Code}
\label{index:source-code}\label{index:parallel-patternlets}
Please download all examples from this tarball:
\code{patternlets.tgz}


\chapter{Patternlet Examples}
\label{index:patternlet-examples}

\section{Message Passing Parallel Patternlets}
\label{MessagePassing/MPI_Patternlets:message-passing-parallel-patternlets}\label{MessagePassing/MPI_Patternlets::doc}
Parallel programs contain \emph{patterns}:  code that recurs over and over again
in solutions to many problems.  The following examples show very simple
examples of small portions of
these patterns that can be combined to solve a problem.  These C code examples use the
Message Passing Interface (MPI) library, which is suitable for use on either a
single pultiprocessor machine or a cluster
of machines.


\subsection{Source Code}
\label{MessagePassing/MPI_Patternlets:source-code}
Please download all examples from this tarball:
\code{patternlets.tgz}

A C code file for each example below can be found in subdirectories of the MPI directory,
along with a makefile and an example of how to execute the program.


\subsection{0. Hello, World}
\label{MessagePassing/MPI_Patternlets:hello-world}
First let us illustrate the basic components of an MPI program.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* hello.c}
\PYG{c+cm}{ * ... illustrates the use of the basic MPI commands...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
	\PYG{k+kt}{char} \PYG{n}{myHostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{myHostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}


	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from process \PYGZpc{}d of \PYGZpc{}d on \PYGZpc{}s}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
		\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{myHostName}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{1. The Master-Worker Implementation Strategy Pattern}
\label{MessagePassing/MPI_Patternlets:the-master-worker-implementation-strategy-pattern}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* masterServer.c}
\PYG{c+cm}{ * ... illustrates the basic master-worker pattern in MPI ...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numWorkers} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
  \PYG{k+kt}{char} \PYG{n}{hostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]}\PYG{p}{;}

  \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numWorkers}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{hostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// process 0 is the master}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from the master, \PYGZsh{} \PYGZpc{}d (\PYGZpc{}s) of \PYGZpc{}d processes}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
            \PYG{n}{id}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{,} \PYG{n}{numWorkers}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// processes with ids \PYGZgt{} 0 are workers}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from a worker, \PYGZsh{} \PYGZpc{}d (\PYGZpc{}s) of \PYGZpc{}d processes}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
            \PYG{n}{id}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{,} \PYG{n}{numWorkers}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

  \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{2. Send-Receive (basic message passing pattern)}
\label{MessagePassing/MPI_Patternlets:send-receive-basic-message-passing-pattern}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* sendRecv.c}
\PYG{c+cm}{ * ... illustrates the use of the MPI\PYGZus{}Send() and MPI\PYGZus{}Recv() commands...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}math.h\PYGZgt{}   }\PYG{c+c1}{// sqrt()}

\PYG{k+kt}{int} \PYG{n+nf}{odd}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{number}\PYG{p}{)} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{number} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;} 
	\PYG{k+kt}{float} \PYG{n}{sendValue} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{receivedValue} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Status} \PYG{n}{status}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{numProcesses} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1} \PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}} \PYG{o}{!}\PYG{n}{odd}\PYG{p}{(}\PYG{n}{numProcesses}\PYG{p}{)} \PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{sendValue} \PYG{o}{=} \PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
		\PYG{k}{if} \PYG{p}{(} \PYG{n}{odd}\PYG{p}{(}\PYG{n}{id}\PYG{p}{)} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// odd processors send, then receive }
			\PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{sendValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
			\PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{receivedValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} 
					\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
		\PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// even processors receive, then send}
			\PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{receivedValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} 
					\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
			\PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{sendValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
		\PYG{p}{\PYGZcb{}}

		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d of \PYGZpc{}d computed \PYGZpc{}f and received \PYGZpc{}f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{sendValue}\PYG{p}{,} \PYG{n}{receivedValue}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{k}{if} \PYG{p}{(} \PYG{o}{!}\PYG{n}{id}\PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// only process 0 does this part}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Please run this program using -np N where N is positive and even.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{3. Data Decomposition: on \emph{slices} using parallel-for}
\label{MessagePassing/MPI_Patternlets:data-decomposition-on-slices-using-parallel-for}
In this example, the data being decomposed in simply the set of integeres from zero to 15, inclusive.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelForSlices.c}
\PYG{c+cm}{ * ... illustrates the parallel for loop pattern in MPI }
\PYG{c+cm}{ *	in which processes perform the loop's iterations in 'slices' }
\PYG{c+cm}{ *	(simple, and useful when loop iterations do not access}
\PYG{c+cm}{ *	 memory/cache locations) ...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{ITERS} \PYG{o}{=} \PYG{l+m+mi}{16}\PYG{p}{;}
	\PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{i} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{n}{id}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{ITERS}\PYG{p}{;} \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{n}{numProcesses}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d is performing iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			\PYG{n}{id}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{4. Data Decomposition: on \emph{blocks} using parallel-for}
\label{MessagePassing/MPI_Patternlets:data-decomposition-on-blocks-using-parallel-for}
This is a basic example that does not yet include a data array, though
it would typically be used when each process would be working on a portion
of an array that could have been looped over in a sequential solution.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelForBlocks.c}
\PYG{c+cm}{ * ... illustrates the parallel for loop pattern in MPI }
\PYG{c+cm}{ *	in which processes perform the loop's iterations in 'blocks' }
\PYG{c+cm}{ *	(preferable when loop iterations do access memory/cache locations) ...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{ITERS} \PYG{o}{=} \PYG{l+m+mi}{16}\PYG{p}{;}
	\PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{i} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,}
		\PYG{n}{start} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{stop} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{blockSize} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{blockSize} \PYG{o}{=} \PYG{n}{ITERS} \PYG{o}{/} \PYG{n}{numProcesses}\PYG{p}{;}     \PYG{c+c1}{// integer division}
	\PYG{n}{start} \PYG{o}{=} \PYG{n}{id} \PYG{o}{*} \PYG{n}{blockSize}\PYG{p}{;}               \PYG{c+c1}{// find starting index}
                                              \PYG{c+c1}{// find stopping index}
	\PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{\PYGZlt{}} \PYG{n}{numProcesses} \PYG{o}{-} \PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{\PYGZob{}}        \PYG{c+c1}{// if not the last process}
		\PYG{n}{stop} \PYG{o}{=} \PYG{p}{(}\PYG{n}{id} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n}{blockSize}\PYG{p}{;}  \PYG{c+c1}{//  stop where next process starts}
	\PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}                              \PYG{c+c1}{// else}
		\PYG{n}{stop} \PYG{o}{=} \PYG{n}{ITERS}\PYG{p}{;}                 \PYG{c+c1}{//  last process does leftovers}
	\PYG{p}{\PYGZcb{}}

	\PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{n}{start}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{stop}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}      \PYG{c+c1}{// iterate through your range}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d is performing iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			\PYG{n}{id}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{5. Broadcast: a special form of message passing}
\label{MessagePassing/MPI_Patternlets:broadcast-a-special-form-of-message-passing}
This example shows how to ensure that all processes have a copy of an array
created by a single \emph{master} node.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* bcast.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Bcast()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{c+c1}{// fill an array with values}
\PYG{k+kt}{void} \PYG{n+nf}{fill}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{size}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
	\PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{size}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{11}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{// display a string, a process id, and its array values}
\PYG{k+kt}{void} \PYG{n+nf}{print}\PYG{p}{(}\PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{str}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{process \PYGZpc{}d \PYGZpc{}s: \PYGZob{}\PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
	   \PYG{n}{id}\PYG{p}{,} \PYG{n}{str}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define MAX 8}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{array}\PYG{p}{[}\PYG{n}{MAX}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numProcs}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{n}{fill}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{)}\PYG{p}{;}
     
	\PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{array before}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{array}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Bcast}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{array after}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{array}\PYG{p}{)}\PYG{p}{;}

 	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{6. Collective Communication: Reduction}
\label{MessagePassing/MPI_Patternlets:collective-communication-reduction}
Once processes have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite commen to then \emph{reduce}
those individual computations into one value.  This example shows a simple
calculation done by each process being reduced to a sum and a maximum.
In this example, MPI, has built-in computations, indicated by MPI\_SUM and
MPI\_MAX in the following code.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* reduce.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Reduce()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{numProcs} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{myRank} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{square} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{max} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sum} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{square} \PYG{o}{=} \PYG{p}{(}\PYG{n}{myRank}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{myRank}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}
     
	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d computed \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{square}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{MPI\PYGZus{}Reduce}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{square}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{sum}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}SUM}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{MPI\PYGZus{}Reduce}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{square}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{max}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}MAX}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{The sum of the squares is \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{sum}\PYG{p}{)}\PYG{p}{;}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{The max of the squares is \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{max}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

 	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{7. Collective communication: Scatter for message-passing data decomposition}
\label{MessagePassing/MPI_Patternlets:collective-communication-scatter-for-message-passing-data-decomposition}
If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the scatter patternlet can be used to ensure that each process receives
a copy of its portion of the array.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* scatter.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Scatter()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{MAX} \PYG{o}{=} \PYG{l+m+mi}{8}\PYG{p}{;}
	\PYG{k+kt}{int} \PYG{n}{aSend}\PYG{p}{[}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{22}\PYG{p}{,} \PYG{l+m+mi}{33}\PYG{p}{,} \PYG{l+m+mi}{44}\PYG{p}{,} \PYG{l+m+mi}{55}\PYG{p}{,} \PYG{l+m+mi}{66}\PYG{p}{,} \PYG{l+m+mi}{77}\PYG{p}{,} \PYG{l+m+mi}{88}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
	\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{aRcv}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{,} \PYG{n}{numProcs}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{numSent}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}
     
        \PYG{n}{numSent} \PYG{o}{=} \PYG{n}{MAX} \PYG{o}{/} \PYG{n}{numProcs}\PYG{p}{;}
	\PYG{n}{aRcv} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{numSent} \PYG{o}{*} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{int}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Scatter}\PYG{p}{(}\PYG{n}{aSend}\PYG{p}{,} \PYG{n}{numSent}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{aRcv}\PYG{p}{,} \PYG{n}{numSent}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d: }\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}
	\PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{numSent}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{ \PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{aRcv}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}
	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{free}\PYG{p}{(}\PYG{n}{aRcv}\PYG{p}{)}\PYG{p}{;}
 	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{8. Collective communication: Gather for message-passing data decomposition}
\label{MessagePassing/MPI_Patternlets:collective-communication-gather-for-message-passing-data-decomposition}
If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the gather patternlet can be used to ensure that each process sends
a copy of its portion of the array back to the root, or master process.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* gather.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Gather()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{MAX} \PYG{o}{=} \PYG{l+m+mi}{3}\PYG{p}{;}
   \PYG{k+kt}{int} \PYG{n}{computeArray}\PYG{p}{[}\PYG{n}{MAX}\PYG{p}{]}\PYG{p}{;}
   \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{gatherArray} \PYG{o}{=} \PYG{n+nb}{NULL}\PYG{p}{;}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{,} \PYG{n}{numProcs}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{totalGatheredVals}\PYG{p}{;}

   \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
   \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
   \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{MAX}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{computeArray}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{myRank} \PYG{o}{*} \PYG{l+m+mi}{10} \PYG{o}{+} \PYG{n}{i}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
     
   \PYG{n}{totalGatheredVals} \PYG{o}{=} \PYG{n}{MAX} \PYG{o}{*} \PYG{n}{numProcs}\PYG{p}{;}
   \PYG{n}{gatherArray} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{totalGatheredVals} \PYG{o}{*} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{int}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}

   \PYG{n}{MPI\PYGZus{}Gather}\PYG{p}{(}\PYG{n}{computeArray}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,}
               \PYG{n}{gatherArray}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

   \PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{totalGatheredVals}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
         \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{ \PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{gatherArray}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
      \PYG{p}{\PYGZcb{}}
      \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}

   \PYG{n}{free}\PYG{p}{(}\PYG{n}{gatherArray}\PYG{p}{)}\PYG{p}{;}
   \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

   \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\section{Shared Memory Parallel Patternlets in OpenMP}
\label{SharedMemory/OpenMP_Patternlets:shared-memory-parallel-patternlets-in-openmp}\label{SharedMemory/OpenMP_Patternlets::doc}
When writing programs for shared-memory hardware with multiple cores,
a programmer could use a
low-level thread package, such as pthreads. An alternative is to use
a compiler that processes OpenMP \emph{pragmas}, which are compiler directives that
enable the compiler to generate threaded code.  Whereas pthreads uses an \textbf{explicit}
multithreading mosel in which the programmer must explicitly create and manage threads,
OpenMP uses an \textbf{implicit} multithreading model in which the library handles
thread creation and management, thus making the programmer's task much simpler and
less error-prone.

The following are examples of C code with OpenMP pragmas
The firrst three are basic illustrations to get used to the OpenMP pragmas.
The rest illustrate how to implement particular patterns and what can
go wrong when mutual exclusion is not properly ensured.

Note: by default OpenMP uses the \textbf{Thread Pool} pattern of concurrent execution.
OpenMP programs initialze a group of threads to be used by a given program
(often called a pool of threads).  These threads will execute concurrently
during portions of the code specified by the programmer.


\subsection{Source Code}
\label{SharedMemory/OpenMP_Patternlets:source-code}
Please download all examples from this tarball:
\code{patternlets.tgz}

A C code file for each example below can be found in subdirectories of the OpenMP directory,
along with a makefile.


\subsection{0. The OMP parallel pragma}
\label{SharedMemory/OpenMP_Patternlets:the-omp-parallel-pragma}
The \emph{omp parallel} pragma on line 18, when uncommented, tells the compiler to
fork a set of threads to execute that particular line of code.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* simpleParallel.c}
\PYG{c+cm}{ * ... illustrates the use of OpenMP's parallel directive...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./simpleParallel }
\PYG{c+cm}{ * Compile \PYGZam{} run, uncomment the pragma, recompile \PYGZam{} run, compare.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Before...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel }
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{During...}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{1. Hello, World: default number of OpenMP threads}
\label{SharedMemory/OpenMP_Patternlets:hello-world-default-number-of-openmp-threads}
Note how there are OpenMP functions to
obtain a thread number and the total number of threads.
When the pragma is uncommented, note what the default number of threads
is.  Here the threads are forked and execute the block of code insode the
curly braces on lines 18 through 20.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* parallelHello.c}
\PYG{c+cm}{ * ... illustrates the use of two basic OpenMP commands...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./parallelHello}
\PYG{c+cm}{ * Compile \PYGZam{} run, uncomment pragma, recompile \PYGZam{} run, compare results}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel }
    \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{rank}  \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Hello from thread \PYGZpc{}d of \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{rank}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{2. Hello, World}
\label{SharedMemory/OpenMP_Patternlets:hello-world}
Here we enter the number of threads to use on the command line.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelHello2.c}
\PYG{c+cm}{ * ... illustrates the use of two basic OpenMP commands}
\PYG{c+cm}{ * 	using the commandline to control the number of threads...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./parallelHello2 [numThreads]}
\PYG{c+cm}{ * Compile \PYGZam{} run with no commandline arg, rerun with different commandline args}
\PYG{c+cm}{ */}



\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{rank}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel }
    \PYG{p}{\PYGZob{}}
        \PYG{n}{rank}  \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Hello from thread \PYGZpc{}d of \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{rank}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{3. Master-Worker Implementation Strategy}
\label{SharedMemory/OpenMP_Patternlets:master-worker-implementation-strategy}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* masterWorker.c}
\PYG{c+cm}{ * ... illustrates the master-worker pattern in OpenMP}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ * Usage: ./masterWorker}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * - Compile and run as is.}
\PYG{c+cm}{ * - Uncomment \PYGZsh{}pragma directive, re-compile and re-run}
\PYG{c+cm}{ * - Compare and trace the different executions.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numThreads} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel}
    \PYG{p}{\PYGZob{}}
        \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

        \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// thread with ID 0 is master}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from the master, \PYGZsh{} \PYGZpc{}d of \PYGZpc{}d threads}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			    \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// threads with IDs \PYGZgt{} 0 are workers}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from a worker, \PYGZsh{} \PYGZpc{}d of \PYGZpc{}d threads}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			    \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{4. Shared Data Decomposition Pattern:  blocking of threads in a parallel for loop}
\label{SharedMemory/OpenMP_Patternlets:shared-data-decomposition-pattern-blocking-of-threads-in-a-parallel-for-loop}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelForBlocks.c}
\PYG{c+cm}{ * ... illustrates the use of OpenMP's default parallel for loop}
\PYG{c+cm}{ *  	in which threads iterate through blocks of the index range}
\PYG{c+cm}{ *	(cache-beneficial when accessing adjacent memory locations).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ * Usage: ./parallelForBlocks [numThreads]}
\PYG{c+cm}{ * Exercise}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{rank}\PYG{p}{,} \PYG{n}{i}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for private(rank, i) }
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{8}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{rank}  \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Thread \PYGZpc{}d performed iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                 \PYG{n}{rank}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{5. Shared Data Decomposition Pattern:  striping of threads in a parallel for loop}
\label{SharedMemory/OpenMP_Patternlets:shared-data-decomposition-pattern-striping-of-threads-in-a-parallel-for-loop}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelForStripes.c}
\PYG{c+cm}{ * ... illustrates how to make OpenMP map threads to }
\PYG{c+cm}{ *	parallel for-loop iterations in 'stripes' instead of blocks}
\PYG{c+cm}{ *	(use only when not accesssing memory).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./parallelForStripes [numThreads]}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel}
    \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{rank}  \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
        \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{n}{rank}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{8}\PYG{p}{;} \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{n}{numThreads}\PYG{p}{)} \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Thread \PYGZpc{}d performed iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                     \PYG{n}{rank}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{6. Collective Communication: Reduction}
\label{SharedMemory/OpenMP_Patternlets:collective-communication-reduction}
Once processes have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite commen to then \emph{reduce}
those individual computations into one value. In this example, an array of randomly assigned integers represents a set of shared data. All values
in the array are summed together by using the OpenMP
parallel for pragma with the \emph{reduction(+:sum)} clause on the variable \textbf{sum},
which is computed in line 61.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* reduction.c}
\PYG{c+cm}{ * ... illustrates the OpenMP parallel-for loop's reduction clause}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./reduction }
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run.  Note that correct output is produced.}
\PYG{c+cm}{ * - Uncomment \PYGZsh{}pragma in function parallelSum() }
\PYG{c+cm}{ *    but leave its reduction clause commented out}
\PYG{c+cm}{ * - Recompile and rerun.  Note that correct output is NOT produced.}
\PYG{c+cm}{ * - Uncomment 'reduction(+:sum)' clause of \PYGZsh{}pragma in parallelSum()}
\PYG{c+cm}{ * - Recompile and rerun.  Note that correct output is produced again.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}assert.h\PYGZgt{}}

\PYG{k+kt}{void} \PYG{n}{initialize}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{int} \PYG{n}{sequentialSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{int} \PYG{n}{parallelSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)}\PYG{p}{;}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define SIZE 1000000}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{array}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{;}

   \PYG{n}{initialize}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{)}\PYG{p}{;}
   \PYG{n}{assert}\PYG{p}{(} \PYG{n}{sequentialSum}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{n}{parallelSum}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
   \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Sequential and parallel functions produced the same result}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

   \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}} 

\PYG{c+c1}{// fill array with random values}
\PYG{k+kt}{void} \PYG{n+nf}{initialize}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{1000}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{// sum the array sequentially}
\PYG{k+kt}{int} \PYG{n+nf}{sequentialSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{sum} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{sum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
   \PYG{k}{return} \PYG{n}{sum}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{// sum the array using multiple threads}
\PYG{k+kt}{int} \PYG{n+nf}{parallelSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{sum} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
\PYG{c+c1}{//   \PYGZsh{}pragma omp parallel for // reduction(+:sum)}
   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{sum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
   \PYG{k}{return} \PYG{n}{sum}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{7. Shared Data Pattern: Parallel-for-loop needs non-shared, private variables}
\label{SharedMemory/OpenMP_Patternlets:shared-data-pattern-parallel-for-loop-needs-non-shared-private-variables}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* private.c}
\PYG{c+cm}{ * ... illustrates why private variables are needed with OpenMP's parallel for loop}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ * Usage: ./private }
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * - Run, noting that the serial program produces correct results}
\PYG{c+cm}{ * - Uncomment line A, recompile/run and compare}
\PYG{c+cm}{ * - Recomment line A, uncomment line B, recompile/run and compare}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define SIZE 100}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{,} \PYG{n}{ok} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{m}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{;}

    \PYG{c+c1}{// set all array entries to 1}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for                     // A}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for private(j)          // B}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k}{for} \PYG{p}{(}\PYG{n}{j} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{j} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{j}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
            \PYG{n}{m}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+c1}{// test (without using threads)}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k}{for} \PYG{p}{(}\PYG{n}{j} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{j} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{j}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
            \PYG{k}{if} \PYG{p}{(} \PYG{n}{m}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{!}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{\PYGZob{}}
                \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Element [\PYGZpc{}d,\PYGZpc{}d] not set... }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{)}\PYG{p}{;}
                \PYG{n}{ok} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
            \PYG{p}{\PYGZcb{}}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{if} \PYG{p}{(} \PYG{n}{ok} \PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{All elements correctly set to 1}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{8. Race Condition: missing the mutual exclusion patterm}
\label{SharedMemory/OpenMP_Patternlets:race-condition-missing-the-mutual-exclusion-patterm}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* atomic.c}
\PYG{c+cm}{ * ... illustrates a race condition when multiple threads write to a shared variable}
\PYG{c+cm}{ *  (and explores OpenMP private variables and atomic operations).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ * Usage: ./atomic}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile and run 10 times; note that it always produces the final balance 0}
\PYG{c+cm}{ *  - To parallelize, uncomment A1+A2, recompile and rerun, compare results}
\PYG{c+cm}{ *  - Try 1: recomment A1+A2, uncomment B1+B2, recompile/run, compare}
\PYG{c+cm}{ *  - To fix: recomment B1+B2, uncomment A1+A2, C1+C2, recompile and rerun, compare}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{1000000}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
  
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Your starting bank account balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate many deposits}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for                      // A1}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for private(balance)     // B1}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
\PYG{c+c1}{//        \PYGZsh{}pragma omp atomic                        // C1}
        \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{10.0}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}10 deposits, your balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
		\PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate the same number of withdrawals}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for                      // A2}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for private(balance)     // B2}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
\PYG{c+c1}{//        \PYGZsh{}pragma omp atomic                          // C2}
        \PYG{n}{balance} \PYG{o}{-}\PYG{o}{=} \PYG{l+m+mf}{10.0}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+c1}{// balance should be zero}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}10 withdrawals, your balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
            \PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{9. Mutual Exclusion: two ways to ensure}
\label{SharedMemory/OpenMP_Patternlets:mutual-exclusion-two-ways-to-ensure}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* critical.c}
\PYG{c+cm}{ * ... fixes a race condition when multiple threads write to a shared variable}
\PYG{c+cm}{ *  	using the OpenMP critical directive.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ * Usage: ./critical}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile and run several times; note that it always produces the final balance 0}
\PYG{c+cm}{ *  - Comment out A1+A2; recompile/run and note incorrect result}
\PYG{c+cm}{ *  - To fix: uncomment B1a+B1b+B1c, B2a+B2b+B2c, recompile and rerun, compare}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{1000000}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
  
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Your starting bank account balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate many deposits}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp atomic                          }\PYG{c+c1}{// A1}
\PYG{c+c1}{//        \PYGZsh{}pragma omp critical                      // B1a}
\PYG{c+c1}{//        \PYGZob{}                                         // B1b}
        \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{10.0}\PYG{p}{;}
\PYG{c+c1}{//        \PYGZcb{}                                         // B1c}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}10 deposits, your balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
		\PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate the same number of withdrawals}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp atomic                          }\PYG{c+c1}{// A2}
\PYG{c+c1}{//        \PYGZsh{}pragma omp critical                      // B2a}
\PYG{c+c1}{//        \PYGZob{}                                         // B2b}
        \PYG{n}{balance} \PYG{o}{-}\PYG{o}{=} \PYG{l+m+mf}{10.0}\PYG{p}{;}
\PYG{c+c1}{//        \PYGZcb{}                                         // B2c}
  \PYG{p}{\PYGZcb{}}

    \PYG{c+c1}{// balance should be zero}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}10 withdrawals, your balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
		\PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{10.  Mutual Exclusion Pattern: compare performance}
\label{SharedMemory/OpenMP_Patternlets:mutual-exclusion-pattern-compare-performance}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* critical.c}
\PYG{c+cm}{ * ... compares the performance of OpenMP's critical and atomic directives}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ * Usage: ./critical}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile, run, compare times for critical vs. atomic}
\PYG{c+cm}{ *  - Compute how much more costly critical is than atomic, on average}
\PYG{c+cm}{ *  - Create an expression setting a shared variable that atomic cannot handle (research)}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{void} \PYG{n+nf}{print}\PYG{p}{(}\PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{label}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{reps}\PYG{p}{,} \PYG{k+kt}{double} \PYG{n}{balance}\PYG{p}{,} \PYG{k+kt}{double} \PYG{n}{total}\PYG{p}{,} \PYG{k+kt}{double} \PYG{n}{average}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}10 deposits using '\PYGZpc{}s': }\PYG{l+s}{\PYGZbs{}}
\PYG{l+s}{            }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{- balance = \PYGZpc{}0.2f, }\PYG{l+s}{\PYGZbs{}}
\PYG{l+s}{            }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{- total time = \PYGZpc{}0.12f, }\PYG{l+s}{\PYGZbs{}}
\PYG{l+s}{            }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{- average time per deposit = \PYGZpc{}0.12f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
               \PYG{n}{reps}\PYG{p}{,} \PYG{n}{label}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{,} \PYG{n}{total}\PYG{p}{,} \PYG{n}{average}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{1000000}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}
           \PYG{n}{startTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,} 
           \PYG{n}{stopTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}
           \PYG{n}{totalTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
  
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Your starting bank account balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate many deposits using atomic}
    \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for }
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp atomic}
        \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{10.0}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{stopTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{totalTime} \PYG{o}{=} \PYG{n}{stopTime} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}
    \PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{atomic}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{,} \PYG{n}{totalTime}\PYG{p}{,} \PYG{n}{totalTime}\PYG{o}{/}\PYG{n}{REPS}\PYG{p}{)}\PYG{p}{;}


    \PYG{c+c1}{// simulate the same number of deposits using critical}
    \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
    \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for }
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
         \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp critical}
         \PYG{p}{\PYGZob{}}
             \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{10.0}\PYG{p}{;}
         \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{stopTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{totalTime} \PYG{o}{=} \PYG{n}{stopTime} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}
    \PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{critical}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{,} \PYG{n}{totalTime}\PYG{p}{,} \PYG{n}{totalTime}\PYG{o}{/}\PYG{n}{REPS}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{11. Task Decomposition Pattern using OpenMP section directive}
\label{SharedMemory/OpenMP_Patternlets:task-decomposition-pattern-using-openmp-section-directive}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* sections.c}
\PYG{c+cm}{ * ... illustrates the use of OpenMP's parallel section/sections directives...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Before...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel sections num\PYGZus{}threads(4)}
    \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section }
        \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Section A performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                    \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section }
        \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Section B performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                    \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section}
        \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Section C performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                    \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section }
        \PYG{p}{\PYGZob{}}
                \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Section D performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                         \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}



\renewcommand{\indexname}{Index}
\printindex
\end{document}
