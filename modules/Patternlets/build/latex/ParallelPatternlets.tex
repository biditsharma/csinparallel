% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,openany,oneside]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}


\title{Parallel Patternlets}
\date{June 10, 2016}
\release{}
\author{CSinParallel Project}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.25,0.82}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.19,0.19,0.19}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


\textbf{Last Updated:} 2016-06-10

This document contains simple examples of basic elements that are combined to form patterns often used in programs employing parallelism.  We call these examples \emph{patternlets} because they are deliberately trivial, small, yet functioning programs that illustrate a basic shell of how a particular parallel pattern is created in a program.  They are starting points you can use to create realistic working programs of your own that use the patterns.  Before diving into the examples, first there will be some background on parallel programming patterns.


\chapter{Parallel Programming Patterns}
\label{PatternsIntro:parallel-programming-patterns}\label{PatternsIntro::doc}\label{PatternsIntro:parallel-patternlets}
Like all programs, parallel programs contain many \textbf{patterns}: useful ways of writing code that are used repeatedly by most developers because they work well in practice.  These patterns have been documented by developers over time so that useful ways of organizing and writing good parallel code can be learned by new programmers (and even seasoned veterans).


\section{An organization of parallel patterns}
\label{PatternsIntro:an-organization-of-parallel-patterns}
When writing parallel programs, developers use patterns that can be grouped into two main categories:
\begin{enumerate}
\item {} 
Strategies

\item {} 
Concurrent Execution Mechanisms

\end{enumerate}


\subsection{Strategies}
\label{PatternsIntro:strategies}
When you set out to write a program, whether it is parallel or not, you should be considering two primary strategic considerations:
\begin{enumerate}
\item {} 
What \emph{algorithmic strategies} to use

\item {} 
Given the algorithmic strategies, what \emph{implementation strategies} to use

\end{enumerate}

In the examples in this document we introduce some well-used patterns for both algorithmic strategies and implementation strategies.  Parallel algorithmic strategies primarily have to do with making choices about what tasks can be done concurrently by multiple processing units executing concurrently.  Parallel programs often make use of several patterns of implementation strategies.  Some of these patterns contribute to the overall structure of the program, and others are concerned with how the data that is being computed by multiple processing units is structured.  As you will see, the patternlets introduce more algorithmic strategy patterns and program structure implementation strategy patterns than data structure implementation strategy patterns.


\subsection{Concurrent Execution Mechanisms}
\label{PatternsIntro:concurrent-execution-mechanisms}
There are a number of parallel code patterns that are closely related to the system or hardware that a program is being written for and the software library used to enable parallelism, or concurrent execution.  These \emph{concurrent execution} patterns fall into two major categories:
\begin{enumerate}
\item {} 
\emph{Process/Thread control} patterns, which dictate how the processing units of parallel execution on the hardware (either a process or a thread, depending on the hardware and software used) are controlled at run time.  For the patternlets described in this document, the software libraries that provide system parallelism have these patterns built into them, so they will be hidden from the programmer.

\item {} 
\emph{Coordination} patterns, which set up how multiple concurrently running tasks on processing units coordinate to complete the parallel computation desired.

\end{enumerate}

In parallel processing, most software uses one of
two major \emph{coordination patterns}:
\begin{enumerate}
\item {} 
\textbf{message passing} between concurrent processes on either single multiprocessor machines or clusters of distributed computers, and

\item {} 
\textbf{mutual exclusion} between threads executing concurrently on a single shared memory system.

\end{enumerate}

These two types of computation are often realized using two very popular C/C++ libraries:
\begin{enumerate}
\item {} 
MPI, or Message Passing Interface, for message passing.

\item {} 
OpenMP for threaded, shared memory applications.

\end{enumerate}

OpenMP is built on a lower-level POSIX library called Pthreads, which can also be used by itself on shared memory systems.

A third emerging type of parallel implementation involves a \emph{hybrid computation} that uses both of the above patterns together, using a cluster of computers, each of which executes multiple threads.  This type of hybrid program often uses MPI and OpenMP together in one program, which runs on multiple computers in a cluster.

This document is split into chapters of examples.  There are examples for message passing using MPI and shared memory using OpenMP.
(In the future we will include shared memory examples using Pthreads, and hybrid computations using a combination of MPI and OpenMP.)

Most of the examples are illustrated
with the C programming language, using standard popular available libraries. In a few cases, C++
is used to illustrate a particular difference in code execution between the two languages or to make use of a C++ BigInt class.

There are many small examples that serve to illustrate a common pattern.  They are designed for you to try compiling and running on your own to see how they work.  For each example, there are comments within the code to guide you as you try them out.  In many cases, there may be code snippets that you can comment and/or uncomment to see how the execution of the code changes after you do so and re-compile it.

Depending on you interest, you can now explore MPI Patternlets or OpenMP Patternlets.

{\hyperref[MessagePassing/MPI_Patternlets::doc]{\emph{Message Passing Parallel Patternlets}}}

{\hyperref[SharedMemory/OpenMP_Patternlets::doc]{\emph{Shared Memory Parallel Patternlets in OpenMP}}}


\chapter{Message Passing Parallel Patternlets}
\label{MessagePassing/MPI_Patternlets:message-passing-parallel-patternlets}\label{MessagePassing/MPI_Patternlets::doc}
Parallel programs contain \emph{patterns}:  code that recurs over and over again
in solutions to many problems.  The following examples show very simple
examples of small portions of
these patterns that can be combined to solve a problem.  These C code examples use the
Message Passing Interface (MPI) library, which is suitable for use on either a
single multiprocessor machine or a cluster
of machines.


\section{Source Code}
\label{MessagePassing/MPI_Patternlets:source-code}
Please download all examples from this tarball:
\code{MPI.tgz}

A C code file for each example below can be found in subdirectories of the MPI directory,
along with a makefile and an example of how to execute the program.


\section{00. Single Program, Multiple Data}
\label{MessagePassing/MPI_Patternlets:single-program-multiple-data}
First let us illustrate the basic components of an MPI program,
which by its nature uses a single program that runs on each process.
Note what gets printed is different for each process, thus the
processes using this one single program can have different data values
for its variables.  This is why we call it single program, multiple data.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* spmd.c}
\PYG{c+cm}{ * ... illustrates the single program multiple data}
\PYG{c+cm}{ *      (SPMD) pattern using basic MPI commands.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np 4 ./spmd}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run.}
\PYG{c+cm}{ * - Compare source code to output.}
\PYG{c+cm}{ * - Rerun, using varying numbers of processes}
\PYG{c+cm}{ *    (i.e., vary the argument to 'mpirun -np').}
\PYG{c+cm}{ * - Explain what "multiple data" values this}
\PYG{c+cm}{ *    "single program" is generating.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}     }\PYG{c+c1}{// MPI functions}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{char} \PYG{n}{myHostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{myHostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from process \PYGZsh{}\PYGZpc{}d of \PYGZpc{}d on \PYGZpc{}s}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
             \PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{myHostName}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/00.spmd/spmd.c}


\section{01. The Master-Worker Implementation Strategy Pattern}
\label{MessagePassing/MPI_Patternlets:the-master-worker-implementation-strategy-pattern}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* masterWorker.c}
\PYG{c+cm}{ * ... illustrates the basic master-worker pattern in MPI ...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./masterWorker}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run the program, varying N from 1 through 8.}
\PYG{c+cm}{ * - Explain what stays the same and what changes as the}
\PYG{c+cm}{ *    number of processes changes.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
  \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numWorkers} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
  \PYG{k+kt}{char} \PYG{n}{hostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]}\PYG{p}{;}

  \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numWorkers}\PYG{p}{)}\PYG{p}{;}
  \PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{hostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}

  \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// process 0 is the master }
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from the master, \PYGZsh{}\PYGZpc{}d (\PYGZpc{}s) of \PYGZpc{}d processes}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
             \PYG{n}{id}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{,} \PYG{n}{numWorkers}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// processes with ids \PYGZgt{} 0 are workers }
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from a worker, \PYGZsh{}\PYGZpc{}d (\PYGZpc{}s) of \PYGZpc{}d processes}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
             \PYG{n}{id}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{,} \PYG{n}{numWorkers}\PYG{p}{)}\PYG{p}{;}
  \PYG{p}{\PYGZcb{}}

  \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
  \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/01.masterWorker/masterWorker.c}


\section{02. Message passing 1, using Send-Receive of a single value}
\label{MessagePassing/MPI_Patternlets:message-passing-1-using-send-receive-of-a-single-value}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* messagePassing.c}
\PYG{c+cm}{ * ... illustrates the use of the MPI\PYGZus{}Send() and MPI\PYGZus{}Recv() commands...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./messagePassing}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, using N = 4, 6, 8, and 10 processes.}
\PYG{c+cm}{ * - Use source code to trace execution.}
\PYG{c+cm}{ * - Explain what each process:}
\PYG{c+cm}{ * -- computes}
\PYG{c+cm}{ * -- sends}
\PYG{c+cm}{ * -- receives}
\PYG{c+cm}{ * -- outputs.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}math.h\PYGZgt{}   }\PYG{c+c1}{// sqrt()}

\PYG{k+kt}{int} \PYG{n+nf}{odd}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{number}\PYG{p}{)} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{number} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;} 
    \PYG{k+kt}{float} \PYG{n}{sendValue} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{receivedValue} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Status} \PYG{n}{status}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{numProcesses} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1} \PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}} \PYG{o}{!}\PYG{n}{odd}\PYG{p}{(}\PYG{n}{numProcesses}\PYG{p}{)} \PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{sendValue} \PYG{o}{=} \PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
        \PYG{k}{if} \PYG{p}{(} \PYG{n}{odd}\PYG{p}{(}\PYG{n}{id}\PYG{p}{)} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// odd processors send, then receive }
            \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{sendValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
            \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{receivedValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} 
                       \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// even processors receive, then send }
            \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{receivedValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} 
                       \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
            \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{sendValue}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}FLOAT}\PYG{p}{,} \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}

        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d of \PYGZpc{}d computed \PYGZpc{}f and received \PYGZpc{}f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                \PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{sendValue}\PYG{p}{,} \PYG{n}{receivedValue}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{k}{if} \PYG{p}{(} \PYG{o}{!}\PYG{n}{id}\PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// only process 0 does this part }
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Please run this program using -np N where N is positive and even.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/02.messagePassing/messagePassing.c}


\section{03. Message passing 2,  using Send-Receive of an array of values}
\label{MessagePassing/MPI_Patternlets:message-passing-2-using-send-receive-of-an-array-of-values}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* messagePassing2.c}
\PYG{c+cm}{ * ... illustrates using MPI\PYGZus{}Send() and MPI\PYGZus{}Recv() commands on arrays...}
\PYG{c+cm}{ * While this example sends and receives char arrays (strings),}
\PYG{c+cm}{ *  the same approach works on arrays of numbers or other types.}
\PYG{c+cm}{ * Joel Adams, Calvin College, September 2013.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./messagePassing2}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, varying N: 1, 2, 4, 8.}
\PYG{c+cm}{ * - Trace execution using source code.}
\PYG{c+cm}{ * - Compare to messagePassing1.c; note send/receive differences.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}     }\PYG{c+c1}{// MPI}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}  }\PYG{c+c1}{// malloc()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}string.h\PYGZgt{}  }\PYG{c+c1}{// strlen()}

\PYG{k+kt}{int} \PYG{n+nf}{odd}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{number}\PYG{p}{)} \PYG{p}{\PYGZob{}} \PYG{k}{return} \PYG{n}{number} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;} 
    \PYG{k+kt}{char} \PYG{o}{*} \PYG{n}{sendString} \PYG{o}{=} \PYG{n+nb}{NULL}\PYG{p}{;}
    \PYG{k+kt}{char} \PYG{o}{*} \PYG{n}{receivedString} \PYG{o}{=} \PYG{n+nb}{NULL}\PYG{p}{;}
    \PYG{k+kt}{char} \PYG{n}{hostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Status} \PYG{n}{status}\PYG{p}{;}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{SIZE} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{32}\PYG{o}{+}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{)} \PYG{o}{*} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{char}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{hostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{numProcesses} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1} \PYG{o}{\PYGZam{}}\PYG{o}{\PYGZam{}} \PYG{o}{!}\PYG{n}{odd}\PYG{p}{(}\PYG{n}{numProcesses}\PYG{p}{)} \PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{sendString} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{char}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{SIZE} \PYG{p}{)}\PYG{p}{;}
        \PYG{n}{receivedString} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{char}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{SIZE} \PYG{p}{)}\PYG{p}{;}
        \PYG{n}{sprintf}\PYG{p}{(}\PYG{n}{sendString}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d is on host }\PYG{l+s+se}{\PYGZbs{}"}\PYG{l+s}{\PYGZpc{}s}\PYG{l+s+se}{\PYGZbs{}"}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{)}\PYG{p}{;}

        \PYG{k}{if} \PYG{p}{(} \PYG{n}{odd}\PYG{p}{(}\PYG{n}{id}\PYG{p}{)} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// odd processes send, then receive }
            \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{n}{sendString}\PYG{p}{,} \PYG{n}{strlen}\PYG{p}{(}\PYG{n}{sendString}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} 
                       \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{n}{id}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
            \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{n}{receivedString}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{n}{id}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} 
                       \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// even processes receive, then send }
            \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{n}{receivedString}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} 
                       \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
            \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{n}{sendString}\PYG{p}{,} \PYG{n}{strlen}\PYG{p}{(}\PYG{n}{sendString}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} 
                       \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}

        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Process \PYGZpc{}d of \PYGZpc{}d received the message:}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{'\PYGZpc{}s'}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                \PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{receivedString}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{free}\PYG{p}{(}\PYG{n}{sendString}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{free}\PYG{p}{(}\PYG{n}{receivedString}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{k}{if} \PYG{p}{(} \PYG{o}{!}\PYG{n}{id}\PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// only process 0 does this part }
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Please run this program using -np N where N is positive and even.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/03.messagePassing2/messagePassing2.c}


\section{04. Message passing 3,  using Send-Receive with master-worker pattern}
\label{MessagePassing/MPI_Patternlets:message-passing-3-using-send-receive-with-master-worker-pattern}
\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* messagePassing3.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Send() and MPI\PYGZus{}Recv(),}
\PYG{c+cm}{ *      in combination with the master-worker pattern. }
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./messagePassing3}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * - Run the program, varying the value of N from 1-8.}
\PYG{c+cm}{ * - Explain the behavior you observe.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}    }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}string.h\PYGZgt{}   }\PYG{c+c1}{// strlen()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}      }\PYG{c+c1}{// MPI}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define MAX 256}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;} 
    \PYG{k+kt}{char} \PYG{n}{sendBuffer}\PYG{p}{[}\PYG{n}{MAX}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+sc}{'\PYGZbs{}0'}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
    \PYG{k+kt}{char} \PYG{n}{recvBuffer}\PYG{p}{[}\PYG{n}{MAX}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+sc}{'\PYGZbs{}0'}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Status} \PYG{n}{status}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{numProcesses} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{p}{)} \PYG{p}{\PYGZob{}}                              \PYG{c+c1}{// master:}
            \PYG{n}{sprintf}\PYG{p}{(}\PYG{n}{sendBuffer}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{\PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{)}\PYG{p}{;}            \PYG{c+c1}{//  create msg}

            \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{n}{sendBuffer}\PYG{p}{,}                      \PYG{c+c1}{//  msg sent}
                      \PYG{n}{strlen}\PYG{p}{(}\PYG{n}{sendBuffer}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,}         \PYG{c+c1}{//  num chars + NULL}
                      \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,}                       \PYG{c+c1}{//  type}
                      \PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,}                           \PYG{c+c1}{//  destination}
                      \PYG{l+m+mi}{1}\PYG{p}{,}                              \PYG{c+c1}{//  tag}
                      \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}                \PYG{c+c1}{//  communicator}

            \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{n}{recvBuffer}\PYG{p}{,}                      \PYG{c+c1}{//  msg received}
                      \PYG{n}{MAX}\PYG{p}{,}                            \PYG{c+c1}{//  buffer size}
                      \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,}                       \PYG{c+c1}{//  type}
                      \PYG{n}{numProcesses}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,}                 \PYG{c+c1}{//  sender}
                      \PYG{l+m+mi}{1}\PYG{p}{,}                              \PYG{c+c1}{//  tag}
                      \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,}                 \PYG{c+c1}{//  communicator}
                      \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}                       \PYG{c+c1}{//  recv status}
        \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}                                      \PYG{c+c1}{// workers:}
            \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{n}{recvBuffer}\PYG{p}{,}                      \PYG{c+c1}{//  msg received}
                      \PYG{n}{MAX}\PYG{p}{,}                            \PYG{c+c1}{//  buffer size}
                      \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,}                       \PYG{c+c1}{//  type}
                      \PYG{n}{MPI\PYGZus{}ANY\PYGZus{}SOURCE}\PYG{p}{,}                 \PYG{c+c1}{//  sender (anyone)}
                      \PYG{l+m+mi}{1}\PYG{p}{,}                              \PYG{c+c1}{//  tag}
                      \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,}                 \PYG{c+c1}{//  communicator}
                      \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}                       \PYG{c+c1}{//  recv status}

            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZsh{}\PYGZpc{}d of \PYGZpc{}d received \PYGZpc{}s}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{c+c1}{// show msg}
                    \PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{recvBuffer}\PYG{p}{)}\PYG{p}{;}

            \PYG{c+c1}{// build msg to send by appending id to msg received}
            \PYG{n}{sprintf}\PYG{p}{(}\PYG{n}{sendBuffer}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{\PYGZpc{}s \PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{recvBuffer}\PYG{p}{,} \PYG{n}{id}\PYG{p}{)}\PYG{p}{;}

            \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{n}{sendBuffer}\PYG{p}{,}                      \PYG{c+c1}{//  msg to send}
                      \PYG{n}{strlen}\PYG{p}{(}\PYG{n}{sendBuffer}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,}         \PYG{c+c1}{//  num chars + NULL}
                      \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,}                       \PYG{c+c1}{//  type}
                      \PYG{p}{(}\PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{n}{numProcesses}\PYG{p}{,}          \PYG{c+c1}{//  destination}
                      \PYG{l+m+mi}{1}\PYG{p}{,}                              \PYG{c+c1}{//  tag}
                      \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}                \PYG{c+c1}{//  communicator}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Please run this program with at least 2 processes}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/04.messagePassing3/messagePassing3.c}


\section{05. Data Decomposition: on \emph{equal-sized chunks} using parallel-for}
\label{MessagePassing/MPI_Patternlets:data-decomposition-on-equal-sized-chunks-using-parallel-for}
In this example, the data being decomposed is simply the set of integers
from zero to REPS * numProcesses, which are used in the for loop.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelLoopEqualChunks.c}
\PYG{c+cm}{ * ... illustrates the parallel for loop pattern in MPI }
\PYG{c+cm}{ *	in which processes perform the loop's iterations in equal-sized 'chunks' }
\PYG{c+cm}{ *	(preferable when loop iterations access memory/cache locations) ...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./parallelForEqualChunks}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8}
\PYG{c+cm}{ * - Change REPS to 16, save, recompile, rerun, varying N again.}
\PYG{c+cm}{ * - Explain how this pattern divides the iterations of the loop}
\PYG{c+cm}{ *    among the processes.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{} }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}   }\PYG{c+c1}{// MPI}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}math.h\PYGZgt{}  }\PYG{c+c1}{// ceil()}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{8}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{i} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{n}{start} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{stop} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{chunkSize} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{chunkSize} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{int}\PYG{p}{)}\PYG{n}{ceil}\PYG{p}{(}\PYG{p}{(}\PYG{p}{(}\PYG{k+kt}{double}\PYG{p}{)}\PYG{n}{REPS}\PYG{p}{)} \PYG{o}{/} \PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{// find chunk size}
    \PYG{n}{start} \PYG{o}{=} \PYG{n}{id} \PYG{o}{*} \PYG{n}{chunkSize}\PYG{p}{;}                               \PYG{c+c1}{// find starting index}
                                                          \PYG{c+c1}{// find stopping index:}
    \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{\PYGZlt{}} \PYG{n}{numProcesses} \PYG{o}{-} \PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{\PYGZob{}}                        \PYG{c+c1}{//  if not the last process}
        \PYG{n}{stop} \PYG{o}{=} \PYG{p}{(}\PYG{n}{id} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n}{chunkSize}\PYG{p}{;}                      \PYG{c+c1}{//   stop where next process starts}
    \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}                                              \PYG{c+c1}{//  else }
        \PYG{n}{stop} \PYG{o}{=} \PYG{n}{REPS}\PYG{p}{;}                                      \PYG{c+c1}{//   last process does leftovers }
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{n}{start}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{stop}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}                      \PYG{c+c1}{// iterate through our range }
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d is performing iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/05.parallelLoop-equalChunks/parallelLoopEqualChunks.c}


\section{06. Data Decomposition: on \emph{chunks of size 1} using parallel-for}
\label{MessagePassing/MPI_Patternlets:data-decomposition-on-chunks-of-size-1-using-parallel-for}
This is a basic example that does not yet include a data array, though
it would typically be used when each process would be working on a portion
of an array that could have been looped over in a sequential solution.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* parallelLoopChunksOf1.c}
\PYG{c+cm}{ * ... illustrates the parallel for loop pattern in MPI }
\PYG{c+cm}{ *	in which processes perform the loop's iterations in 'chunks' }
\PYG{c+cm}{ *      of size 1 (simple, and useful when loop iterations }
\PYG{c+cm}{ *      do not access memory/cache locations) ...}
\PYG{c+cm}{ * Note this is much simpler than the 'equal chunks' loop.}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./parallelForSlices}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8}
\PYG{c+cm}{ * - Change REPS to 16, save, recompile, rerun, varying N again.}
\PYG{c+cm}{ * - Explain how this pattern divides the iterations of the loop}
\PYG{c+cm}{ *    among the processes.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}  }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}    }\PYG{c+c1}{// MPI}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{8}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{i} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{n}{id}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{n}{numProcesses}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d is performing iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/06.parallelLoop-chunksOf1/parallelLoopChunksOf1.c}


\section{07. Broadcast: a special form of message passing}
\label{MessagePassing/MPI_Patternlets:broadcast-a-special-form-of-message-passing}
This example shows how a data item read from a file can be sent to all the processes.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* broadcast.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Bcast() with a scalar value...}
\PYG{c+cm}{ *      (compare to array version).}
\PYG{c+cm}{ * Joel Adams, Calvin College, April 2016.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./broadcast}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run several times,}
\PYG{c+cm}{ *     using 2, 4, and 8 processes}
\PYG{c+cm}{ * - Use source code to trace execution and output}
\PYG{c+cm}{ *     (noting contents of file "data.txt");}
\PYG{c+cm}{ * - Explain behavior/effect of MPI\PYGZus{}Bcast().}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}assert.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{answer} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numProcs} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{myRank} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}
               \PYG{k+kt}{FILE} \PYG{o}{*}\PYG{n}{filePtr} \PYG{o}{=} \PYG{n}{fopen}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{data.txt}\PYG{l+s}{"}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{r}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;} 
               \PYG{n}{assert}\PYG{p}{(} \PYG{n}{filePtr} \PYG{o}{!}\PYG{o}{=} \PYG{n+nb}{NULL} \PYG{p}{)}\PYG{p}{;}
               \PYG{n}{fscanf}\PYG{p}{(}\PYG{n}{filePtr}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{ \PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{answer}\PYG{p}{)}\PYG{p}{;}
               \PYG{n}{fclose}\PYG{p}{(}\PYG{n}{filePtr}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}

	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{BEFORE the broadcast, process \PYGZpc{}d's answer = \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                 \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{answer}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{MPI\PYGZus{}Bcast}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{answer}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{AFTER the broadcast, process \PYGZpc{}d's answer = \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                 \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{answer}\PYG{p}{)}\PYG{p}{;}

 	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/07.broadcast/broadcast.c}


\section{08. Broadcast: send data to all processes}
\label{MessagePassing/MPI_Patternlets:broadcast-send-data-to-all-processes}
This example shows how to ensure that all processes have a copy of an array
created by a single \emph{master} node.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* broadcast2.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Bcast() for arrays...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./broadcast}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, using 2, 4, and 8 processes}
\PYG{c+cm}{ * - Use source code to trace execution and output}
\PYG{c+cm}{ * - Explain behavior/effect of MPI\PYGZus{}Bcast().}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{c+cm}{/* fill an array with some arbitrary values }
\PYG{c+cm}{ * @param: a, an int*.}
\PYG{c+cm}{ * @param: size, an int.}
\PYG{c+cm}{ * Precondition: a is the address of an array of ints.}
\PYG{c+cm}{ *              \PYGZam{}\PYGZam{} size is the number of ints a can hold.}
\PYG{c+cm}{ * Postcondition: a has been filled with arbitrary values }
\PYG{c+cm}{ *                \PYGZob{} 11, 12, 13, ... \PYGZcb{}.}
\PYG{c+cm}{ */}
\PYG{k+kt}{void} \PYG{n+nf}{fill}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{size}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
	\PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{size}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{11}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+cm}{/* display a string, a process id, and its array values }
\PYG{c+cm}{ * @param: str, a char*}
\PYG{c+cm}{ * @param: id, an int}
\PYG{c+cm}{ * @param: a, an int*.}
\PYG{c+cm}{ * Precondition: str points to either "BEFORE" or "AFTER"}
\PYG{c+cm}{ *              \PYGZam{}\PYGZam{} id is the rank of this MPI process}
\PYG{c+cm}{ *              \PYGZam{}\PYGZam{} a is the address of an 8-element int array.}
\PYG{c+cm}{ * Postcondition: str, id, and a have all been written to stdout.}
\PYG{c+cm}{ */}
\PYG{k+kt}{void} \PYG{n+nf}{print}\PYG{p}{(}\PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{str}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{\PYGZpc{}s broadcast, process \PYGZpc{}d has: \PYGZob{}\PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d, \PYGZpc{}d\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
	   \PYG{n}{str}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,} \PYG{n}{a}\PYG{p}{[}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define MAX 8}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{k+kt}{int} \PYG{n}{array}\PYG{p}{[}\PYG{n}{MAX}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numProcs}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{n}{fill}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{)}\PYG{p}{;}
     
	\PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{BEFORE}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{array}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{MPI\PYGZus{}Bcast}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{AFTER}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{array}\PYG{p}{)}\PYG{p}{;}

 	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/08.broadcast2/broadcast2.c}


\section{09. Collective Communication: Reduction}
\label{MessagePassing/MPI_Patternlets:collective-communication-reduction}
Once processes have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite common to then \emph{reduce}
those individual computations into one value.  This example shows a simple
calculation done by each process being reduced to a sum and a maximum.
In this example, MPI, has built-in computations, indicated by MPI\_SUM and
MPI\_MAX in the following code.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* reduction.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Reduce()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./reduction}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * - Compile and run, varying N: 4, 6, 8, 10.}
\PYG{c+cm}{ * - Explain behavior of MPI\PYGZus{}Reduce().}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{numProcs} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{myRank} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{square} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{max} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sum} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}

	\PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

	\PYG{n}{square} \PYG{o}{=} \PYG{p}{(}\PYG{n}{myRank}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{myRank}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}
     
	\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d computed \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{myRank}\PYG{p}{,} \PYG{n}{square}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{MPI\PYGZus{}Reduce}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{square}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{sum}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}SUM}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

        \PYG{n}{MPI\PYGZus{}Reduce}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{square}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{max}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}MAX}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{The sum of the squares is \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{sum}\PYG{p}{)}\PYG{p}{;}
		\PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{The max of the squares is \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{max}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

 	\PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

	\PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/09.reduction/reduction.c}


\section{10. Collective Communication: Reduction}
\label{MessagePassing/MPI_Patternlets:id1}
Here is a second reduction example using arrays of data.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* reduction2.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Reduce() using arrays...}
\PYG{c+cm}{ * Joel Adams, Calvin College, January 2015.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np 4 ./reduction2}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, comparing output to source code.}
\PYG{c+cm}{ * - Uncomment the 'commented out' call to printArray.}
\PYG{c+cm}{ * - Save, recompile, rerun, comparing output to source code.}
\PYG{c+cm}{ * - Explain behavior of MPI\PYGZus{}Reduce() in terms of }
\PYG{c+cm}{ *     srcArr and destArr.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define ARRAY\PYGZus{}SIZE 5}

\PYG{k+kt}{void} \PYG{n}{printArray}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{arrayName}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{array}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{SIZE}\PYG{p}{)}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{myRank} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{srcArr}\PYG{p}{[}\PYG{n}{ARRAY\PYGZus{}SIZE}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{destArr}\PYG{p}{[}\PYG{n}{ARRAY\PYGZus{}SIZE}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+m+mi}{0}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Before reduction: }\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printArray}\PYG{p}{(}\PYG{n}{myRank}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{destArr}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{destArr}\PYG{p}{,} \PYG{n}{ARRAY\PYGZus{}SIZE}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}} 

    \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{unsigned} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{ARRAY\PYGZus{}SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{srcArr}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{myRank} \PYG{o}{*} \PYG{n}{i}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

\PYG{c+c1}{//    printArray(myRank, "srcArr", srcArr, ARRAY\PYGZus{}SIZE);}

    \PYG{n}{MPI\PYGZus{}Reduce}\PYG{p}{(}\PYG{n}{srcArr}\PYG{p}{,} \PYG{n}{destArr}\PYG{p}{,} \PYG{n}{ARRAY\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}SUM}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After reduction:  }\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printArray}\PYG{p}{(}\PYG{n}{myRank}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{destArr}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{destArr}\PYG{p}{,} \PYG{n}{ARRAY\PYGZus{}SIZE}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}} 

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cm}{/* utility to display an array}
\PYG{c+cm}{ * params: id, the rank of the current process}
\PYG{c+cm}{ *         arrayName, the name of the array being displayed}
\PYG{c+cm}{ *         array, the array being displayed}
\PYG{c+cm}{ *         SIZE, the number of items in array.}
\PYG{c+cm}{ * postcondition:}
\PYG{c+cm}{ *         the id, name, and items in array have been printed to stdout.}
\PYG{c+cm}{ */}
\PYG{k+kt}{void} \PYG{n+nf}{printArray}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{arrayName}\PYG{p}{,} \PYG{k+kt}{int} \PYG{o}{*} \PYG{n}{array}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{SIZE}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d, \PYGZpc{}s: [}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{arrayName}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{\PYGZpc{}3d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{array}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
        \PYG{k}{if} \PYG{p}{(}\PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{,}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{]}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/10.reduction2/reduction2.c}


\section{11. Collective communication: Scatter for message-passing data decomposition}
\label{MessagePassing/MPI_Patternlets:collective-communication-scatter-for-message-passing-data-decomposition}
If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the scatter pattern can be used to ensure that each process receives
a copy of its portion of the array.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* scatter.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Scatter()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./scatter}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, varying N: 1, 2, 4, 8}
\PYG{c+cm}{ * - Trace execution through source code.}
\PYG{c+cm}{ * - Explain behavior/effect of MPI\PYGZus{}Scatter().}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}      }\PYG{c+c1}{// MPI}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}    }\PYG{c+c1}{// printf(), etc.}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}   }\PYG{c+c1}{// malloc()}

\PYG{k+kt}{void} \PYG{n}{print}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{arrName}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{arr}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{arrSize}\PYG{p}{)}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{MAX} \PYG{o}{=} \PYG{l+m+mi}{8}\PYG{p}{;}
    \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{arrSend} \PYG{o}{=} \PYG{n+nb}{NULL}\PYG{p}{;}
    \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{arrRcv} \PYG{o}{=} \PYG{n+nb}{NULL}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{numProcs} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{myRank} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numSent} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}                            \PYG{c+c1}{// initialize}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}                                 \PYG{c+c1}{// master process:}
        \PYG{n}{arrSend} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{MAX} \PYG{o}{*} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{int}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}  \PYG{c+c1}{//  allocate array1}
        \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{MAX}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}                \PYG{c+c1}{//  load with values}
            \PYG{n}{arrSend}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{11}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
        \PYG{n}{print}\PYG{p}{(}\PYG{n}{myRank}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{arrSend}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{arrSend}\PYG{p}{,} \PYG{n}{MAX}\PYG{p}{)}\PYG{p}{;}        \PYG{c+c1}{//  display array1}
    \PYG{p}{\PYGZcb{}}
     
    \PYG{n}{numSent} \PYG{o}{=} \PYG{n}{MAX} \PYG{o}{/} \PYG{n}{numProcs}\PYG{p}{;}                          \PYG{c+c1}{// all processes:}
    \PYG{n}{arrRcv} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{numSent} \PYG{o}{*} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{int}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}   \PYG{c+c1}{//  allocate array2}

    \PYG{n}{MPI\PYGZus{}Scatter}\PYG{p}{(}\PYG{n}{arrSend}\PYG{p}{,} \PYG{n}{numSent}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{n}{arrRcv}\PYG{p}{,}     \PYG{c+c1}{//  scatter array1 }
                 \PYG{n}{numSent}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;} \PYG{c+c1}{//   into array2}

    \PYG{n}{print}\PYG{p}{(}\PYG{n}{myRank}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{arrRcv}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{arrRcv}\PYG{p}{,} \PYG{n}{numSent}\PYG{p}{)}\PYG{p}{;}          \PYG{c+c1}{// display array2}

    \PYG{n}{free}\PYG{p}{(}\PYG{n}{arrSend}\PYG{p}{)}\PYG{p}{;}                                     \PYG{c+c1}{// clean up}
    \PYG{n}{free}\PYG{p}{(}\PYG{n}{arrRcv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{void} \PYG{n+nf}{print}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{arrName}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{arr}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{arrSize}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d, \PYGZpc{}s: }\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{arrName}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{arrSize}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{ \PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{arr}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/11.scatter/scatter.c}


\section{12. Collective communication: Gather for message-passing data decomposition}
\label{MessagePassing/MPI_Patternlets:collective-communication-gather-for-message-passing-data-decomposition}
If processes can independently work on portions of a larger data array
using the geometric data decomposition pattern,
the gather pattern can be used to ensure that each process sends
a copy of its portion of the array back to the root, or master process.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* gather.c}
\PYG{c+cm}{ * ... illustrates the use of MPI\PYGZus{}Gather()...}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np N ./gather}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run, varying N: 1, 2, 4, 8.}
\PYG{c+cm}{ * - Trace execution through source.}
\PYG{c+cm}{ * - Explain behavior of MPI\PYGZus{}Gather().}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}       }\PYG{c+c1}{// MPI}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}     }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}    }\PYG{c+c1}{// malloc()}

\PYG{k+kt}{void} \PYG{n}{print}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{arrName}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{arr}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{arrSize}\PYG{p}{)}\PYG{p}{;}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define SIZE 3}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int}  \PYG{n}{computeArray}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{;}                          \PYG{c+c1}{// array1}
   \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{gatherArray} \PYG{o}{=} \PYG{n+nb}{NULL}\PYG{p}{;}                          \PYG{c+c1}{// array2}
   \PYG{k+kt}{int}  \PYG{n}{numProcs} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{myRank} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} 
        \PYG{n}{totalGatheredVals} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}

   \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}                           \PYG{c+c1}{// initialize}
   \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}
   \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{myRank}\PYG{p}{)}\PYG{p}{;}
                                                     \PYG{c+c1}{// all processes:}
   \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}                  \PYG{c+c1}{//  load array1 with}
      \PYG{n}{computeArray}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{myRank} \PYG{o}{*} \PYG{l+m+mi}{10} \PYG{o}{+} \PYG{n}{i}\PYG{p}{;}             \PYG{c+c1}{//   3 distinct values}
   \PYG{p}{\PYGZcb{}}

   \PYG{n}{print}\PYG{p}{(}\PYG{n}{myRank}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{computeArray}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{computeArray}\PYG{p}{,}       \PYG{c+c1}{//  show array1}
           \PYG{n}{SIZE}\PYG{p}{)}\PYG{p}{;}

   \PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}                                \PYG{c+c1}{// master:}
      \PYG{n}{totalGatheredVals} \PYG{o}{=} \PYG{n}{SIZE} \PYG{o}{*} \PYG{n}{numProcs}\PYG{p}{;}           \PYG{c+c1}{//  allocate array2}
      \PYG{n}{gatherArray} \PYG{o}{=} \PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*}\PYG{p}{)} \PYG{n}{malloc}\PYG{p}{(} \PYG{n}{totalGatheredVals} \PYG{o}{*} \PYG{k}{sizeof}\PYG{p}{(}\PYG{k+kt}{int}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}

   \PYG{n}{MPI\PYGZus{}Gather}\PYG{p}{(}\PYG{n}{computeArray}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,}           \PYG{c+c1}{//  gather array1 vals}
               \PYG{n}{gatherArray}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}INT}\PYG{p}{,}           \PYG{c+c1}{//   into array2}
               \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}                   \PYG{c+c1}{//   at master process               }

   \PYG{k}{if} \PYG{p}{(}\PYG{n}{myRank} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{p}{\PYGZob{}}                                \PYG{c+c1}{// master process:}
      \PYG{n}{print}\PYG{p}{(}\PYG{n}{myRank}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{gatherArray}\PYG{l+s}{"}\PYG{p}{,}                   \PYG{c+c1}{//  show array2}
             \PYG{n}{gatherArray}\PYG{p}{,} \PYG{n}{totalGatheredVals}\PYG{p}{)}\PYG{p}{;} 
   \PYG{p}{\PYGZcb{}}

   \PYG{n}{free}\PYG{p}{(}\PYG{n}{gatherArray}\PYG{p}{)}\PYG{p}{;}                                \PYG{c+c1}{// clean up}
   \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
   \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{void} \PYG{n+nf}{print}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{arrName}\PYG{p}{,} \PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{arr}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{arrSize}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Process \PYGZpc{}d, \PYGZpc{}s: }\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{arrName}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{arrSize}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{ \PYGZpc{}d}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{arr}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/12.gather/gather.c}


\section{13. The Barrier Coordination Pattern}
\label{MessagePassing/MPI_Patternlets:the-barrier-coordination-pattern}
A barrier is used when you want all the processes to complete a portion of
code before continuing. Use this exercise to verify that is is ocurring when
you add the call to the MPI\_Barrier funtion.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* barrier.c }
\PYG{c+cm}{ *  ... illustrates the behavior of MPI\PYGZus{}Barrier() ...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, May 2013.}
\PYG{c+cm}{ * Bill Siever, April 2016 }
\PYG{c+cm}{ *   (Converted to master/worker pattern).}
\PYG{c+cm}{ * Joel Adams, April 2016}
\PYG{c+cm}{ *   (Refactored code so that just one barrier needed).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np 8 ./barrier}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ *  - Compile; then run the program several times, }
\PYG{c+cm}{ *     noting the interleaved outputs.}
\PYG{c+cm}{ *  - Uncomment the MPI\PYGZus{}Barrier() call; then recompile and rerun,}
\PYG{c+cm}{ *     noting how the output changes.}
\PYG{c+cm}{ *  - Explain what effect MPI\PYGZus{}Barrier() has on process behavior.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}     }\PYG{c+c1}{// MPI}

\PYG{c+cm}{/* Have workers send messages to the master, which prints them.}
\PYG{c+cm}{ * @param: id, an int}
\PYG{c+cm}{ * @param: numProcesses, an int}
\PYG{c+cm}{ * @param: hostName, a char*}
\PYG{c+cm}{ * @param: position, a char*}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Precondition: this function is being called by an MPI process}
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} id is the MPI rank of that process }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} numProcesses is the number of processes in the computation }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} hostName points to a char array containing the name of the}
\PYG{c+cm}{ *                    host on which this MPI process is running }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} position points to "BEFORE" or "AFTER".}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Postcondition: each process whose id \PYGZgt{} 0 has sent a message to process 0}
\PYG{c+cm}{ *                     containing id, numProcesses, hostName, and position }
\PYG{c+cm}{ *                \PYGZam{}\PYGZam{} process 0 has received and output each message.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define BUFFER\PYGZus{}SIZE 200}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define MASTER      0}

\PYG{k+kt}{void} \PYG{n+nf}{sendReceivePrint}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{hostName}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{position}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{char} \PYG{n}{buffer}\PYG{p}{[}\PYG{n}{BUFFER\PYGZus{}SIZE}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+sc}{'\PYGZbs{}0'}\PYG{p}{\PYGZcb{}}\PYG{p}{;}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Status} \PYG{n}{status}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{id} \PYG{o}{!}\PYG{o}{=} \PYG{n}{MASTER}\PYG{p}{)} \PYG{p}{\PYGZob{}} 
        \PYG{c+c1}{// Worker: Build a message and send it to the Master}
        \PYG{k+kt}{int} \PYG{n}{length} \PYG{o}{=} \PYG{n}{sprintf}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{,}
                              \PYG{l+s}{"}\PYG{l+s}{Process \PYGZsh{}\PYGZpc{}d of \PYGZpc{}d on \PYGZpc{}s is \PYGZpc{}s the barrier.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                                \PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{,} \PYG{n}{position}\PYG{p}{)}\PYG{p}{;}       
        \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{,} \PYG{n}{length}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}
        \PYG{c+c1}{// Master: Receive and print the messages from all Workers}
        \PYG{k}{for}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{numProcesses}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
           \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{,} \PYG{n}{BUFFER\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}ANY\PYGZus{}SOURCE}\PYG{p}{,}
                     \PYG{n}{MPI\PYGZus{}ANY\PYGZus{}TAG}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
           \PYG{n}{printf}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}


\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{char} \PYG{n}{myHostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+sc}{'\PYGZbs{}0'}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{myHostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{sendReceivePrint}\PYG{p}{(}\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{myHostName}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{BEFORE}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//    MPI\PYGZus{}Barrier(MPI\PYGZus{}COMM\PYGZus{}WORLD);}

    \PYG{n}{sendReceivePrint}\PYG{p}{(}\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{myHostName}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{AFTER}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/13.barrier/barrier.c}


\section{14. Timing code using the Barrier Coordination Pattern}
\label{MessagePassing/MPI_Patternlets:timing-code-using-the-barrier-coordination-pattern}
In this example you can run the code several times and determine the average, median, and minimum
execution time when the code has a barrier and when it does not. The primary purpose of this exercise
is to illustrate that one of the most useful uses of a barrier is to ensure that you are getting legitimate
timings for your code examples. By using a barrier, you ensure that all processes have finished before
recording the time using the master node.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* barrier+timing.c }
\PYG{c+cm}{ *  ... illustrates the behavior of MPI\PYGZus{}Barrier() }
\PYG{c+cm}{ *       to coordinate process-timing.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, April 2016}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np 8 ./barrier+timing}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ *  - Compile; then run the program five times, }
\PYG{c+cm}{ *  - In a spreadsheet, compute the average,}
\PYG{c+cm}{ *     median, and minimum of the five times.}
\PYG{c+cm}{ *  - Uncomment the two MPI\PYGZus{}Barrier() calls;}
\PYG{c+cm}{ *     then recompile, rerun five times, and}
\PYG{c+cm}{ *     compute the new average, median, and min}
\PYG{c+cm}{ *     times.}
\PYG{c+cm}{ *  - Why did uncommenting the barrier calls}
\PYG{c+cm}{ *     produce the change you observed?}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}     }\PYG{c+c1}{// MPI}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}unistd.h\PYGZgt{}  }\PYG{c+c1}{// sleep()}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define  MASTER 0}

\PYG{c+cm}{/* answer the ultimate question of life, the universe, }
\PYG{c+cm}{ *  and everything, based on id and numProcs.}
\PYG{c+cm}{ * @param: id, an int}
\PYG{c+cm}{ * @param: numProcs, an int}
\PYG{c+cm}{ * Precondition: id is the MPI rank of this process}
\PYG{c+cm}{ *             \PYGZam{}\PYGZam{} numProcs is the number of MPI processes.}
\PYG{c+cm}{ * Postcondition: The return value is 42.}
\PYG{c+cm}{ */}
\PYG{k+kt}{int} \PYG{n+nf}{solveProblem}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{numProcs}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{n}{sleep}\PYG{p}{(} \PYG{p}{(}\PYG{p}{(}\PYG{k+kt}{double}\PYG{p}{)}\PYG{n}{id}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{/} \PYG{n}{numProcs}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{42}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{startTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{totalTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{answer} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//    MPI\PYGZus{}Barrier(MPI\PYGZus{}COMM\PYGZus{}WORLD);}
    \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{n}{MASTER}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{MPI\PYGZus{}Wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{answer} \PYG{o}{=} \PYG{n}{solveProblem}\PYG{p}{(}\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//    MPI\PYGZus{}Barrier(MPI\PYGZus{}COMM\PYGZus{}WORLD);}
    \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{n}{MASTER} \PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{totalTime} \PYG{o}{=} \PYG{n}{MPI\PYGZus{}Wtime}\PYG{p}{(}\PYG{p}{)} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{The answer is \PYGZpc{}d; computing it took \PYGZpc{}f secs.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                   \PYG{n}{answer}\PYG{p}{,} \PYG{n}{totalTime}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/14.barrier+Timing/barrier+timing.c}


\section{15. Sequence Numbers}
\label{MessagePassing/MPI_Patternlets:sequence-numbers}
Tags can be placed on messages that are sent from a non-master node and received by the master node.
Using tags is an alternative form of simulating the barrier example in example 13 above.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{c+cm}{/* sequenceNumbers.c}
\PYG{c+cm}{ *  ... shows how to acheive barrier-like behavior}
\PYG{c+cm}{ *      by using MPI message tags as sequence numbers.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, April 2016.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: mpirun -np 8 ./sequenceNumbers}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * 1. Compile; then run the program several times, }
\PYG{c+cm}{ *     noting the intermixed outputs}
\PYG{c+cm}{ * 2. Comment out the sendReceivePrint(..., "SECOND", 1); call;}
\PYG{c+cm}{ *      uncomment the sendReceivePrint(..., "SECOND", 2); call;}
\PYG{c+cm}{ *      then recompile and rerun, noting how the output changes.}
\PYG{c+cm}{ * 3. Uncomment the sendReceivePrint(..., "THIRD", 3); }
\PYG{c+cm}{ *      and sendReceivePrint(..., "FOURTH", 4); calls,}
\PYG{c+cm}{ *      then recompile and rerun, noting how the output changes.}
\PYG{c+cm}{ * 4. Explain the differences: what has caused the changes}
\PYG{c+cm}{ *      in the program's behavior, and why?}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}mpi.h\PYGZgt{}     }\PYG{c+c1}{// MPI}

\PYG{c+cm}{/* Have workers send messages to the master, which prints them.}
\PYG{c+cm}{ * @param: id, an int}
\PYG{c+cm}{ * @param: numProcesses, an int}
\PYG{c+cm}{ * @param: hostName, a char*}
\PYG{c+cm}{ * @param: messageNum, a char*}
\PYG{c+cm}{ * @param: tagValue, an int}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Precondition: this routine is being called by an MPI process }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} id is the MPI rank of that process }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} numProcesses is the number of processes in the computation }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} hostName points to a char array containing the name of the}
\PYG{c+cm}{ *                    host on which this MPI process is running }
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} messageNum is "FIRST", "SECOND", "THIRD", ...}
\PYG{c+cm}{ *               \PYGZam{}\PYGZam{} tagValue is the value for the tags of the message}
\PYG{c+cm}{ *                    being sent and received this invocation of the function.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Postcondition: each process whose id \PYGZgt{} 0 has sent a message to process 0}
\PYG{c+cm}{ *                    containing id, numProcesses, hostName, messageNum,}
\PYG{c+cm}{ *                    and tagValue }
\PYG{c+cm}{ *                \PYGZam{}\PYGZam{} process 0 has received and output each message.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define BUFFER\PYGZus{}SIZE 200}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define MASTER      0}

\PYG{k+kt}{void} \PYG{n+nf}{sendReceivePrint}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{hostName}\PYG{p}{,}
                        \PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{messageNum}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{tagValue}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{char} \PYG{n}{buffer}\PYG{p}{[}\PYG{n}{BUFFER\PYGZus{}SIZE}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+sc}{'\PYGZbs{}0'}\PYG{p}{\PYGZcb{}}\PYG{p}{;}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Status} \PYG{n}{status}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{id} \PYG{o}{!}\PYG{o}{=} \PYG{n}{MASTER}\PYG{p}{)} \PYG{p}{\PYGZob{}} 
        \PYG{c+c1}{// Worker: Build a message and send it to the Master}
        \PYG{k+kt}{int} \PYG{n}{length} \PYG{o}{=} \PYG{n}{sprintf}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{,}
                              \PYG{l+s}{"}\PYG{l+s}{This is the \PYGZpc{}s message from process \PYGZsh{}\PYGZpc{}d of \PYGZpc{}d on \PYGZpc{}s.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                                \PYG{n}{messageNum}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{hostName}\PYG{p}{)}\PYG{p}{;}       
        \PYG{n}{MPI\PYGZus{}Send}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{,} \PYG{n}{length}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{tagValue}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}
        \PYG{c+c1}{// Master: Receive and print the messages from all Workers}
        \PYG{k}{for}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{numProcesses}\PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
           \PYG{n}{MPI\PYGZus{}Recv}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{,} \PYG{n}{BUFFER\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}CHAR}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}ANY\PYGZus{}SOURCE}\PYG{p}{,}
                     \PYG{n}{tagValue}\PYG{p}{,} \PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{status}\PYG{p}{)}\PYG{p}{;}
           \PYG{n}{printf}\PYG{p}{(}\PYG{n}{buffer}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}


\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{numProcesses} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{length} \PYG{o}{=} \PYG{o}{-}\PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{char} \PYG{n}{myHostName}\PYG{p}{[}\PYG{n}{MPI\PYGZus{}MAX\PYGZus{}PROCESSOR\PYGZus{}NAME}\PYG{p}{]} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+sc}{'\PYGZbs{}0'}\PYG{p}{\PYGZcb{}}\PYG{p}{;}

    \PYG{n}{MPI\PYGZus{}Init}\PYG{p}{(}\PYG{o}{\PYGZam{}}\PYG{n}{argc}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{argv}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}rank}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{id}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Comm\PYGZus{}size}\PYG{p}{(}\PYG{n}{MPI\PYGZus{}COMM\PYGZus{}WORLD}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{numProcesses}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{MPI\PYGZus{}Get\PYGZus{}processor\PYGZus{}name} \PYG{p}{(}\PYG{n}{myHostName}\PYG{p}{,} \PYG{o}{\PYGZam{}}\PYG{n}{length}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{sendReceivePrint}\PYG{p}{(}\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{myHostName}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{FIRST}\PYG{l+s}{"}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{sendReceivePrint}\PYG{p}{(}\PYG{n}{id}\PYG{p}{,} \PYG{n}{numProcesses}\PYG{p}{,} \PYG{n}{myHostName}\PYG{p}{,} \PYG{l+s}{"}\PYG{l+s}{SECOND}\PYG{l+s}{"}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{;}
\PYG{c+c1}{//    sendReceivePrint(id, numProcesses, myHostName, "SECOND", 2);}
\PYG{c+c1}{//    sendReceivePrint(id, numProcesses, myHostName, "THIRD", 3);}
\PYG{c+c1}{//    sendReceivePrint(id, numProcesses, myHostName, "FOURTH", 4);}

    \PYG{n}{MPI\PYGZus{}Finalize}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

\emph{file: patternlets/MPI/15.sequenceNumbers/sequenceNumbers.c}


\chapter{Shared Memory Parallel Patternlets in OpenMP}
\label{SharedMemory/OpenMP_Patternlets:shared-memory-parallel-patternlets-in-openmp}\label{SharedMemory/OpenMP_Patternlets::doc}
When writing programs for shared-memory hardware with multiple cores,
a programmer could use a
low-level thread package, such as pthreads. An alternative is to use
a compiler that processes OpenMP \emph{pragmas}, which are compiler directives that
enable the compiler to generate threaded code.  Whereas pthreads uses an \textbf{explicit}
multithreading model in which the programmer must explicitly create and manage threads,
OpenMP uses an \textbf{implicit} multithreading model in which the library handles
thread creation and management, thus making the programmer's task much simpler and
less error-prone.  OpenMP is a standard that compilers who implement it must adhere to.

The following are examples of C code with OpenMP pragmas.  There is one C++
example that is used to illustrate a point about that language. The first
three are basic illustrations so you can get used to the OpenMP pragmas and
conceptualize the two primary patterns used as
\textbf{program structure implementation strategies} that almost all shared-memory
parallel programs have:
\begin{itemize}
\item {} 
\textbf{fork/join}:  forking threads and joining them back, and

\item {} 
\textbf{single program, multiple data}:  writing one program in which separate threads maybe performing different computations simultaneously on different data, some of which might be shared in memory.

\end{itemize}

The rest of the examples illustrate how to implement other patterns
along with the above two and what can go wrong when mutual exclusion
is not properly ensured.

Note: by default OpenMP uses the \textbf{Thread Pool} pattern of concurrent execution control.
OpenMP programs initialize a group of threads to be used by a given program
(often called a pool of threads).  These threads will execute concurrently
during portions of the code specified by the programmer.  In addition, the \textbf{multiple instruction, multiple data} pattern is used in OpenMP programs because multiple threads can be executing different instructions on different data in memory at the same point in time.


\section{Source Code}
\label{SharedMemory/OpenMP_Patternlets:source-code}
Please download all examples from this tarball:
\code{openMP.tgz}

A C code file and a Makefile for each example below can be found in
subdirectories of the openMP directory created by extracting the above tarball.
The number for each example below corresponds to one used in subdirectory
names containing each one.

To compile and run these examples, you will need a C compiler with OpenMP.  The GNU C compiler is OpenMP compliant.  We assume you are building and executing these on a Unix command line.


\section{Patternlets Grouped By Type}
\label{SharedMemory/OpenMP_Patternlets:patternlets-grouped-by-type}
If you are working on these for the first time, you may want to visit them in order.  If you are returning to review a particular patternlet or the pattern categorization diagram, you can refer to them individually.

{\hyperref[SharedMemory/ProgStructure_Barrier::doc]{\emph{Shared Memory Program Structure and Coordination Patterns}}}

{\hyperref[SharedMemory/DataDecomp_Reduction::doc]{\emph{Data Decomposition Algorithm Strategies and Related Coordination Strategies}}}

{\hyperref[SharedMemory/MutualExclusion::doc]{\emph{Patterns used when threads share data values}}}

{\hyperref[SharedMemory/TaskDecomp::doc]{\emph{Task Decomposition Algorithm Strategies}}}

{\hyperref[SharedMemory/patterns_diagram::doc]{\emph{Categorizing Patterns}}}


\subsection{Shared Memory Program Structure and Coordination Patterns}
\label{SharedMemory/ProgStructure_Barrier:shared-memory-program-structure-and-coordination-patterns}\label{SharedMemory/ProgStructure_Barrier::doc}

\subsubsection{0. Program Structure Implementation Strategy: The basic fork-join pattern}
\label{SharedMemory/ProgStructure_Barrier:program-structure-implementation-strategy-the-basic-fork-join-pattern}
\emph{file: openMP/00.forkJoin/forkJoin.c}

\emph{Build inside 00.forkJoin directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make forkjoin
\end{Verbatim}

\emph{Execute on the command line inside 00.forkJoin directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./forkjoin
\end{Verbatim}

The \emph{omp parallel} pragma on line 21, when uncommented, tells the compiler to
fork a set of threads to execute the next line of code (later you will see how this is done for a block of code).  You can conceptualize how this works using the following diagram, where time is moving from left to right:

\includegraphics{ForkJoin.png}

Observe what happens on the machine
where you are running this code, both when you have the pragma commented (no fork) and when you uncomment it (adding a fork).

Note that in OpenMP the join is implicit and does not require a pragma directive.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* forkJoin.c}
\PYG{c+cm}{ * ... illustrates the fork-join pattern }
\PYG{c+cm}{ *      using OpenMP's parallel directive.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./forkJoin}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile \PYGZam{} run, uncomment the pragma,}
\PYG{c+cm}{ *    recompile \PYGZam{} run, compare results.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}     }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}       }\PYG{c+c1}{// OpenMP}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Before...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel }
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{During...}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{1. Program Structure Implementation Strategy: Fork-join with setting the number of threads}
\label{SharedMemory/ProgStructure_Barrier:program-structure-implementation-strategy-fork-join-with-setting-the-number-of-threads}
\emph{file openMP/01.forkJoin2/forkJoin2.c}

\emph{Build inside 01.forkJoin2 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make forkjoin2
\end{Verbatim}

\emph{Execute on the command line inside 01.forkJoin2 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./forkjoin2
\end{Verbatim}

This code illustrates that one program can fork and join more than once
and that programmers can set the number of threads to use in the parallel forked code.

Note on line 28 there is an OpenMP function called \emph{omp\_set\_num\_threads}
for setting the number of threads to use for each
\emph{fork}, which occur when the omp\_parallel pragma is used.
Also note on line 35 that you can set the number of threads for the very next
fork indicated by an omp\_parallel pragma by augmenting the pragma as shown in line 35.
Follow the instructions in the header of the code file to understand the difference
between these.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* forkJoin2.c}
\PYG{c+cm}{ * ... illustrates the fork-join pattern }
\PYG{c+cm}{ *      using multiple OpenMP parallel directives,}
\PYG{c+cm}{ *      and changing the number of threads two ways.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, May 2013.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./forkJoin2}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile \PYGZam{} run, compare results to source.}
\PYG{c+cm}{ * - Predict how many threads will be used in 'Part IV'?}
\PYG{c+cm}{ * - Uncomment 'Part IV', recompile, rerun.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}    }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}      }\PYG{c+c1}{// OpenMP}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Beginning}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel }
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Part I}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Between I and II...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel }
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Part II...}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Between II and III...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel num\PYGZus{}threads(5)}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Part III...}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
\PYG{c+cm}{/*}
\PYG{c+cm}{    printf("\PYGZbs{}n\PYGZbs{}nBetween III and IV...\PYGZbs{}n");}

\PYG{c+cm}{    \PYGZsh{}pragma omp parallel }
\PYG{c+cm}{    printf("\PYGZbs{}nPart IV...");}
\PYG{c+cm}{*/}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{End}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{2. Program Structure Implementation Strategy: Single Program, multiple data}
\label{SharedMemory/ProgStructure_Barrier:program-structure-implementation-strategy-single-program-multiple-data}
\emph{file: openMP/02.spmd/spmd.c}

\emph{Build inside 02.spmd directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make spmd
\end{Verbatim}

\emph{Execute on the command line inside 02.spmd directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./spmd
\end{Verbatim}

Note how there are OpenMP functions to
obtain a thread number and the total number of threads.
We have one program, but multiple threads executing in the forked section,
each with a copy of the id and num\_threads variables.
Programmers write one program, but write it in such a way that
each thread has its own data values for particular variables.
This is why this is called the \emph{single program, multiple data} (SPMD) pattern.

Most parallel programs use this SPMD pattern, because writing one program
is ultimately the most efficient method for programmers.  It does require you
as a programmer to understand how this works, however.  Think carefully about
how each thread executing in parallel has its own set of variables.  Conceptually,
it looks like this, where each thread has its own memory for the variables id and numThreads:

\includegraphics{ForkJoin_SPMD.png}

When the pragma is uncommented in the code below, note what the default number of threads
is.  Here the threads are forked and execute the block of code inside the
curly braces on lines 22 through 26.  This is how we can have a block of code executed
concurrently on each thread.

When you execute the parallel version containing the pragma (uncommenting line 20),
what do you observe about the order of the printed lines?  Run the program multiple times--
does the ordering change?  This illustrates an important point about threaded programs:
\emph{the ordering of execution of statements between threads is not guaranteed.}  This is also
illustrated in the diagram above.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* spmd.c}
\PYG{c+cm}{ * ... illustrates the single-program-multiple-data (SPMD)}
\PYG{c+cm}{ *      pattern using two basic OpenMP commands...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./spmd}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile \PYGZam{} run }
\PYG{c+cm}{ * - Uncomment pragma, recompile \PYGZam{} run, compare results}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel }
    \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Hello from thread \PYGZpc{}d of \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{3. Program Structure Implementation Strategy: Single Program, multiple data with user-defined number of threads}
\label{SharedMemory/ProgStructure_Barrier:program-structure-implementation-strategy-single-program-multiple-data-with-user-defined-number-of-threads}
\emph{file: openMP/03.spmd2/spmd2.c}

\emph{Build inside 03.spmd2 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make spmd2
\end{Verbatim}

\emph{Execute on the command line inside 03.spmd2 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./spmd2 4
Replace 4 with other values for the number of threads
\end{Verbatim}

Here we enter the number of threads to use on the command line.  This is a useful way to
make your code versatile so that you can use as many threads as you would like.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* spmd2.c}
\PYG{c+cm}{ * ... illustrates the SPMD pattern in OpenMP,}
\PYG{c+cm}{ * 	using the commandline arguments }
\PYG{c+cm}{ *      to control the number of threads.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./spmd2 [numThreads]}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile \PYGZam{} run with no commandline args }
\PYG{c+cm}{ * - Rerun with different commandline args,}
\PYG{c+cm}{ *    until you see a problem with thread ids}
\PYG{c+cm}{ * - Fix the race condition}
\PYG{c+cm}{ *    (if necessary, compare to 02.spmd)}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel }
    \PYG{p}{\PYGZob{}}
        \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Hello from thread \PYGZpc{}d of \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{4. Coordination: Synchronization with a Barrier}
\label{SharedMemory/ProgStructure_Barrier:coordination-synchronization-with-a-barrier}
\emph{file: openMP/04.barrier/barrier.c}

\emph{Build inside 04.barrier directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make barrier
\end{Verbatim}

\emph{Execute on the command line inside 04.barrier directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./barrier 4
Replace 4 with other values for the number of threads
\end{Verbatim}

The barrier pattern is used in parallel programs to ensure that all threads complete
a parallel section of code before execution continues. This can be necessary when
threads are generating computed data (in an array, for example) that needs to be
completed for use in another computation.

Conceptually, the running code is excuting like this:

\includegraphics{ForkJoin_Barrier.png}

Note what happens with and without the commented pragma on line 31.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* barrier.c}
\PYG{c+cm}{ * ... illustrates the use of the OpenMP barrier command,}
\PYG{c+cm}{ * 	using the commandline to control the number of threads...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, May 2013.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./barrier [numThreads]}
\PYG{c+cm}{ * }
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile \PYGZam{} run several times, noting interleaving of outputs.}
\PYG{c+cm}{ * - Uncomment the barrier directive, recompile, rerun,}
\PYG{c+cm}{ *    and note the change in the outputs.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel }
    \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Thread \PYGZpc{}d of \PYGZpc{}d is BEFORE the barrier.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}

\PYG{c+c1}{//        \PYGZsh{}pragma omp barrier }

        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Thread \PYGZpc{}d of \PYGZpc{}d is AFTER the barrier.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{5. Program Structure: The Master-Worker Implementation Strategy}
\label{SharedMemory/ProgStructure_Barrier:program-structure-the-master-worker-implementation-strategy}
\emph{file: openMP/05.masterWorker/masterWorker.c}

\emph{Build inside 05.masterWorker directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make masterWorker
\end{Verbatim}

\emph{Execute on the command line inside 05.masterWorker directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./masterWorker 4
Replace 4 with other values for the number of threads
\end{Verbatim}

Once you have mastered the notion of fork-join and single-program, multiple data,
the next common pattern that programmers use in association with these patterns
is to have one thread, called the master, execute one block of code when it forks while the rest
of the threads, called workers, execute a different block of code when they fork.
This is illustrated in this simple example (useful code would be more complicated).

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* masterWorker.c}
\PYG{c+cm}{ * ... illustrates the master-worker pattern in OpenMP}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./masterWorker}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * - Compile and run as is.}
\PYG{c+cm}{ * - Uncomment \PYGZsh{}pragma directive, re-compile and re-run}
\PYG{c+cm}{ * - Compare and trace the different executions.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}   }\PYG{c+c1}{// atoi()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}     }\PYG{c+c1}{// OpenMP}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel }
    \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}

        \PYG{k}{if} \PYG{p}{(} \PYG{n}{id} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// thread with ID 0 is master}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from the master, \PYGZsh{} \PYGZpc{}d of \PYGZpc{}d threads}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			    \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}} \PYG{k}{else} \PYG{p}{\PYGZob{}}          \PYG{c+c1}{// threads with IDs \PYGZgt{} 0 are workers }
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Greetings from a worker, \PYGZsh{} \PYGZpc{}d of \PYGZpc{}d threads}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
			    \PYG{n}{id}\PYG{p}{,} \PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{Data Decomposition Algorithm Strategies and Related Coordination Strategies}
\label{SharedMemory/DataDecomp_Reduction::doc}\label{SharedMemory/DataDecomp_Reduction:data-decomposition-algorithm-strategies-and-related-coordination-strategies}

\subsubsection{6. Shared Data Decomposition Algorithm Strategy:  chunks of data per thread using a parallel for loop implementation strategy}
\label{SharedMemory/DataDecomp_Reduction:shared-data-decomposition-algorithm-strategy-chunks-of-data-per-thread-using-a-parallel-for-loop-implementation-strategy}
\emph{file: openMP/06.parallelLoop-equalChunks/parallelLoopEqualChunks.c}

\emph{Build inside 06.parallelLoop-equalChunks directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make parallelLoopEqualChunks
\end{Verbatim}

\emph{Execute on the command line inside 06.parallelLoop-equalChunks directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./parallelLoopEqualChunks 4
Replace 4 with other values for the number of threads, or leave off
\end{Verbatim}

An iterative for loop is a remarkably common pattern in all programming, primarily used to
perform a calculation N times, often over a set of data containing N elements, using each
element in turn inside the for loop.  If there are no dependencies between the calculations
(i.e. the order of them is not important), then the code inside the loop can be split
between forked threads.  When doing this, a decision the programmer needs to make is to
decide how to partition the work between the threads by answering this question:
\begin{itemize}
\item {} 
How many and which iterations of the loop will each thread complete on its own?

\end{itemize}

We refer to this as the \textbf{data decomposition} pattern because we are decomposing the
amount of work to be done (typically on a set of data) across multiple threads.
In the following code, this is done in OpenMP using the \emph{omp parallel for} pragma
just in front of the for statement (line 27) in the following code.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* parallelLoopEqualChunks.c}
\PYG{c+cm}{ * ... illustrates the use of OpenMP's default parallel for loop in which}
\PYG{c+cm}{ *  	threads iterate through equal sized chunks of the index range}
\PYG{c+cm}{ *	(cache-beneficial when accessing adjacent memory locations).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./parallelLoopEqualChunks [numThreads]}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise}
\PYG{c+cm}{ * - Compile and run, comparing output to source code}
\PYG{c+cm}{ * - try with different numbers of threads, e.g.: 2, 3, 4, 6, 8}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}    }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}   }\PYG{c+c1}{// atoi()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}      }\PYG{c+c1}{// OpenMP}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{16}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for  }
    \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Thread \PYGZpc{}d performed iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                 \PYG{n}{id}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

Once you run this code, verify that the default behavior for this pragma is this
sort of decomposition of iterations of the loop to threads, when you set the
number of threads to 4 on the command line:

\includegraphics{ParalleFor_Chunks-4_threads-1.png}

What happens when the number of iterations (16 in this code) is not evenly divisible by the number of threads?  Try several cases to be certain how the compiler splits up the work.
This type of decomposition is commonly used when accessing data that is stored in
consecutive memory locations (such as an array) that might be cached by each thread.


\subsubsection{7. Shared Data Decomposition Algorithm Strategy:  one iteration per thread in a parallel for loop implementation strategy}
\label{SharedMemory/DataDecomp_Reduction:shared-data-decomposition-algorithm-strategy-one-iteration-per-thread-in-a-parallel-for-loop-implementation-strategy}
\emph{file: openMP/07.parallelLoop-chunksOf1/parallelLoopChunksOf1.c}

\emph{Build inside 07.parallelLoop-chunksOf1 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make parallelLoopChunksOf1
\end{Verbatim}

\emph{Execute on the command line inside 07.parallelLoop-chunksOf1 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./parallelLoopChunksOf1 4
Replace 4 with other values for the number of threads, or leave off
\end{Verbatim}

You can imagine other ways of assigning threads to iterations of a loop besides that
shown above for four threads and 16 iterations.  A simple decomposition sometimes used
when your loop is not accessing consecutive memory locations would be to let each
thread do one iteration, up to N threads, then
start again with thread 0 taking the next iteration.  This is declared in OpenMP
using the pragma on line 31 of the following code.  Also note that the commented code
below it is an alternative explicit way of doing it.  The schedule clause is the preferred style
when using OpenMP and is more versatile, because you can easily change the \emph{chunk size}
that each thread will work on.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* parallelLoopChunksOf1.c}
\PYG{c+cm}{ * ... illustrates how to make OpenMP map threads to }
\PYG{c+cm}{ *	parallel loop iterations in chunks of size 1}
\PYG{c+cm}{ *	(use when not accesssing memory).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./parallelLoopChunksOf1 [numThreads]}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * 1. Compile and run, comparing output to source code,}
\PYG{c+cm}{ *    and to the output of the 'equal chunks' version.}
\PYG{c+cm}{ * 2. Uncomment the "commented out" code below,}
\PYG{c+cm}{ *    and verify that both loops produce the same output.}
\PYG{c+cm}{ *    The first loop is simpler but more restrictive;}
\PYG{c+cm}{ *    the second loop is more complex but less restrictive.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{16}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for schedule(static,1)}
    \PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Thread \PYGZpc{}d performed iteration \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                 \PYG{n}{id}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

\PYG{c+cm}{/*}
\PYG{c+cm}{    printf("\PYGZbs{}n---\PYGZbs{}n\PYGZbs{}n");}

\PYG{c+cm}{    \PYGZsh{}pragma omp parallel}
\PYG{c+cm}{    \PYGZob{}}
\PYG{c+cm}{        int id = omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num();}
\PYG{c+cm}{        int numThreads = omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads();}
\PYG{c+cm}{        for (int i = id; i \PYGZlt{} REPS; i += numThreads) \PYGZob{}}
\PYG{c+cm}{            printf("Thread \PYGZpc{}d performed iteration \PYGZpc{}d\PYGZbs{}n", }
\PYG{c+cm}{                     id, i);}
\PYG{c+cm}{        \PYGZcb{}}
\PYG{c+cm}{    \PYGZcb{}}
\PYG{c+cm}{*/}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

This can be made even more
efficient if the next available thread simply takes the next iteration.
In OpenMP, this is done by using \emph{dynamic} scheduling instead of the static scheduling shown
in the above code.  Also note that the number of iterations, or chunk size, could
be greater than 1 inside the schedule clause.


\subsubsection{8. Coordination Using Collective Communication: Reduction}
\label{SharedMemory/DataDecomp_Reduction:coordination-using-collective-communication-reduction}
\emph{file: openMP/08.reduction/reduction.c}

\emph{Build inside 08.reduction directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make reduction
\end{Verbatim}

\emph{Execute on the command line inside 08.reduction directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./reduction 4
Replace 4 with other values for the number of threads, or leave off
\end{Verbatim}

Once threads have performed independent concurrent computations, possibly
on some portion of decomposed data, it is quite common to then \emph{reduce}
those individual computations into one value. This type of operation is
called a \textbf{collective communication} pattern because the threads must somehow
work together to create the final desired single value.

In this example, an array of randomly assigned integers represents a set of shared data (a more realistic program would perform a computation
that creates meaningful data values; this is just an example).
Note the common sequential code pattern found in the function called \emph{sequentialSum} in the code
below (starting line 51): a for loop is used to sum up all the values in the array.

Next let's consider how this can be done in parallel with threads.
Somehow the threads must implicitly \emph{communicate} to keep the overall sum updated
as each of them works on a portion of the array.
In the \emph{parallelSum} function, line 64 shows a special clause that
can be used with the parallel for pragma in OpenMP for this. All values
in the array are summed together by using the OpenMP
parallel for pragma with the \emph{reduction(+:sum)} clause on the variable \textbf{sum},
which is computed in line 66.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* reduction.c}
\PYG{c+cm}{ * ... illustrates the OpenMP parallel-for loop's reduction clause}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./reduction }
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ * - Compile and run.  Note that correct output is produced.}
\PYG{c+cm}{ * - Uncomment \PYGZsh{}pragma in function parallelSum(), }
\PYG{c+cm}{ *    but leave its reduction clause commented out}
\PYG{c+cm}{ * - Recompile and rerun.  Note that correct output is NOT produced.}
\PYG{c+cm}{ * - Uncomment 'reduction(+:sum)' clause of \PYGZsh{}pragma in parallelSum()}
\PYG{c+cm}{ * - Recompile and rerun.  Note that correct output is produced again.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}   }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}     }\PYG{c+c1}{// OpenMP}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}  }\PYG{c+c1}{// rand()}

\PYG{k+kt}{void} \PYG{n}{initialize}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{int} \PYG{n}{sequentialSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)}\PYG{p}{;}
\PYG{k+kt}{int} \PYG{n}{parallelSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)}\PYG{p}{;}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define SIZE 1000000}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{array}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{;}

   \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}

   \PYG{n}{initialize}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{)}\PYG{p}{;}
   \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Seq. sum: }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{\PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Par. sum: }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{\PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
            \PYG{n}{sequentialSum}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{parallelSum}\PYG{p}{(}\PYG{n}{array}\PYG{p}{,} \PYG{n}{SIZE}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}

   \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}} 

\PYG{c+cm}{/* fill array with random values */}
\PYG{k+kt}{void} \PYG{n+nf}{initialize}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{1000}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+cm}{/* sum the array sequentially */}
\PYG{k+kt}{int} \PYG{n+nf}{sequentialSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{sum} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{sum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
   \PYG{k}{return} \PYG{n}{sum}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{c+cm}{/* sum the array using multiple threads */}
\PYG{k+kt}{int} \PYG{n+nf}{parallelSum}\PYG{p}{(}\PYG{k+kt}{int}\PYG{o}{*} \PYG{n}{a}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
   \PYG{k+kt}{int} \PYG{n}{sum} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
   \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
\PYG{c+c1}{//   \PYGZsh{}pragma omp parallel for // reduction(+:sum)}
   \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
      \PYG{n}{sum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{a}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{;}
   \PYG{p}{\PYGZcb{}}
   \PYG{k}{return} \PYG{n}{sum}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\paragraph{Something to think about}
\label{SharedMemory/DataDecomp_Reduction:something-to-think-about}
Do you have an ideas about why the parallel for pragma without the reduction clause did not
produce the correct result?  Later examples will hopefully shed some light on this.


\subsubsection{9. Coordination Using Collective Communication: Reduction revisited}
\label{SharedMemory/DataDecomp_Reduction:coordination-using-collective-communication-reduction-revisited}
\emph{Build inside 09.reduction-userDefined directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make reduction2
\end{Verbatim}

\emph{Execute on the command line inside 09.reduction-userDefined directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./reduction  4 4096
Replace 4 with other values for the number of threads
Replace 4096 with other values for n (computing up to n factorial)
\end{Verbatim}

The next example uses many threads to generate computations of factorials of n. Though there are likely other better ways to compute factorials, this
example uses a very simple approach to illustrate how reduction can be used with the
multiplication operation instead of addition in the previous example. The pragma for
this is on line 34 in the code below, which also makes use of an additional C++ file, BigInt.h:

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* reduction2.cpp computes a table of factorial values,}
\PYG{c+cm}{ *  using Owen Astrachan's BigInt class to explore}
\PYG{c+cm}{ *  OpenMP's user-defined reductions.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ *  Joel Adams, Calvin College, December 2015.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ *  Usage: ./reduction2 [numThreads] [n]}
\PYG{c+cm}{ *}
\PYG{c+cm}{ *  Exercise:}
\PYG{c+cm}{ *  - Build and run, record sequential time in a spreadsheet}
\PYG{c+cm}{ *  - Uncomment \PYGZsh{}pragma omp parallel for directive, rebuild,}
\PYG{c+cm}{ *     and read the error message carefully.}
\PYG{c+cm}{ *  - Uncomment the \PYGZsh{}pragma omp declare directive, rebuild,}
\PYG{c+cm}{ *     and note the user-defined * reduction for a BigInt.}
\PYG{c+cm}{ *  - Rerun, using 2, 4, 6, 8, ... threads, recording}
\PYG{c+cm}{ *     the times in the spreadsheet.}
\PYG{c+cm}{ *  - Create a chart that plots the times vs the \PYGZsh{} of threads.}
\PYG{c+cm}{ *  - Experiment with different n values}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include "BigInt.h"}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}cassert\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}

\PYG{c+cm}{/*}
\PYG{c+cm}{\PYGZsh{}pragma omp declare reduction(*: BigInt: \PYGZbs{}}
\PYG{c+cm}{				omp\PYGZus{}out = omp\PYGZus{}out * omp\PYGZus{}in) \PYGZbs{}}
\PYG{c+cm}{				initializer( omp\PYGZus{}priv=BigInt(1))}
\PYG{c+cm}{*/}

\PYG{n}{BigInt} \PYG{n}{factorial}\PYG{p}{(}\PYG{k+kt}{unsigned} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{n}{BigInt} \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}

\PYG{c+c1}{//	\PYGZsh{}pragma omp parallel for reduction(*:result) }
	\PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{unsigned} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{n}\PYG{p}{;} \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{result} \PYG{o}{*}\PYG{o}{=} \PYG{n}{i}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

	\PYG{k}{return} \PYG{n}{result}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}  \PYG{c+c1}{// on a 2GHz i7 CPU:}
	\PYG{k+kt}{unsigned} \PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{4096}\PYG{p}{;}         \PYG{c+c1}{// \PYGZti{}10 secs sequentially}
	\PYG{k+kt}{unsigned} \PYG{n}{numThreads} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}

	\PYG{k}{switch} \PYG{p}{(}\PYG{n}{argc}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{k}{case} \PYG{l+m+mi}{3}\PYG{o}{:} \PYG{n}{n} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
		\PYG{k}{case} \PYG{l+m+mi}{2}\PYG{o}{:} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{;}
		\PYG{k}{case} \PYG{l+m+mi}{1}\PYG{o}{:} \PYG{k}{break}\PYG{p}{;}
		\PYG{k}{default}\PYG{o}{:} \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Usage: ./reduction2 [numThreads] [n]}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{n}{numThreads}\PYG{p}{)}\PYG{p}{;}

	\PYG{k+kt}{double} \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
	\PYG{n}{BigInt} \PYG{n}{nFactorial} \PYG{o}{=} \PYG{n}{factorial}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{;}
	\PYG{k+kt}{double} \PYG{n}{time} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}

	\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{Computing }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{n} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{! using }\PYG{l+s}{"}
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{numThreads}  \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ threads took: }\PYG{l+s}{"} 
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{time} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ secs}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

	\PYG{c+c1}{// run a few tests to validate the results}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{2} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{6} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{24} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{120} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{32}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{n}{BigInt}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{263130836933693530167218012160000000}\PYG{l+s}{"}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorial}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{=}\PYG{o}{=} \PYG{n}{BigInt}\PYG{p}{(} \PYG{n}{string}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{9332621544394415268169923885}\PYG{l+s}{"}\PYG{p}{)}
			                            \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{6266700490715968264381621468}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{5929638952175999932299156089}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{4146397615651828625369792082}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{7223758251185210916864000000}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{000000000000000000}\PYG{l+s}{"} \PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{All tests passed!}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{flush}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}

With this code you can begin to explore the time it takes to execute the program when using increasing numbers of threads for various values of n. Follow the instructions at the top of the file.


\subsubsection{10. Dynamic Data Decomposition}
\label{SharedMemory/DataDecomp_Reduction:dynamic-data-decomposition}
\emph{Build inside 10.parallelLoop-dynamicSchedule directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make dynamicScheduling
\end{Verbatim}

\emph{Execute on the command line inside 10.parallelLoop-dynamicSchedule directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./dynamicScheduling 4
Replace 4 with other values for the number of threads
\end{Verbatim}

The following example computes factorials for the numbers 2 through 512, placing the result in an array. This array of results is the data in this data decomposition pattern. Since each number will take a different amount of time to compute, this is
a case where using dynamic scheduling of the work improves the performance. Try the tasks lsited in the header of the code shown below to see this.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* dynamicScheduling.cpp computes a table of factorial values,}
\PYG{c+cm}{ *  using Owen Astrachan's BigInt class to explore}
\PYG{c+cm}{ *  OpenMP's schedule() clause.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ *  @author: Joel Adams, Calvin College, Dec 2015.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ *  Usage: ./dynamicScheduling [numThreads]}
\PYG{c+cm}{ *}
\PYG{c+cm}{ *  Exercise:}
\PYG{c+cm}{ *  - Build and run, record sequential run time in a spreadsheet}
\PYG{c+cm}{ *  - Uncomment \PYGZsh{}pragma omp parallel for, rebuild,}
\PYG{c+cm}{ *      run using 2, 4, 6, 8, ... threads, record run times.}
\PYG{c+cm}{ *  - Uncomment schedule(dynamic), rebuild,}
\PYG{c+cm}{ *      run using 2, 4, 6, 8, ... threads, record run times.}
\PYG{c+cm}{ *  - Create a line chart plotting run times vs \PYGZsh{} of threads.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include "BigInt.h"                 }\PYG{c+c1}{// class BigInt}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}                    }\PYG{c+c1}{// OpenMP functions}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}cassert\PYGZgt{}                  }\PYG{c+c1}{// assert()}

\PYG{c+cm}{/* factorial(n) computes n!}
\PYG{c+cm}{ * @param: n, an unsigned int.}
\PYG{c+cm}{ * @return: n!, a BigInt.}
\PYG{c+cm}{ */}
\PYG{n}{BigInt} \PYG{n}{factorial}\PYG{p}{(}\PYG{k+kt}{unsigned} \PYG{n}{n}\PYG{p}{)} \PYG{p}{\PYGZob{}}
	\PYG{n}{BigInt} \PYG{n}{result} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}              \PYG{c+c1}{// 0! or 1!}

	\PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{unsigned} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{2}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{n}\PYG{p}{;} \PYG{o}{+}\PYG{o}{+}\PYG{n}{i}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{result} \PYG{o}{*}\PYG{o}{=} \PYG{n}{i}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}

	\PYG{k}{return} \PYG{n}{result}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}            \PYG{c+c1}{// on a 2 GHz i7 CPU:}
	\PYG{k}{const} \PYG{k+kt}{unsigned} \PYG{n}{MAX} \PYG{o}{=} \PYG{l+m+mi}{512}\PYG{p}{;}            \PYG{c+c1}{//  \PYGZti{}14 secs sequentially}
\PYG{c+c1}{//	const unsigned MAX = 800;            //  \PYGZti{}60 secs sequentially}
	\PYG{n}{BigInt} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{n}{MAX}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{;}

	\PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}} \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} \PYG{p}{\PYGZcb{}}

	\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Depending on the speed of your computer,}\PYG{l+s}{"}
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{  this program may take a while to complete,}\PYG{l+s}{"}
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{  so please wait patiently...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

	\PYG{k+kt}{double} \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{c+c1}{//	\PYGZsh{}pragma omp parallel for // schedule(dynamic)}
	\PYG{k}{for} \PYG{p}{(}\PYG{k+kt}{unsigned} \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{MAX}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
		\PYG{n}{factorialTable}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{factorial}\PYG{p}{(}\PYG{n}{i}\PYG{p}{)}\PYG{p}{;}
	\PYG{p}{\PYGZcb{}}
	\PYG{k+kt}{double} \PYG{n}{totalTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}

	\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{Computing 0! .. }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{MAX} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{! took: }\PYG{l+s}{"} 
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{totalTime} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ secs}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}

	\PYG{c+c1}{// run a few tests to validate the results}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{2} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{6} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{24} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{l+m+mi}{120} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{32}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{n}{BigInt}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{263130836933693530167218012160000000}\PYG{l+s}{"}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{assert}\PYG{p}{(} \PYG{n}{factorialTable}\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{]} \PYG{o}{=}\PYG{o}{=} \PYG{n}{BigInt}\PYG{p}{(} \PYG{n}{string}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{9332621544394415268169923885}\PYG{l+s}{"}\PYG{p}{)}
			                            \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{6266700490715968264381621468}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{5929638952175999932299156089}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{4146397615651828625369792082}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{7223758251185210916864000000}\PYG{l+s}{"}
						    \PYG{o}{+} \PYG{l+s}{"}\PYG{l+s}{000000000000000000}\PYG{l+s}{"} \PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
	\PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{All tests passed!}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{endl}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{Patterns used when threads share data values}
\label{SharedMemory/MutualExclusion:patterns-used-when-threads-share-data-values}\label{SharedMemory/MutualExclusion::doc}

\subsubsection{11. Shared Data Algorithm Strategy: Parallel-for-loop pattern needs non-shared, private variables}
\label{SharedMemory/MutualExclusion:shared-data-algorithm-strategy-parallel-for-loop-pattern-needs-non-shared-private-variables}
\emph{file: openMP/11.private/private.c}

\emph{Build inside 11.private directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make private
\end{Verbatim}

\emph{Execute on the command line inside 11.private directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./private
\end{Verbatim}

In this example, you will try a parallel for loop where additional variables (i, j in the code) cannot be shared by all of the threads, but must instead be \emph{private} to each thread, which means that each thread has its own copy of that variable.  In this case, the outer loop is being split into chunks and given to each thread, but the inner loop is being executed by each thread for each of the elements in its chunk.  The loop counting variables must be maintained separately by each thread.  Because they were initially declared outside the loops at the begininning of the program, by default these variables are shared by all the threads.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* private.c}
\PYG{c+cm}{ * ... illustrates why private variables are needed with OpenMP's parallel for loop}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./private }
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: }
\PYG{c+cm}{ * - Run, noting that the sequential program produces correct results}
\PYG{c+cm}{ * - Uncomment line A, recompile/run and compare}
\PYG{c+cm}{ * - Recomment line A, uncomment line B, recompile/run and compare}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{define SIZE 100}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{,} \PYG{n}{ok} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{m}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{[}\PYG{n}{SIZE}\PYG{p}{]}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{c+c1}{// set all array entries to 1}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for                     // A}
\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel for private(i,j)        // B}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k}{for} \PYG{p}{(}\PYG{n}{j} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{j} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{j}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
            \PYG{n}{m}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{;}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{c+c1}{// test (without using threads)}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{k}{for} \PYG{p}{(}\PYG{n}{j} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{j} \PYG{o}{\PYGZlt{}} \PYG{n}{SIZE}\PYG{p}{;} \PYG{n}{j}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
            \PYG{k}{if} \PYG{p}{(} \PYG{n}{m}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{j}\PYG{p}{]} \PYG{o}{!}\PYG{o}{=} \PYG{l+m+mi}{1} \PYG{p}{)} \PYG{p}{\PYGZob{}}
                \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Element [\PYGZpc{}d,\PYGZpc{}d] not set... }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{)}\PYG{p}{;}
                \PYG{n}{ok} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;}
            \PYG{p}{\PYGZcb{}}
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{if} \PYG{p}{(} \PYG{n}{ok} \PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{All elements correctly set to 1}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{12. Race Condition: missing the mutual exclusion coordination pattern}
\label{SharedMemory/MutualExclusion:race-condition-missing-the-mutual-exclusion-coordination-pattern}
\emph{file: openMP/12.mutualExclusion-atomic/atomic.c}

\emph{Build inside 12.mutualExclusion-atomic directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make atomic
\end{Verbatim}

\emph{Execute on the command line inside 12.mutualExclusion-atomic directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./atomic
\end{Verbatim}

When a variable must be shared by all the threads, as in this example below, an issue called a \emph{race condition} can occur when the threads are updating that variable concurrently.  This happens because there are multiple underlying machine instructions needed to complete the update of the memory location and each thread must execute all of them atomically before another thread does so, thus ensuring \textbf{mutual exclusion} between the threads when updating a shared variable.  This is done using the OpenMP pragma shown in this code.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* atomic.c}
\PYG{c+cm}{ * ... illustrates a race condition when multiple threads read from / }
\PYG{c+cm}{ *  write to a shared variable (and explores OpenMP atomic operations).}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./atomic}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile and run 10 times; note that it always produces the correct balance: \PYGZdl{}1,000,000.00}
\PYG{c+cm}{ *  - To parallelize, uncomment A, recompile and rerun multiple times, compare results}
\PYG{c+cm}{ *  - To fix: uncomment B, recompile and rerun, compare}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}  }\PYG{c+c1}{// printf()}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}    }\PYG{c+c1}{// OpenMP}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{1000000}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
  
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Your starting bank account balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
               \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate many deposits}
    \PYG{c+c1}{// \PYGZsh{}pragma omp parallel for                      // A}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{c+c1}{// \PYGZsh{}pragma omp atomic                        // B}
        \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{1.0}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}1 deposits, your balance is \PYGZdl{}\PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
		\PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{13. The Mutual Exclusion Coordination Pattern: two ways to ensure}
\label{SharedMemory/MutualExclusion:the-mutual-exclusion-coordination-pattern-two-ways-to-ensure}
\emph{file: openMP/13.mutualExclusion-critical/critical.c}

\emph{Build inside 13.mutualExclusion-critical directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make critical
\end{Verbatim}

\emph{Execute on the command line inside 13.mutualExclusion-critical directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./critical
\end{Verbatim}

Here is another way to ensure \textbf{mutual exclusion} in OpenMP.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* critical.c}
\PYG{c+cm}{ * ... fixes a race condition when multiple threads read from / }
\PYG{c+cm}{ *  write to a shared variable	using the OpenMP critical directive.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./critical}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile and run several times; note that it always produces the correct balance \PYGZdl{}1,000,000.00 }
\PYG{c+cm}{ *  - Comment out A; recompile/run, and note incorrect result}
\PYG{c+cm}{ *  - To fix: uncomment B1+B2+B3, recompile and rerun, compare}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{1000000}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
  
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Your starting bank account balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate many deposits}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for}
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp atomic                          }\PYG{c+c1}{// A}
\PYG{c+c1}{//        \PYGZsh{}pragma omp critical                      // B1}
\PYG{c+c1}{//        \PYGZob{}                                         // B2}
        \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{1.0}\PYG{p}{;}
\PYG{c+c1}{//        \PYGZcb{}                                         // B3}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}1 deposits, your balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
		\PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{14.  Mutual Exclusion Coordination Pattern: compare performance}
\label{SharedMemory/MutualExclusion:mutual-exclusion-coordination-pattern-compare-performance}
\emph{file: openMP/14.mutualExclusion-critical2/critical2.c}

\emph{Build inside 14.mutualExclusion-critical2 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make critical2
\end{Verbatim}

\emph{Execute on the command line inside 14.mutualExclusion-critical2 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./critical2
\end{Verbatim}

Here is an example of how to compare the performance of using the atomic pragma directive and the critical pragma directive.  Note that there is a function in OpenMP that lets you obtain the current time, which enables us to determine how long it took to run a particular section of our program.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* critical2.c}
\PYG{c+cm}{ * ... compares the performance of OpenMP's critical and atomic directives}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./critical2}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile, run, compare times for critical vs. atomic}
\PYG{c+cm}{ *  - Note how much more costly critical is than atomic}
\PYG{c+cm}{ *  - Research: Create an expression that, when assigned to balance,}
\PYG{c+cm}{ *     critical can handle but atomic cannot }
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}omp.h\PYGZgt{}}

\PYG{k+kt}{void} \PYG{n+nf}{print}\PYG{p}{(}\PYG{k+kt}{char}\PYG{o}{*} \PYG{n}{label}\PYG{p}{,} \PYG{k+kt}{int} \PYG{n}{reps}\PYG{p}{,} \PYG{k+kt}{double} \PYG{n}{balance}\PYG{p}{,} \PYG{k+kt}{double} \PYG{n}{total}\PYG{p}{,} \PYG{k+kt}{double} \PYG{n}{average}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After \PYGZpc{}d \PYGZdl{}1 deposits using '\PYGZpc{}s': }\PYG{l+s}{\PYGZbs{}}
\PYG{l+s}{            }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{- balance = \PYGZpc{}0.2f, }\PYG{l+s}{\PYGZbs{}}
\PYG{l+s}{            }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{- total time = \PYGZpc{}0.12f, }\PYG{l+s}{\PYGZbs{}}
\PYG{l+s}{            }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s}{- average time per deposit = \PYGZpc{}0.12f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
               \PYG{n}{reps}\PYG{p}{,} \PYG{n}{label}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{,} \PYG{n}{total}\PYG{p}{,} \PYG{n}{average}\PYG{p}{)}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{k}{const} \PYG{k+kt}{int} \PYG{n}{REPS} \PYG{o}{=} \PYG{l+m+mi}{1000000}\PYG{p}{;}
    \PYG{k+kt}{int} \PYG{n}{i}\PYG{p}{;}
    \PYG{k+kt}{double} \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}
           \PYG{n}{startTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,} 
           \PYG{n}{stopTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}
           \PYG{n}{atomicTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}
           \PYG{n}{criticalTime} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
  
    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Your starting bank account balance is \PYGZpc{}0.2f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+c1}{// simulate many deposits using atomic}
    \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for }
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp atomic}
        \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{1.0}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{stopTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{atomicTime} \PYG{o}{=} \PYG{n}{stopTime} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}
    \PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{atomic}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{,} \PYG{n}{atomicTime}\PYG{p}{,} \PYG{n}{atomicTime}\PYG{o}{/}\PYG{n}{REPS}\PYG{p}{)}\PYG{p}{;}


    \PYG{c+c1}{// simulate the same number of deposits using critical}
    \PYG{n}{balance} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{;}
    \PYG{n}{startTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel for }
    \PYG{k}{for} \PYG{p}{(}\PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{;} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n}{REPS}\PYG{p}{;} \PYG{n}{i}\PYG{o}{+}\PYG{o}{+}\PYG{p}{)} \PYG{p}{\PYGZob{}}
         \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp critical}
         \PYG{p}{\PYGZob{}}
             \PYG{n}{balance} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mf}{1.0}\PYG{p}{;}
         \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}
    \PYG{n}{stopTime} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}wtime}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
    \PYG{n}{criticalTime} \PYG{o}{=} \PYG{n}{stopTime} \PYG{o}{-} \PYG{n}{startTime}\PYG{p}{;}
    \PYG{n}{print}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{critical}\PYG{l+s}{"}\PYG{p}{,} \PYG{n}{REPS}\PYG{p}{,} \PYG{n}{balance}\PYG{p}{,} \PYG{n}{criticalTime}\PYG{p}{,} \PYG{n}{criticalTime}\PYG{o}{/}\PYG{n}{REPS}\PYG{p}{)}\PYG{p}{;}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{criticalTime / atomicTime ratio: \PYGZpc{}0.12f}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
             \PYG{n}{criticalTime} \PYG{o}{/} \PYG{n}{atomicTime}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsubsection{15.  Mutual Exclusion Coordination Pattern: language difference}
\label{SharedMemory/MutualExclusion:mutual-exclusion-coordination-pattern-language-difference}
\emph{file: openMP/15.mutualExclusion-critical3/critical3.c}

\emph{Build inside 15.mutualExclusion-critical3 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make critical3
\end{Verbatim}

\emph{Execute on the command line inside 15.mutualExclusion-critical3 directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./critical3
\end{Verbatim}

The following is a C++ code example to illustrate some language differences between C and C++.  Try the exercises described in the code below.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* critical3.c}
\PYG{c+cm}{ * ... a simple case where OpenMP's critical works but atomic does not.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./critical3}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise:}
\PYG{c+cm}{ *  - Compile, run, note resulting output is correct.}
\PYG{c+cm}{ *  - Uncomment line A, recompile, rerun, note results.}
\PYG{c+cm}{ *  - Uncomment line B, recompile, note results.}
\PYG{c+cm}{ *  - Recomment line B, uncomment line C, recompile, }
\PYG{c+cm}{ *     rerun, note change in results. }
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}iostream\PYGZgt{}   }\PYG{c+c1}{// cout}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include\PYGZlt{}omp.h\PYGZgt{}      }\PYG{c+c1}{// openmp}
\PYG{k}{using} \PYG{k}{namespace} \PYG{n}{std}\PYG{p}{;}

\PYG{k+kt}{int} \PYG{n}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}
    \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{;}

    \PYG{k}{if} \PYG{p}{(}\PYG{n}{argc} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{p}{\PYGZob{}}
        \PYG{n}{omp\PYGZus{}set\PYGZus{}num\PYGZus{}threads}\PYG{p}{(} \PYG{n}{atoi}\PYG{p}{(}\PYG{n}{argv}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

\PYG{c+c1}{//    \PYGZsh{}pragma omp parallel                          // A}
    \PYG{p}{\PYGZob{}}
        \PYG{k+kt}{int} \PYG{n}{id} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
        \PYG{k+kt}{int} \PYG{n}{numThreads} \PYG{o}{=} \PYG{n}{omp\PYGZus{}get\PYGZus{}num\PYGZus{}threads}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{c+c1}{//        \PYGZsh{}pragma omp atomic                        // B}
\PYG{c+c1}{//        \PYGZsh{}pragma omp critical                      // C}
        \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{Hello from thread \PYGZsh{}}\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{id}
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ out of }\PYG{l+s}{"} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{n}{numThreads}
             \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s}{ threads.}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{;}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{cout} \PYG{o}{\PYGZlt{}}\PYG{o}{\PYGZlt{}} \PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\paragraph{Some Explanation}
\label{SharedMemory/MutualExclusion:some-explanation}
A C line like this:
\begin{quote}

printf(``Hello from thread \#\%d of \%dn'', id, numThreads);
\end{quote}

is a single function call that is pretty much performed atomically, so you get pretty good output like.

\begin{Verbatim}[commandchars=\\\{\}]
Hello from thread \#0 of 4
Hello from thread \#2 of 4
Hello from thread \#3 of 4
Hello from thread \#1 of 4
\end{Verbatim}

By contrast, the C++ line:
\begin{quote}

cout \textless{}\textless{} ``Hello from thread \#'' \textless{}\textless{} id \textless{}\textless{} '' of '' \textless{}\textless{} numThreads \textless{}\textless{} endl;
\end{quote}

has 5 different function calls, so the outputs from these functions get interleaved within the shared stream cout as the threads `race' to write to it.  You may have observed output similar to this:
\begin{quote}

Hello from thread \#Hello from thread\#Hello from thread\#0 of 4Hello from thread\#

2 of 43 of 4

1 of 4
\end{quote}

The other facet that this particular patternlet shows is that OpenMP's atomic directive will not fix this -- it is too complex for atomic, so the compiler flags that as an error.  To make this statement execute indivisibly, you need to use the critical directive, providing a pretty simple case where critical works and atomic does not.


\subsection{Task Decomposition Algorithm Strategies}
\label{SharedMemory/TaskDecomp:task-decomposition-algorithm-strategies}\label{SharedMemory/TaskDecomp::doc}
All threaded programs have some form of task decomposition, that is, delineating which threads will do what tasks in parallel at certain points in the program. We have seen one way of dictating this by using the master-worker implementation, where one thread does one task and all the others to another.  Here we introduce a more general approach that can be used.


\subsubsection{16. Task Decomposition Algorithm Strategy using OpenMP section directive}
\label{SharedMemory/TaskDecomp:task-decomposition-algorithm-strategy-using-openmp-section-directive}
\emph{file: openMP/16.sections/sections.c}

\emph{Build inside 16.sections directory:}

\begin{Verbatim}[commandchars=\\\{\}]
make sections
\end{Verbatim}

\emph{Execute on the command line inside 16.sections directory:}

\begin{Verbatim}[commandchars=\\\{\}]
./sections
\end{Verbatim}

This example shows how to create a program with arbitrary separate tasks that run concurrently.  This is useful if you have tasks that are not dependent on one another.

\begin{Verbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+cm}{/* sections.c}
\PYG{c+cm}{ * ... illustrates the use of OpenMP's parallel section/sections directives,}
\PYG{c+cm}{ *      which can be used for task parallelism...}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Joel Adams, Calvin College, November 2009.}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Usage: ./sections}
\PYG{c+cm}{ *}
\PYG{c+cm}{ * Exercise: Compile, run (several times), compare output to source code.}
\PYG{c+cm}{ */}

\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdio.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}omp.h\PYGZgt{}}
\PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{include \PYGZlt{}stdlib.h\PYGZgt{}}

\PYG{k+kt}{int} \PYG{n+nf}{main}\PYG{p}{(}\PYG{k+kt}{int} \PYG{n}{argc}\PYG{p}{,} \PYG{k+kt}{char}\PYG{o}{*}\PYG{o}{*} \PYG{n}{argv}\PYG{p}{)} \PYG{p}{\PYGZob{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{Before...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp parallel sections num\PYGZus{}threads(4)}
    \PYG{p}{\PYGZob{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section }
        \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Task/section A performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                    \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section }
        \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Task/section B performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                    \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section}
        \PYG{p}{\PYGZob{}}
            \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Task/section C performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,}
                    \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
        \PYG{c+cp}{\PYGZsh{}}\PYG{c+cp}{pragma omp section }
        \PYG{p}{\PYGZob{}}
                \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s}{Task/section D performed by thread \PYGZpc{}d}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{,} 
                         \PYG{n}{omp\PYGZus{}get\PYGZus{}thread\PYGZus{}num}\PYG{p}{(}\PYG{p}{)} \PYG{p}{)}\PYG{p}{;} 
        \PYG{p}{\PYGZcb{}}
    \PYG{p}{\PYGZcb{}}

    \PYG{n}{printf}\PYG{p}{(}\PYG{l+s}{"}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{After...}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s}{"}\PYG{p}{)}\PYG{p}{;}

    \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{;}
\PYG{p}{\PYGZcb{}}
\end{Verbatim}


\subsection{Categorizing Patterns}
\label{SharedMemory/patterns_diagram::doc}\label{SharedMemory/patterns_diagram:categorizing-patterns}
There has been a fair amount of work by several researchers who have catergorized patterns found in parallel programs.  We have shown you simple examples of several of them that are very common when writing OpenMP programs that use shared memory.  Now that you have seen them, you can try to imagine the patterns falling into the categories shown on the following diagram:

\includegraphics{OMP_Patterns.png}

Most programs you will write will include patterns for an Algorithm Strategy (both data decomposition and task decomposition), some of the Implementation Strategies (if not all), and some of the Coordination Mechanisms.  The patternlets show simple examples that you can use as a guide.  In OpenMP, most programs use various shared data in memory as their data structure implementation strategy. The Process/Thread Control Mechanism patterns are built in to any OpenMP program. Multiple Instruction, Multiple Data (MIMD) is built in because forked threads operate independently on different data. Likewise, pools of threads are part of every compiled OpenMP threaded program.



\renewcommand{\indexname}{Index}
\printindex
\end{document}
