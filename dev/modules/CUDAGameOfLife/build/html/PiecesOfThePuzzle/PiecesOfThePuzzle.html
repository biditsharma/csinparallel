<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Pieces of the Puzzle &mdash; CUDA Game of Life</title>
    
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="CUDA Game of Life" href="../index.html" />
    <link rel="next" title="Example Programs and Labs" href="../ExampleProgramsAndLabs/ExampleProgramsAndLabs.html" />
    <link rel="prev" title="Introduction" href="../Introduction/Introduction.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../ExampleProgramsAndLabs/ExampleProgramsAndLabs.html" title="Example Programs and Labs"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../Introduction/Introduction.html" title="Introduction"
             accesskey="P">previous</a> |</li>
        <li><a href="../index.html">CUDA Game of Life</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="pieces-of-the-puzzle">
<h1>Pieces of the Puzzle<a class="headerlink" href="#pieces-of-the-puzzle" title="Permalink to this headline">¶</a></h1>
<div class="section" id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Permalink to this headline">¶</a></h2>
<ul>
<li><dl class="first docutils">
<dt><a class="reference internal" href="#pieces-of-the-puzzle">Pieces of the Puzzle</a></dt>
<dd><ul class="first last simple">
<li><a class="reference internal" href="#an-illustrative-example">An Illustrative Example</a></li>
<li><a class="reference internal" href="#execution-configuration">Execution Configuration</a></li>
<li><a class="reference internal" href="#memory-management">Memory Management</a></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>CUDA encourages an approach to solving parrallelizable problems that is likely unfamiliar and novel. CUDA also exposes a level of control that can be daunting, but promises the programmer the ability to squeeze out every ounce of performance from their CUDA-enabled GPU. Because of these unique features, learning to understand how to write CUDA programs is akin to solving a jigsaw puzzle — you must grasp a few different concepts before the bigger picture comes together.</p>
<p>A good way to learn CUDA is to compare typical CPU-only code with CUDA code. We&#8217;ll show you an example in the next section. Pay attention to what&#8217;s different and <em>why</em>; doing so should reveal how programming in CUDA is a unique. After the example, we&#8217;ll go into more detail about some of the pieces of the puzzle.</p>
</div>
<div class="section" id="an-illustrative-example">
<h2>An Illustrative Example<a class="headerlink" href="#an-illustrative-example" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s say you have two large and equal-sized vectors (1D arrays) of numbers. You want to add them (by doing pairwise addition), and produce a third vector of the same size. A simple sequential solution to this trivial teaser might look like this:</p>
<div class="highlight-python"><pre>void add_vectors(int *result, int *a, int *b, int length) {
  int i;
  for (i=0; i&lt;length; i++) {
      result[i] = a[i] + b[i];
  }
}

int main(void) {
  ... // load the input vectors
  add_vectors(result, a, b, length);
}</pre>
</div>
<p>The solutions for the elements in <tt class="docutils literal"><span class="pre">result</span></tt> are computed sequentially &#8211; one element is completely computed before moving to the next part. Now imagine that we unroll the loop, turning it into <tt class="docutils literal"><span class="pre">length</span></tt> many lines of code, with the first line reading <tt class="docutils literal"><span class="pre">result[0]</span> <span class="pre">=</span> <span class="pre">a[0]</span> <span class="pre">+</span> <span class="pre">b[0]</span></tt>, the second <tt class="docutils literal"><span class="pre">result[1]</span> <span class="pre">=</span> <span class="pre">a[1]</span> <span class="pre">+</span> <span class="pre">b[1]</span></tt>, and so on. Because each of these lines of code are independent of each other, we can use the GPU to run them all at once instead of sequentially. In CUDA, the code to do that might look like:</p>
<div class="highlight-python"><pre>__global__ void add_vectors_kernel(int *results_dev, int *a_dev, int *b_dev) {
   int i = threadIdx.x;
   results_dev[i] = a_dev[i] + b_dev[i];
}

int main(void) {
    ... // load the input vectors
    ... // copy the input vectors to the GPU's memory
    add_vectors_kernel&lt;&lt;&lt;1, length&gt;&gt;&gt;(result_dev, a_dev, b_dev);
}</pre>
</div>
<p>Now that you&#8217;ve seen some CUDA, see if this makes sense (it&#8217;s OK if it doesn&#8217;t yet). A kernel (indicated by the <tt class="docutils literal"><span class="pre">__global__</span></tt> keyword) is a bit of code that runs on the GPU. One executes many copies of the <tt class="docutils literal"><span class="pre">kernel</span></tt> at a time by creating a grid of <tt class="docutils literal"><span class="pre">blocks</span></tt>. Each block is composed of <tt class="docutils literal"><span class="pre">threads</span></tt>, where each thread executes a single copy of the kernel. Each thread is given a unique ID so that it can differentiate itself from the other threads.</p>
</div>
<div class="section" id="how-the-cuda-code-is-different">
<h2>How the CUDA code is different<a class="headerlink" href="#how-the-cuda-code-is-different" title="Permalink to this headline">¶</a></h2>
<p>Lets discuss two of the main differences between the CPU and CUDA versions of the code. The first is the <tt class="docutils literal"><span class="pre">__global__</span></tt> keyword in front of the <tt class="docutils literal"><span class="pre">add_vectors_kernel</span></tt> function. This keyword identifies the function as a <em>kernel</em>. A kernel is a bit of code that runs on the GPU.</p>
<p>The second difference is the line in the main function that reads <tt class="docutils literal"><span class="pre">add_vectors_kernel&lt;&lt;&lt;1,</span> <span class="pre">length&gt;&gt;&gt;(result_dev,</span> <span class="pre">a_dev,</span> <span class="pre">b_dev)</span></tt>. This is how kernel functions are launched from the host CPU. The values in the angle brackets determine how many blocks and threads are used in the grid to execute the kernel. In this case, we have one block with <tt class="docutils literal"><span class="pre">length</span></tt> many threads in that block. We can see that the kernel was written to use a thread&#8217;s index (<tt class="docutils literal"><span class="pre">threadIdx.x</span></tt>, in this case) to determine which element of the result it should compute the answer for.</p>
<p>Notice how the code in the kernel is like the inside of our CPU vector-adding loop. In this example, we effectively unroll the loop and give each thread a different addition.</p>
<p>In truth, this is a toy example and, excepting the largest vectors, the problem is solved faster on the CPU than on the GPU. Imagine however, that instead of addition, we were using a more expensive many-instruction operator or that we were adding vectors with many thousands of elements. The benefits of GPU parallelization become clear in those cases.</p>
</div>
<div class="section" id="execution-configuration">
<h2>Execution Configuration<a class="headerlink" href="#execution-configuration" title="Permalink to this headline">¶</a></h2>
<p><em>Execution configuration</em> is the name given to the parameters inside of the &lt;&lt;&lt; and &gt;&gt;&gt;. <tt class="docutils literal"><span class="pre">kernel_func&lt;&lt;&lt;3,</span> <span class="pre">1000&gt;&gt;&gt;()</span></tt>, for example, will run <tt class="docutils literal"><span class="pre">kernel_func</span></tt> using three blocks of 1,000 threads each, for a total of 3,000 threads. Using integers as those two arguments forces one-dimensional configuration of grid and block sizes. The first execution configuration argument specifies the number of blocks in the grid, and the second argument specifies the number of threads in each block. The division of threads into blocks is significant because threads that are in the same block have access to a small but fast <em>shared memory.</em></p>
<hr class="docutils" />
<p><strong>Attention!</strong>
Be aware of some limitations on grid and block sizes. For example, modern cards (compute capability 2.x) support blocks with no more than 1024 threads. Earlier cards are limited to 512 threads per block. The limit on grid sizes is much harder to reach (no more than 65535 blocks in any of the dimensions x, y, or z.)</p>
<hr class="docutils" />
<p>If we&#8217;d rather have our threads configured in a 20×10 2D configuration, we can do so by specifying our dimensions by using dim3 (a typedef&#8217;d structure):</p>
<div class="highlight-python"><pre>dim3 dimGrid(1); // 1 block
dim3 dimBlock(20, 10, 1); // 200 threads
mat_mult_kernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(result_dev, A_dev, B_dev);</pre>
</div>
<p>This flexibility exists for the programmer&#8217;s benefit. Since the location of each thread within the grid is exposed to the kernel by <tt class="docutils literal"><span class="pre">threadIdx.[xyz]</span></tt> and <tt class="docutils literal"><span class="pre">blockIdx.[xyz]</span></tt>, we can choose an execution configuration that causes the ... <tt class="docutils literal"><span class="pre">Idx</span></tt> variables to map neatly to our current problem&#8217;s data. We might use <tt class="docutils literal"><span class="pre">kernel_func&lt;&lt;&lt;5000,</span> <span class="pre">1000&gt;&gt;&gt;</span></tt> for something involving a five-million long vector (recall the block size limitations), or the above example involving <tt class="docutils literal"><span class="pre">dim3</span></tt> for a 20×10 matrix.</p>
<hr class="docutils" />
<p><strong>Note</strong></p>
<p>See the <a class="reference external" href="http://legacy.lclark.edu/~jmache/parallel/CUDA/examples/matrix_multiplication.cu">matrix_multiplication.cu</a> example for a demonstration of using <tt class="docutils literal"><span class="pre">threadIdx</span></tt> and <tt class="docutils literal"><span class="pre">blockIdx</span></tt>.</p>
<hr class="docutils" />
<img alt="../_images/gridBlockThread.png" src="../_images/gridBlockThread.png" />
<blockquote>
<div><p><em>Threads are arranged inside of blocks; blocks are arranged inside of a grid. This image shows a 3×2 grid with 4×3 blocks.</em></p>
<p>Image source: <em>NVIDIA CUDA Programming Guide v 4.0</em> (pg. 9)</p>
</div></blockquote>
</div>
<div class="section" id="memory-management">
<h2>Memory Management<a class="headerlink" href="#memory-management" title="Permalink to this headline">¶</a></h2>
<p>In the previous examples, we have explained how to setup an execution configuration and launch a kernel. In order to run a kernel though, you must first understand how to manage memory between the device and host. To put it simply, there is no memory shared between the host and device, so data must be explicitly transferred. Transferring data to a CUDA device is a two-step process that requires we allocate space for the data and then copy the data into that space. A simple CUDA program that does one calculation will usually have four steps:</p>
<blockquote>
<div><ol class="arabic simple">
<li>allocate memory on the CUDA device</li>
<li>copy data from the host to the allocated device memory</li>
<li>invoke the kernel</li>
<li>copy the results from the device to the host</li>
</ol>
</div></blockquote>
<p>To give an example of how this is achieved we will look back at the vector addition example:</p>
<div class="highlight-python"><pre>__global__ void add_vectors_kernel(int *results_dev, int *a_dev, int *b_dev) {
  int i = threadIdx.x;
  results_dev[i] = a_dev[i] + b_dev[i];
}

int main(void) {
  int length = 5;
  int a[] = {0, 1, 2, 3, 4};
  int b[] = {5, 6, 7, 8, 9};
  int results[length];
  int *a_dev, *b_dev, *results_dev;

  // 1. Allocate memory space on CUDA device
  cudaMalloc((void**)&amp;a_dev, sizeof(int) * length);
  cudaMalloc((void**)&amp;b_dev, sizeof(int) * length);
  cudaMalloc((void**)&amp;results_dev, sizeof(int) * length);

  // 2. Copy host data to space allocated on the device
  cudaMemcpy(a_dev, a, sizeof(int) * length, cudaMemcpyHostToDevice);
  cudaMemcpy(b_dev, b, sizeof(int) * length, cudaMemcpyHostToDevice);

  // 3. Execute the kernel
  add_vectors_kernel&lt;&lt;&lt;1, length&gt;&gt;&gt;(result_dev, a_dev, b_dev);

  // 4. Copy device data (the result) back to the host
  cudaMemcpy(results, results_dev, sizeof(int) * length, cudaMemcpyDeviceToHost);
}</pre>
</div>
<p>Now hopefully that example isn&#8217;t too overwhelming. There are really only two new CUDA functions in this example. Here are their definitions:</p>
<div class="highlight-python"><pre>cudaError_t cudaMalloc (void **devPtr, size_t size)
cudaError_t cudaMemcpy(void *dst, const void *src, size_t size, enum cudaMemcpyKind kind)</pre>
</div>
<p>We won&#8217;t discuss the return types, but know that they provide a way of checking the success of the operation.</p>
<p>The <tt class="docutils literal"><span class="pre">cudaMalloc</span></tt> is similar to a standard C <tt class="docutils literal"><span class="pre">malloc</span></tt> call, but instead of capturing it&#8217;s return value to store the pointer to allocated memory, we pass in the address of a pointer; <tt class="docutils literal"><span class="pre">cudaMalloc</span></tt> will write the correct pointer to that location. It is important to stress that the only difference between pointers from <tt class="docutils literal"><span class="pre">malloc</span></tt> and <tt class="docutils literal"><span class="pre">cudaMalloc</span></tt> is that they only make sense on their respective devices (host vs device). This means that dereferencing a device pointer on the host doesn&#8217;t make sense! This also explains why the convention to add <tt class="docutils literal"><span class="pre">_d</span></tt> or <tt class="docutils literal"><span class="pre">_dev</span></tt> to device pointers.</p>
<p>For the <tt class="docutils literal"><span class="pre">cudaMemcpy()</span></tt> function, know that <tt class="docutils literal"><span class="pre">dst</span></tt> and <tt class="docutils literal"><span class="pre">src</span></tt> are pointers which specify where to put (<tt class="docutils literal"><span class="pre">dst</span></tt>) the data that you are copying (at <tt class="docutils literal"><span class="pre">src</span></tt>) and that <tt class="docutils literal"><span class="pre">kind</span></tt> is a memory copy type (direction). A few different directions are available, including: <tt class="docutils literal"><span class="pre">cudaMemcpyDeviceToHost</span></tt>, <tt class="docutils literal"><span class="pre">cudaMemcpyHostToDevice</span></tt>, and even <tt class="docutils literal"><span class="pre">cudaMemcpyDeviceToDevice</span></tt>.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/CSInParallel200wide.png" alt="Logo"/>
            </a></p>
  <h3><a href="../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Pieces of the Puzzle</a><ul>
<li><a class="reference internal" href="#contents">Contents</a></li>
<li><a class="reference internal" href="#an-illustrative-example">An Illustrative Example</a></li>
<li><a class="reference internal" href="#how-the-cuda-code-is-different">How the CUDA code is different</a></li>
<li><a class="reference internal" href="#execution-configuration">Execution Configuration</a></li>
<li><a class="reference internal" href="#memory-management">Memory Management</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../Introduction/Introduction.html"
                        title="previous chapter">Introduction</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../ExampleProgramsAndLabs/ExampleProgramsAndLabs.html"
                        title="next chapter">Example Programs and Labs</a></p>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../ExampleProgramsAndLabs/ExampleProgramsAndLabs.html" title="Example Programs and Labs"
             >next</a> |</li>
        <li class="right" >
          <a href="../Introduction/Introduction.html" title="Introduction"
             >previous</a> |</li>
        <li><a href="../index.html">CUDA Game of Life</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2b1.
    </div>
  </body>
</html>